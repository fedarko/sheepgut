{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform read smoothing then assemble with LJA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T10:07:59.325361Z",
     "iopub.status.busy": "2022-03-21T10:07:59.324407Z",
     "iopub.status.idle": "2022-03-21T10:07:59.804545Z",
     "shell.execute_reply": "2022-03-21T10:07:59.804979Z"
    }
   },
   "outputs": [],
   "source": [
    "%run \"Header.ipynb\"\n",
    "%run \"../main-workflow/utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T10:07:59.808857Z",
     "iopub.status.busy": "2022-03-21T10:07:59.808185Z",
     "iopub.status.idle": "2022-03-21T10:08:00.193791Z",
     "shell.execute_reply": "2022-03-21T10:08:00.193048Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pysam\n",
    "import skbio\n",
    "from collections import defaultdict, Counter\n",
    "from linked_mutations_utils import find_mutated_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Quick sanity check: ensure that all $k$-mers ($k$ = 5,001) are unique in each MAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T10:08:00.204856Z",
     "iopub.status.busy": "2022-03-21T10:08:00.204009Z",
     "iopub.status.idle": "2022-03-21T10:09:44.206226Z",
     "shell.execute_reply": "2022-03-21T10:09:44.206867Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On start pos 0 in CAMP.\n",
      "On start pos 1,000,000 in CAMP.\n",
      "The most common k = 5,001-mer in CAMP occurred 1 time(s).\n",
      "On start pos 0 in BACT1.\n",
      "On start pos 1,000,000 in BACT1.\n",
      "On start pos 2,000,000 in BACT1.\n",
      "The most common k = 5,001-mer in BACT1 occurred 1 time(s).\n",
      "On start pos 0 in BACT2.\n",
      "On start pos 1,000,000 in BACT2.\n",
      "On start pos 2,000,000 in BACT2.\n",
      "The most common k = 5,001-mer in BACT2 occurred 1 time(s).\n"
     ]
    }
   ],
   "source": [
    "# I know there are actual k-mer counting tools you can use but no reason to overcomplicate things for now\n",
    "\n",
    "k = 5001\n",
    "\n",
    "for seq in SEQS:\n",
    "    fasta = skbio.DNA.read(f\"../seqs/{seq}.fasta\")\n",
    "    bargain_bin_kmer_counter = Counter()\n",
    "    \n",
    "    # The skbio.DNA object is 0-indexed, so 0 is the leftmost k-mer start position and\n",
    "    # ((seq length) - k) is the rightmost k-mer start position. The + 1 is because python ranges don't include\n",
    "    # the right endpoint.\n",
    "    for start_pos in range(0, seq2len[seq] - k + 1):\n",
    "        \n",
    "        # NOTE: this is a terrible no good very bad way to do this; it's more efficient to use a \"sliding window\"\n",
    "        # approach where you store the entire k-mer and then, with each step, just remove the first character and\n",
    "        # add on a new last character. \"But, uh, this code will only be run on these three MAGs, so I'm gonna\n",
    "        # prioritize clarity over optimization,\" says me, the insane person who just spent like a minute writing\n",
    "        # this comment when I could've been optimizing this code instead look WHATEVER this counts k-mers and it's\n",
    "        # 4am let's not overcomplicate it, look if you're on GitHub right now and you see this inane comment\n",
    "        # we can both just pretend that you were looking at some really optimized code and we'll both walk away\n",
    "        # satisfied, capisce\n",
    "        kmer = fasta[start_pos : start_pos + k]\n",
    "        \n",
    "        bargain_bin_kmer_counter[str(kmer)] += 1\n",
    "        if start_pos % 1000000 == 0: print(f\"On start pos {start_pos:,} in {seq2name[seq]}.\")\n",
    "    \n",
    "    mckc = bargain_bin_kmer_counter.most_common(1)[0][1]\n",
    "    print(f\"The most common k = {k:,}-mer in {seq2name[seq]} occurred {mckc:,} time(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Smooth reads\n",
    "\n",
    "Lots of this code is duplicated from the `Phasing-01-MakeGraph.ipynb` notebook in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T10:09:44.214524Z",
     "iopub.status.busy": "2022-03-21T10:09:44.213726Z",
     "iopub.status.idle": "2022-03-21T10:09:44.215822Z",
     "shell.execute_reply": "2022-03-21T10:09:44.216462Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set this to True to actually generate ordinary smoothed reads that include called mutations;\n",
    "# set this to False to generate \"sanity check\" perfect smoothed reads, where no mutations are included\n",
    "# and the read entirely matches the reference\n",
    "actually_include_mutations_in_the_smoothed_reads = True\n",
    "\n",
    "add_virtual_reads = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T10:09:44.550099Z",
     "iopub.status.busy": "2022-03-21T10:09:44.220755Z",
     "iopub.status.idle": "2022-03-21T10:10:01.668777Z",
     "shell.execute_reply": "2022-03-21T10:10:01.669440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence edge_6104 has average coverage 4,158.57 and median coverage 4,122.00.\n",
      "Sequence edge_1671 has average coverage 1,415.07 and median coverage 1,436.00.\n",
      "Sequence edge_2358 has average coverage 2,993.46 and median coverage 2,936.00.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'edge_6104': 4158.572468826692,\n",
       " 'edge_1671': 1415.072755380576,\n",
       " 'edge_2358': 2993.461913625056}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll need to know the mean coverage of each sequence when computing virtual reads.\n",
    "seq2meancov = get_meancovs()\n",
    "seq2meancov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T10:10:01.686105Z",
     "iopub.status.busy": "2022-03-21T10:10:01.683412Z",
     "iopub.status.idle": "2022-03-21T12:49:09.479624Z",
     "shell.execute_reply": "2022-03-21T12:49:09.480481Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Identifying mutated positions (p = 1%) in genome CAMP...\n",
      "Found 83 mutated positions (p = 1%) in CAMP.\n",
      "Note that this tally is higher than you'd see in e.g. the CP1/2/3 plots, because now we're including both 'rare' and non-rare mutations. Just so you don't waste five minutes sanity-checking this like I did.\n",
      "Going through these positions...\n",
      "\tOn aln 5,000 in seq CAMP. Time spent on CAMP so far: 11.98 sec.\n",
      "\tOn aln 10,000 in seq CAMP. Time spent on CAMP so far: 31.14 sec.\n",
      "\tOn aln 15,000 in seq CAMP. Time spent on CAMP so far: 49.79 sec.\n",
      "\tOn aln 20,000 in seq CAMP. Time spent on CAMP so far: 69.77 sec.\n",
      "\tOn aln 25,000 in seq CAMP. Time spent on CAMP so far: 88.71 sec.\n",
      "\tOn aln 30,000 in seq CAMP. Time spent on CAMP so far: 106.62 sec.\n",
      "\tOn aln 35,000 in seq CAMP. Time spent on CAMP so far: 124.48 sec.\n",
      "\tOn aln 40,000 in seq CAMP. Time spent on CAMP so far: 142.91 sec.\n",
      "\tOn aln 45,000 in seq CAMP. Time spent on CAMP so far: 161.48 sec.\n",
      "\tOn aln 50,000 in seq CAMP. Time spent on CAMP so far: 178.42 sec.\n",
      "\tOn aln 55,000 in seq CAMP. Time spent on CAMP so far: 196.35 sec.\n",
      "\tOn aln 60,000 in seq CAMP. Time spent on CAMP so far: 214.22 sec.\n",
      "\tOn aln 65,000 in seq CAMP. Time spent on CAMP so far: 232.49 sec.\n",
      "\tOn aln 70,000 in seq CAMP. Time spent on CAMP so far: 252.57 sec.\n",
      "\tOn aln 75,000 in seq CAMP. Time spent on CAMP so far: 271.64 sec.\n",
      "\tOn aln 80,000 in seq CAMP. Time spent on CAMP so far: 291.36 sec.\n",
      "\tOn aln 85,000 in seq CAMP. Time spent on CAMP so far: 311.81 sec.\n",
      "\tOn aln 90,000 in seq CAMP. Time spent on CAMP so far: 332.36 sec.\n",
      "\tOn aln 95,000 in seq CAMP. Time spent on CAMP so far: 350.78 sec.\n",
      "\tOn aln 100,000 in seq CAMP. Time spent on CAMP so far: 369.57 sec.\n",
      "\tOn aln 105,000 in seq CAMP. Time spent on CAMP so far: 387.52 sec.\n",
      "\tOn aln 110,000 in seq CAMP. Time spent on CAMP so far: 404.45 sec.\n",
      "\tOn aln 115,000 in seq CAMP. Time spent on CAMP so far: 421.86 sec.\n",
      "\tOn aln 120,000 in seq CAMP. Time spent on CAMP so far: 439.55 sec.\n",
      "\tOn aln 125,000 in seq CAMP. Time spent on CAMP so far: 456.50 sec.\n",
      "\tOn aln 130,000 in seq CAMP. Time spent on CAMP so far: 473.49 sec.\n",
      "\tOn aln 135,000 in seq CAMP. Time spent on CAMP so far: 490.50 sec.\n",
      "\tOn aln 140,000 in seq CAMP. Time spent on CAMP so far: 508.21 sec.\n",
      "\tOn aln 145,000 in seq CAMP. Time spent on CAMP so far: 527.97 sec.\n",
      "\tOn aln 150,000 in seq CAMP. Time spent on CAMP so far: 545.60 sec.\n",
      "\tOn aln 155,000 in seq CAMP. Time spent on CAMP so far: 563.10 sec.\n",
      "\tOn aln 160,000 in seq CAMP. Time spent on CAMP so far: 581.47 sec.\n",
      "\tOn aln 165,000 in seq CAMP. Time spent on CAMP so far: 599.61 sec.\n",
      "\tOn aln 170,000 in seq CAMP. Time spent on CAMP so far: 617.23 sec.\n",
      "\tOn aln 175,000 in seq CAMP. Time spent on CAMP so far: 635.45 sec.\n",
      "\tOn aln 180,000 in seq CAMP. Time spent on CAMP so far: 653.16 sec.\n",
      "\tOn aln 185,000 in seq CAMP. Time spent on CAMP so far: 670.48 sec.\n",
      "\tOn aln 190,000 in seq CAMP. Time spent on CAMP so far: 687.77 sec.\n",
      "\tOn aln 195,000 in seq CAMP. Time spent on CAMP so far: 705.76 sec.\n",
      "\tOn aln 200,000 in seq CAMP. Time spent on CAMP so far: 725.55 sec.\n",
      "\tOn aln 205,000 in seq CAMP. Time spent on CAMP so far: 742.55 sec.\n",
      "\tOn aln 210,000 in seq CAMP. Time spent on CAMP so far: 759.89 sec.\n",
      "\tOn aln 215,000 in seq CAMP. Time spent on CAMP so far: 778.05 sec.\n",
      "\tOn aln 220,000 in seq CAMP. Time spent on CAMP so far: 795.43 sec.\n",
      "\tOn aln 225,000 in seq CAMP. Time spent on CAMP so far: 813.40 sec.\n",
      "\tOn aln 230,000 in seq CAMP. Time spent on CAMP so far: 830.67 sec.\n",
      "\tOn aln 235,000 in seq CAMP. Time spent on CAMP so far: 847.94 sec.\n",
      "\tOn aln 240,000 in seq CAMP. Time spent on CAMP so far: 865.15 sec.\n",
      "\tOn aln 245,000 in seq CAMP. Time spent on CAMP so far: 882.61 sec.\n",
      "\tOn aln 250,000 in seq CAMP. Time spent on CAMP so far: 899.78 sec.\n",
      "\tOn aln 255,000 in seq CAMP. Time spent on CAMP so far: 916.99 sec.\n",
      "\tOn aln 260,000 in seq CAMP. Time spent on CAMP so far: 934.18 sec.\n",
      "\tOn aln 265,000 in seq CAMP. Time spent on CAMP so far: 952.00 sec.\n",
      "\tOn aln 270,000 in seq CAMP. Time spent on CAMP so far: 970.21 sec.\n",
      "\tOn aln 275,000 in seq CAMP. Time spent on CAMP so far: 988.15 sec.\n",
      "\tOn aln 280,000 in seq CAMP. Time spent on CAMP so far: 1,005.43 sec.\n",
      "\tOn aln 285,000 in seq CAMP. Time spent on CAMP so far: 1,022.80 sec.\n",
      "\tOn aln 290,000 in seq CAMP. Time spent on CAMP so far: 1,039.85 sec.\n",
      "\tOn aln 295,000 in seq CAMP. Time spent on CAMP so far: 1,056.79 sec.\n",
      "\tOn aln 300,000 in seq CAMP. Time spent on CAMP so far: 1,076.11 sec.\n",
      "\tOn aln 305,000 in seq CAMP. Time spent on CAMP so far: 1,095.62 sec.\n",
      "\tOn aln 310,000 in seq CAMP. Time spent on CAMP so far: 1,115.34 sec.\n",
      "\tOn aln 315,000 in seq CAMP. Time spent on CAMP so far: 1,135.47 sec.\n",
      "\tOn aln 320,000 in seq CAMP. Time spent on CAMP so far: 1,154.56 sec.\n",
      "\tOn aln 325,000 in seq CAMP. Time spent on CAMP so far: 1,172.88 sec.\n",
      "\tOn aln 330,000 in seq CAMP. Time spent on CAMP so far: 1,192.83 sec.\n",
      "\tOn aln 335,000 in seq CAMP. Time spent on CAMP so far: 1,211.92 sec.\n",
      "\tOn aln 340,000 in seq CAMP. Time spent on CAMP so far: 1,230.57 sec.\n",
      "\tOn aln 345,000 in seq CAMP. Time spent on CAMP so far: 1,248.80 sec.\n",
      "\tOn aln 350,000 in seq CAMP. Time spent on CAMP so far: 1,266.76 sec.\n",
      "\tOn aln 355,000 in seq CAMP. Time spent on CAMP so far: 1,285.23 sec.\n",
      "\tOn aln 360,000 in seq CAMP. Time spent on CAMP so far: 1,304.81 sec.\n",
      "\tOn aln 365,000 in seq CAMP. Time spent on CAMP so far: 1,325.45 sec.\n",
      "\tOn aln 370,000 in seq CAMP. Time spent on CAMP so far: 1,344.91 sec.\n",
      "\tOn aln 375,000 in seq CAMP. Time spent on CAMP so far: 1,363.21 sec.\n",
      "\tOn aln 380,000 in seq CAMP. Time spent on CAMP so far: 1,384.67 sec.\n",
      "\tOn aln 385,000 in seq CAMP. Time spent on CAMP so far: 1,407.36 sec.\n",
      "\tOn aln 390,000 in seq CAMP. Time spent on CAMP so far: 1,426.32 sec.\n",
      "\tOn aln 395,000 in seq CAMP. Time spent on CAMP so far: 1,445.18 sec.\n",
      "\tOn aln 400,000 in seq CAMP. Time spent on CAMP so far: 1,462.29 sec.\n",
      "\tOn aln 405,000 in seq CAMP. Time spent on CAMP so far: 1,479.42 sec.\n",
      "\tOn aln 410,000 in seq CAMP. Time spent on CAMP so far: 1,496.52 sec.\n",
      "\tOn aln 415,000 in seq CAMP. Time spent on CAMP so far: 1,513.64 sec.\n",
      "\tOn aln 420,000 in seq CAMP. Time spent on CAMP so far: 1,532.39 sec.\n",
      "\tOn aln 425,000 in seq CAMP. Time spent on CAMP so far: 1,551.62 sec.\n",
      "\tOn aln 430,000 in seq CAMP. Time spent on CAMP so far: 1,569.12 sec.\n",
      "\tOn aln 435,000 in seq CAMP. Time spent on CAMP so far: 1,587.00 sec.\n",
      "\tOn aln 440,000 in seq CAMP. Time spent on CAMP so far: 1,605.76 sec.\n",
      "\tOn aln 445,000 in seq CAMP. Time spent on CAMP so far: 1,631.83 sec.\n",
      "\tOn aln 450,000 in seq CAMP. Time spent on CAMP so far: 1,640.01 sec.\n",
      "\tOn aln 455,000 in seq CAMP. Time spent on CAMP so far: 1,647.33 sec.\n",
      "\tOn aln 460,000 in seq CAMP. Time spent on CAMP so far: 1,653.69 sec.\n",
      "\tOn aln 465,000 in seq CAMP. Time spent on CAMP so far: 1,660.05 sec.\n",
      "\tOn aln 470,000 in seq CAMP. Time spent on CAMP so far: 1,666.58 sec.\n",
      "\tOn aln 475,000 in seq CAMP. Time spent on CAMP so far: 1,673.11 sec.\n",
      "We ignored 137 linear alignments, fyi.\n",
      "Done with edge_6104! Took 1,674.49 sec.\n",
      "======================================================================\n",
      "Identifying mutated positions (p = 1%) in genome BACT1...\n",
      "Found 22,144 mutated positions (p = 1%) in BACT1.\n",
      "Note that this tally is higher than you'd see in e.g. the CP1/2/3 plots, because now we're including both 'rare' and non-rare mutations. Just so you don't waste five minutes sanity-checking this like I did.\n",
      "Going through these positions...\n",
      "\tOn aln 5,000 in seq BACT1. Time spent on BACT1 so far: 49.76 sec.\n",
      "\tOn aln 10,000 in seq BACT1. Time spent on BACT1 so far: 121.24 sec.\n",
      "\tOn aln 15,000 in seq BACT1. Time spent on BACT1 so far: 213.58 sec.\n",
      "\tOn aln 20,000 in seq BACT1. Time spent on BACT1 so far: 286.29 sec.\n",
      "\tOn aln 25,000 in seq BACT1. Time spent on BACT1 so far: 387.51 sec.\n",
      "\tOn aln 30,000 in seq BACT1. Time spent on BACT1 so far: 486.08 sec.\n",
      "\tOn aln 35,000 in seq BACT1. Time spent on BACT1 so far: 571.02 sec.\n",
      "\tOn aln 40,000 in seq BACT1. Time spent on BACT1 so far: 638.57 sec.\n",
      "\tOn aln 45,000 in seq BACT1. Time spent on BACT1 so far: 737.72 sec.\n",
      "\tOn aln 50,000 in seq BACT1. Time spent on BACT1 so far: 809.63 sec.\n",
      "\tOn aln 55,000 in seq BACT1. Time spent on BACT1 so far: 881.89 sec.\n",
      "\tOn aln 60,000 in seq BACT1. Time spent on BACT1 so far: 986.33 sec.\n",
      "\tOn aln 65,000 in seq BACT1. Time spent on BACT1 so far: 1,091.46 sec.\n",
      "\tOn aln 70,000 in seq BACT1. Time spent on BACT1 so far: 1,157.76 sec.\n",
      "\tOn aln 75,000 in seq BACT1. Time spent on BACT1 so far: 1,260.26 sec.\n",
      "\tOn aln 80,000 in seq BACT1. Time spent on BACT1 so far: 1,331.78 sec.\n",
      "\tOn aln 85,000 in seq BACT1. Time spent on BACT1 so far: 1,399.58 sec.\n",
      "\tOn aln 90,000 in seq BACT1. Time spent on BACT1 so far: 1,472.09 sec.\n",
      "\tOn aln 95,000 in seq BACT1. Time spent on BACT1 so far: 1,550.89 sec.\n",
      "\tOn aln 100,000 in seq BACT1. Time spent on BACT1 so far: 1,648.30 sec.\n",
      "\tOn aln 105,000 in seq BACT1. Time spent on BACT1 so far: 1,727.84 sec.\n",
      "\tOn aln 110,000 in seq BACT1. Time spent on BACT1 so far: 1,840.32 sec.\n",
      "\tOn aln 115,000 in seq BACT1. Time spent on BACT1 so far: 1,919.61 sec.\n",
      "\tOn aln 120,000 in seq BACT1. Time spent on BACT1 so far: 2,002.23 sec.\n",
      "\tOn aln 125,000 in seq BACT1. Time spent on BACT1 so far: 2,114.81 sec.\n",
      "\tOn aln 130,000 in seq BACT1. Time spent on BACT1 so far: 2,240.83 sec.\n",
      "\tOn aln 135,000 in seq BACT1. Time spent on BACT1 so far: 2,319.06 sec.\n",
      "\tOn aln 140,000 in seq BACT1. Time spent on BACT1 so far: 2,378.29 sec.\n",
      "\tOn aln 145,000 in seq BACT1. Time spent on BACT1 so far: 2,441.23 sec.\n",
      "\tOn aln 150,000 in seq BACT1. Time spent on BACT1 so far: 2,510.58 sec.\n",
      "\tOn aln 155,000 in seq BACT1. Time spent on BACT1 so far: 2,605.09 sec.\n",
      "\tOn aln 160,000 in seq BACT1. Time spent on BACT1 so far: 2,733.17 sec.\n",
      "\tOn aln 165,000 in seq BACT1. Time spent on BACT1 so far: 2,819.28 sec.\n",
      "\tOn aln 170,000 in seq BACT1. Time spent on BACT1 so far: 2,949.99 sec.\n",
      "\tOn aln 175,000 in seq BACT1. Time spent on BACT1 so far: 3,052.04 sec.\n",
      "\tOn aln 180,000 in seq BACT1. Time spent on BACT1 so far: 3,132.10 sec.\n",
      "\tOn aln 185,000 in seq BACT1. Time spent on BACT1 so far: 3,220.22 sec.\n",
      "\tOn aln 190,000 in seq BACT1. Time spent on BACT1 so far: 3,313.17 sec.\n",
      "\tOn aln 195,000 in seq BACT1. Time spent on BACT1 so far: 3,413.24 sec.\n",
      "\tOn aln 200,000 in seq BACT1. Time spent on BACT1 so far: 3,534.09 sec.\n",
      "\tOn aln 205,000 in seq BACT1. Time spent on BACT1 so far: 3,608.54 sec.\n",
      "\tOn aln 210,000 in seq BACT1. Time spent on BACT1 so far: 3,699.66 sec.\n",
      "\tOn aln 215,000 in seq BACT1. Time spent on BACT1 so far: 3,829.21 sec.\n",
      "\tOn aln 220,000 in seq BACT1. Time spent on BACT1 so far: 3,928.00 sec.\n",
      "\tOn aln 225,000 in seq BACT1. Time spent on BACT1 so far: 4,031.16 sec.\n",
      "\tOn aln 230,000 in seq BACT1. Time spent on BACT1 so far: 4,114.70 sec.\n",
      "\tOn aln 235,000 in seq BACT1. Time spent on BACT1 so far: 4,186.84 sec.\n",
      "\tOn aln 240,000 in seq BACT1. Time spent on BACT1 so far: 4,294.14 sec.\n",
      "\tOn aln 245,000 in seq BACT1. Time spent on BACT1 so far: 4,420.70 sec.\n",
      "\tOn aln 250,000 in seq BACT1. Time spent on BACT1 so far: 4,531.77 sec.\n",
      "\tOn aln 255,000 in seq BACT1. Time spent on BACT1 so far: 4,612.27 sec.\n",
      "\tOn aln 260,000 in seq BACT1. Time spent on BACT1 so far: 4,733.68 sec.\n",
      "We ignored 8,699 linear alignments, fyi.\n",
      "For reference, there are 2,035 uncovered positions in BACT1.\n",
      "And there are 2 \"runs\" of uncovered positions.\n",
      "Adding \"virtual reads\" spanning each of these runs, at rounded mean coverage of 1,415x, to account for this...\n",
      "Wrote out 2,830 virtual reads.\n",
      "Done with edge_1671! Took 4,763.11 sec.\n",
      "======================================================================\n",
      "Identifying mutated positions (p = 1%) in genome BACT2...\n",
      "Found 372 mutated positions (p = 1%) in BACT2.\n",
      "Note that this tally is higher than you'd see in e.g. the CP1/2/3 plots, because now we're including both 'rare' and non-rare mutations. Just so you don't waste five minutes sanity-checking this like I did.\n",
      "Going through these positions...\n",
      "\tOn aln 5,000 in seq BACT2. Time spent on BACT2 so far: 19.99 sec.\n",
      "\tOn aln 10,000 in seq BACT2. Time spent on BACT2 so far: 45.98 sec.\n",
      "\tOn aln 15,000 in seq BACT2. Time spent on BACT2 so far: 71.46 sec.\n",
      "\tOn aln 20,000 in seq BACT2. Time spent on BACT2 so far: 98.05 sec.\n",
      "\tOn aln 25,000 in seq BACT2. Time spent on BACT2 so far: 126.26 sec.\n",
      "\tOn aln 30,000 in seq BACT2. Time spent on BACT2 so far: 152.26 sec.\n",
      "\tOn aln 35,000 in seq BACT2. Time spent on BACT2 so far: 176.25 sec.\n",
      "\tOn aln 40,000 in seq BACT2. Time spent on BACT2 so far: 200.83 sec.\n",
      "\tOn aln 45,000 in seq BACT2. Time spent on BACT2 so far: 224.53 sec.\n",
      "\tOn aln 50,000 in seq BACT2. Time spent on BACT2 so far: 250.11 sec.\n",
      "\tOn aln 55,000 in seq BACT2. Time spent on BACT2 so far: 275.41 sec.\n",
      "\tOn aln 60,000 in seq BACT2. Time spent on BACT2 so far: 299.62 sec.\n",
      "\tOn aln 65,000 in seq BACT2. Time spent on BACT2 so far: 322.84 sec.\n",
      "\tOn aln 70,000 in seq BACT2. Time spent on BACT2 so far: 347.15 sec.\n",
      "\tOn aln 75,000 in seq BACT2. Time spent on BACT2 so far: 371.48 sec.\n",
      "\tOn aln 80,000 in seq BACT2. Time spent on BACT2 so far: 396.80 sec.\n",
      "\tOn aln 85,000 in seq BACT2. Time spent on BACT2 so far: 420.12 sec.\n",
      "\tOn aln 90,000 in seq BACT2. Time spent on BACT2 so far: 447.66 sec.\n",
      "\tOn aln 95,000 in seq BACT2. Time spent on BACT2 so far: 482.19 sec.\n",
      "\tOn aln 100,000 in seq BACT2. Time spent on BACT2 so far: 505.68 sec.\n",
      "\tOn aln 105,000 in seq BACT2. Time spent on BACT2 so far: 530.84 sec.\n",
      "\tOn aln 110,000 in seq BACT2. Time spent on BACT2 so far: 555.16 sec.\n",
      "\tOn aln 115,000 in seq BACT2. Time spent on BACT2 so far: 579.74 sec.\n",
      "\tOn aln 120,000 in seq BACT2. Time spent on BACT2 so far: 602.72 sec.\n",
      "\tOn aln 125,000 in seq BACT2. Time spent on BACT2 so far: 626.30 sec.\n",
      "\tOn aln 130,000 in seq BACT2. Time spent on BACT2 so far: 655.79 sec.\n",
      "\tOn aln 135,000 in seq BACT2. Time spent on BACT2 so far: 677.77 sec.\n",
      "\tOn aln 140,000 in seq BACT2. Time spent on BACT2 so far: 700.06 sec.\n",
      "\tOn aln 145,000 in seq BACT2. Time spent on BACT2 so far: 721.13 sec.\n",
      "\tOn aln 150,000 in seq BACT2. Time spent on BACT2 so far: 742.20 sec.\n",
      "\tOn aln 155,000 in seq BACT2. Time spent on BACT2 so far: 762.75 sec.\n",
      "\tOn aln 160,000 in seq BACT2. Time spent on BACT2 so far: 783.62 sec.\n",
      "\tOn aln 165,000 in seq BACT2. Time spent on BACT2 so far: 804.56 sec.\n",
      "\tOn aln 170,000 in seq BACT2. Time spent on BACT2 so far: 825.26 sec.\n",
      "\tOn aln 175,000 in seq BACT2. Time spent on BACT2 so far: 846.63 sec.\n",
      "\tOn aln 180,000 in seq BACT2. Time spent on BACT2 so far: 867.32 sec.\n",
      "\tOn aln 185,000 in seq BACT2. Time spent on BACT2 so far: 886.79 sec.\n",
      "\tOn aln 190,000 in seq BACT2. Time spent on BACT2 so far: 906.41 sec.\n",
      "\tOn aln 195,000 in seq BACT2. Time spent on BACT2 so far: 927.48 sec.\n",
      "\tOn aln 200,000 in seq BACT2. Time spent on BACT2 so far: 947.66 sec.\n",
      "\tOn aln 205,000 in seq BACT2. Time spent on BACT2 so far: 966.79 sec.\n",
      "\tOn aln 210,000 in seq BACT2. Time spent on BACT2 so far: 985.70 sec.\n",
      "\tOn aln 215,000 in seq BACT2. Time spent on BACT2 so far: 1,004.28 sec.\n",
      "\tOn aln 220,000 in seq BACT2. Time spent on BACT2 so far: 1,022.96 sec.\n",
      "\tOn aln 225,000 in seq BACT2. Time spent on BACT2 so far: 1,042.87 sec.\n",
      "\tOn aln 230,000 in seq BACT2. Time spent on BACT2 so far: 1,063.00 sec.\n",
      "\tOn aln 235,000 in seq BACT2. Time spent on BACT2 so far: 1,082.33 sec.\n",
      "\tOn aln 240,000 in seq BACT2. Time spent on BACT2 so far: 1,101.23 sec.\n",
      "\tOn aln 245,000 in seq BACT2. Time spent on BACT2 so far: 1,120.26 sec.\n",
      "\tOn aln 250,000 in seq BACT2. Time spent on BACT2 so far: 1,139.82 sec.\n",
      "\tOn aln 255,000 in seq BACT2. Time spent on BACT2 so far: 1,158.59 sec.\n",
      "\tOn aln 260,000 in seq BACT2. Time spent on BACT2 so far: 1,179.17 sec.\n",
      "\tOn aln 265,000 in seq BACT2. Time spent on BACT2 so far: 1,197.16 sec.\n",
      "\tOn aln 270,000 in seq BACT2. Time spent on BACT2 so far: 1,216.38 sec.\n",
      "\tOn aln 275,000 in seq BACT2. Time spent on BACT2 so far: 1,234.92 sec.\n",
      "\tOn aln 280,000 in seq BACT2. Time spent on BACT2 so far: 1,253.65 sec.\n",
      "\tOn aln 285,000 in seq BACT2. Time spent on BACT2 so far: 1,272.10 sec.\n",
      "\tOn aln 290,000 in seq BACT2. Time spent on BACT2 so far: 1,291.64 sec.\n",
      "\tOn aln 295,000 in seq BACT2. Time spent on BACT2 so far: 1,310.96 sec.\n",
      "\tOn aln 300,000 in seq BACT2. Time spent on BACT2 so far: 1,329.75 sec.\n",
      "\tOn aln 305,000 in seq BACT2. Time spent on BACT2 so far: 1,349.41 sec.\n",
      "\tOn aln 310,000 in seq BACT2. Time spent on BACT2 so far: 1,368.43 sec.\n",
      "\tOn aln 315,000 in seq BACT2. Time spent on BACT2 so far: 1,387.81 sec.\n",
      "\tOn aln 320,000 in seq BACT2. Time spent on BACT2 so far: 1,407.10 sec.\n",
      "\tOn aln 325,000 in seq BACT2. Time spent on BACT2 so far: 1,425.55 sec.\n",
      "\tOn aln 330,000 in seq BACT2. Time spent on BACT2 so far: 1,445.23 sec.\n",
      "\tOn aln 335,000 in seq BACT2. Time spent on BACT2 so far: 1,464.25 sec.\n",
      "\tOn aln 340,000 in seq BACT2. Time spent on BACT2 so far: 1,482.93 sec.\n",
      "\tOn aln 345,000 in seq BACT2. Time spent on BACT2 so far: 1,502.15 sec.\n",
      "\tOn aln 350,000 in seq BACT2. Time spent on BACT2 so far: 1,520.30 sec.\n",
      "\tOn aln 355,000 in seq BACT2. Time spent on BACT2 so far: 1,539.24 sec.\n",
      "\tOn aln 360,000 in seq BACT2. Time spent on BACT2 so far: 1,557.50 sec.\n",
      "\tOn aln 365,000 in seq BACT2. Time spent on BACT2 so far: 1,576.57 sec.\n",
      "\tOn aln 370,000 in seq BACT2. Time spent on BACT2 so far: 1,595.58 sec.\n",
      "\tOn aln 375,000 in seq BACT2. Time spent on BACT2 so far: 1,614.80 sec.\n",
      "\tOn aln 380,000 in seq BACT2. Time spent on BACT2 so far: 1,633.70 sec.\n",
      "\tOn aln 385,000 in seq BACT2. Time spent on BACT2 so far: 1,652.61 sec.\n",
      "\tOn aln 390,000 in seq BACT2. Time spent on BACT2 so far: 1,671.73 sec.\n",
      "\tOn aln 395,000 in seq BACT2. Time spent on BACT2 so far: 1,690.46 sec.\n",
      "\tOn aln 400,000 in seq BACT2. Time spent on BACT2 so far: 1,709.99 sec.\n",
      "\tOn aln 405,000 in seq BACT2. Time spent on BACT2 so far: 1,729.92 sec.\n",
      "\tOn aln 410,000 in seq BACT2. Time spent on BACT2 so far: 1,748.56 sec.\n",
      "\tOn aln 415,000 in seq BACT2. Time spent on BACT2 so far: 1,770.61 sec.\n",
      "\tOn aln 420,000 in seq BACT2. Time spent on BACT2 so far: 1,794.07 sec.\n",
      "\tOn aln 425,000 in seq BACT2. Time spent on BACT2 so far: 1,816.05 sec.\n",
      "\tOn aln 430,000 in seq BACT2. Time spent on BACT2 so far: 1,837.13 sec.\n",
      "\tOn aln 435,000 in seq BACT2. Time spent on BACT2 so far: 1,860.79 sec.\n",
      "\tOn aln 440,000 in seq BACT2. Time spent on BACT2 so far: 1,881.73 sec.\n",
      "\tOn aln 445,000 in seq BACT2. Time spent on BACT2 so far: 1,902.35 sec.\n",
      "\tOn aln 450,000 in seq BACT2. Time spent on BACT2 so far: 1,924.10 sec.\n",
      "\tOn aln 455,000 in seq BACT2. Time spent on BACT2 so far: 1,944.49 sec.\n",
      "\tOn aln 460,000 in seq BACT2. Time spent on BACT2 so far: 1,963.75 sec.\n",
      "\tOn aln 465,000 in seq BACT2. Time spent on BACT2 so far: 1,983.54 sec.\n",
      "\tOn aln 470,000 in seq BACT2. Time spent on BACT2 so far: 2,003.56 sec.\n",
      "\tOn aln 475,000 in seq BACT2. Time spent on BACT2 so far: 2,022.84 sec.\n",
      "\tOn aln 480,000 in seq BACT2. Time spent on BACT2 so far: 2,042.78 sec.\n",
      "\tOn aln 485,000 in seq BACT2. Time spent on BACT2 so far: 2,063.04 sec.\n",
      "\tOn aln 490,000 in seq BACT2. Time spent on BACT2 so far: 2,082.23 sec.\n",
      "\tOn aln 495,000 in seq BACT2. Time spent on BACT2 so far: 2,101.74 sec.\n",
      "\tOn aln 500,000 in seq BACT2. Time spent on BACT2 so far: 2,122.43 sec.\n",
      "\tOn aln 505,000 in seq BACT2. Time spent on BACT2 so far: 2,142.87 sec.\n",
      "\tOn aln 510,000 in seq BACT2. Time spent on BACT2 so far: 2,165.44 sec.\n",
      "\tOn aln 515,000 in seq BACT2. Time spent on BACT2 so far: 2,187.37 sec.\n",
      "\tOn aln 520,000 in seq BACT2. Time spent on BACT2 so far: 2,208.28 sec.\n",
      "\tOn aln 525,000 in seq BACT2. Time spent on BACT2 so far: 2,229.69 sec.\n",
      "\tOn aln 530,000 in seq BACT2. Time spent on BACT2 so far: 2,251.86 sec.\n",
      "\tOn aln 535,000 in seq BACT2. Time spent on BACT2 so far: 2,271.05 sec.\n",
      "\tOn aln 540,000 in seq BACT2. Time spent on BACT2 so far: 2,290.76 sec.\n",
      "\tOn aln 545,000 in seq BACT2. Time spent on BACT2 so far: 2,310.22 sec.\n",
      "\tOn aln 550,000 in seq BACT2. Time spent on BACT2 so far: 2,329.78 sec.\n",
      "\tOn aln 555,000 in seq BACT2. Time spent on BACT2 so far: 2,350.10 sec.\n",
      "\tOn aln 560,000 in seq BACT2. Time spent on BACT2 so far: 2,371.66 sec.\n",
      "\tOn aln 565,000 in seq BACT2. Time spent on BACT2 so far: 2,392.39 sec.\n",
      "\tOn aln 570,000 in seq BACT2. Time spent on BACT2 so far: 2,411.76 sec.\n",
      "\tOn aln 575,000 in seq BACT2. Time spent on BACT2 so far: 2,432.08 sec.\n",
      "\tOn aln 580,000 in seq BACT2. Time spent on BACT2 so far: 2,453.32 sec.\n",
      "\tOn aln 585,000 in seq BACT2. Time spent on BACT2 so far: 2,472.14 sec.\n",
      "\tOn aln 590,000 in seq BACT2. Time spent on BACT2 so far: 2,490.32 sec.\n",
      "\tOn aln 595,000 in seq BACT2. Time spent on BACT2 so far: 2,509.26 sec.\n",
      "\tOn aln 600,000 in seq BACT2. Time spent on BACT2 so far: 2,528.40 sec.\n",
      "\tOn aln 605,000 in seq BACT2. Time spent on BACT2 so far: 2,547.86 sec.\n",
      "\tOn aln 610,000 in seq BACT2. Time spent on BACT2 so far: 2,567.00 sec.\n",
      "\tOn aln 615,000 in seq BACT2. Time spent on BACT2 so far: 2,587.04 sec.\n",
      "\tOn aln 620,000 in seq BACT2. Time spent on BACT2 so far: 2,606.11 sec.\n",
      "\tOn aln 625,000 in seq BACT2. Time spent on BACT2 so far: 2,625.13 sec.\n",
      "\tOn aln 630,000 in seq BACT2. Time spent on BACT2 so far: 2,644.18 sec.\n",
      "\tOn aln 635,000 in seq BACT2. Time spent on BACT2 so far: 2,663.48 sec.\n",
      "\tOn aln 640,000 in seq BACT2. Time spent on BACT2 so far: 2,682.57 sec.\n",
      "\tOn aln 645,000 in seq BACT2. Time spent on BACT2 so far: 2,703.06 sec.\n",
      "\tOn aln 650,000 in seq BACT2. Time spent on BACT2 so far: 2,720.97 sec.\n",
      "\tOn aln 655,000 in seq BACT2. Time spent on BACT2 so far: 2,740.97 sec.\n",
      "\tOn aln 660,000 in seq BACT2. Time spent on BACT2 so far: 2,762.49 sec.\n",
      "\tOn aln 665,000 in seq BACT2. Time spent on BACT2 so far: 2,782.63 sec.\n",
      "\tOn aln 670,000 in seq BACT2. Time spent on BACT2 so far: 2,800.51 sec.\n",
      "\tOn aln 675,000 in seq BACT2. Time spent on BACT2 so far: 2,820.18 sec.\n",
      "\tOn aln 680,000 in seq BACT2. Time spent on BACT2 so far: 2,839.80 sec.\n",
      "\tOn aln 685,000 in seq BACT2. Time spent on BACT2 so far: 2,862.24 sec.\n",
      "\tOn aln 690,000 in seq BACT2. Time spent on BACT2 so far: 2,883.11 sec.\n",
      "\tOn aln 695,000 in seq BACT2. Time spent on BACT2 so far: 2,907.06 sec.\n",
      "\tOn aln 700,000 in seq BACT2. Time spent on BACT2 so far: 2,925.75 sec.\n",
      "\tOn aln 705,000 in seq BACT2. Time spent on BACT2 so far: 2,944.95 sec.\n",
      "\tOn aln 710,000 in seq BACT2. Time spent on BACT2 so far: 2,963.79 sec.\n",
      "\tOn aln 715,000 in seq BACT2. Time spent on BACT2 so far: 2,982.09 sec.\n",
      "\tOn aln 720,000 in seq BACT2. Time spent on BACT2 so far: 3,001.82 sec.\n",
      "\tOn aln 725,000 in seq BACT2. Time spent on BACT2 so far: 3,021.15 sec.\n",
      "\tOn aln 730,000 in seq BACT2. Time spent on BACT2 so far: 3,039.92 sec.\n",
      "\tOn aln 735,000 in seq BACT2. Time spent on BACT2 so far: 3,054.15 sec.\n",
      "\tOn aln 740,000 in seq BACT2. Time spent on BACT2 so far: 3,061.89 sec.\n",
      "We ignored 623 linear alignments, fyi.\n",
      "Done with edge_2358! Took 3,064.24 sec.\n",
      "Time taken: 9,547.733855009079 sec.\n"
     ]
    }
   ],
   "source": [
    "bf = pysam.AlignmentFile(\"../main-workflow/output/fully-filtered-and-sorted-aln.bam\", \"rb\")\n",
    "output_dir = \"phasing-data/smoothed-reads/\"\n",
    "\n",
    "# verbose?\n",
    "no_indoor_voice = False\n",
    "\n",
    "def write_out_reads(filepath, readname2seq):\n",
    "    # Notably, this uses the \"a\" (append) method in order to add to the end of a file\n",
    "    with open(filepath, \"a\") as of:\n",
    "        for readname in readname2seq:\n",
    "            # Write out both the header and the sequence for each read\n",
    "            of.write(f\">{readname}\\n{str(readname2seq[readname])}\\n\")\n",
    "            \n",
    "ALN_UPDATE_FREQ = 5000\n",
    "ALN_BUFFER_FREQ = 1000\n",
    "VR_EXTRA_SPAN = 100\n",
    "\n",
    "P = 1\n",
    "            \n",
    "t1 = time.time()\n",
    "for seq in SEQS:\n",
    "    \n",
    "    # Record which positions (0-indexed) aren't covered by any smoothed reads in this MAG.\n",
    "    # We'll add \"virtual reads\" that span these positions.\n",
    "    uncovered_positions = set(range(0, seq2len[seq]))\n",
    "    \n",
    "    fasta = skbio.DNA.read(f\"../seqs/{seq}.fasta\")\n",
    "    \n",
    "    output_smoothed_reads_file = os.path.join(output_dir, f\"{seq}_smoothed_reads.fasta\")\n",
    "    \n",
    "    # Identify all (0-indexed, so compatible with skbio / pysam!)\n",
    "    # mutated positions in this genome up front to save time.\n",
    "    #\n",
    "    # Equivalently, we could also just take in an arbitrary VCF as input\n",
    "    # (e.g. one produced from another variant calling tool), although we'd\n",
    "    # need to be careful to only include SNVs and not indels/etc...\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Identifying mutated positions (p = {P}%) in genome {seq2name[seq]}...\")\n",
    "    mutpos2pileup = find_mutated_positions(seq, p_to_use=P, incl_pileup=True)\n",
    "    # We sort because the code below relies on these being in ascending order\n",
    "    mutated_positions = sorted(mutpos2pileup.keys())\n",
    "    print(f\"Found {len(mutated_positions):,} mutated positions (p = {P}%) in {seq2name[seq]}.\")\n",
    "    print(\n",
    "        f\"Note that this tally is higher than you'd see in e.g. the CP1/2/3 plots, because now we're including \"\n",
    "        \"both 'rare' and non-rare mutations. Just so you don't waste five minutes sanity-checking this like I did.\"\n",
    "    )\n",
    "    \n",
    "    print(\"Going through these positions...\")\n",
    "    \n",
    "    num_ignored_alns = 0\n",
    "    \n",
    "    # Instead of just writing out every smoothed alignment as soon as we generate it, we build up a \"buffer\"\n",
    "    # of these alignments and then write a bunch out at once. This way we limit slowdown due to constantly\n",
    "    # having to open/close files. I don't really have a good source for this as best practice, but I remembered\n",
    "    # to do it while writing this code, so somewhere in College Park the CS faculty at Maryland are smiling\n",
    "    #\n",
    "    # Also fyi this maps read name to smoothed alignment (well, at this point, just read) sequence. The read name\n",
    "    # is useful to preserve in fasta files so we have some idea of provenance (where smoothed reads came from)\n",
    "    smoothed_aln_buffer = {}\n",
    "    \n",
    "    # The first time we see an alignment of a read, it's 1; if we see a supp aln of this read, it's 2; etc.\n",
    "    # Lets us distinguish alignments with different names\n",
    "    readname2freq_so_far = defaultdict(int)\n",
    "    \n",
    "    # Go through all linear alignments of each read to this genome, focusing (for now) on just the primary\n",
    "    # alignments...\n",
    "    ts1 = time.time()\n",
    "    for ai, aln in enumerate(bf.fetch(seq), 1):\n",
    "        \n",
    "        if ai % ALN_UPDATE_FREQ == 0:\n",
    "            print(\n",
    "                f\"\\tOn aln {ai:,} in seq {seq2name[seq]}. \"\n",
    "                f\"Time spent on {seq2name[seq]} so far: {time.time() - ts1:,.2f} sec.\"\n",
    "            )\n",
    "            \n",
    "        if aln.is_secondary:\n",
    "            raise ValueError(\n",
    "                \"Not to get political or anything, but you should've already filtered secondary alns out\"\n",
    "            )\n",
    "            \n",
    "        # Note that supplementary alignments are ok, though! We implicitly handle these here.\n",
    "        #\n",
    "        # Different alignments of the same read will have different new_readnames, because we're gonna\n",
    "        # be treating them as distinct \"reads\". We should have already filtered reference-overlapping\n",
    "        # supp alns so this shouldn't be a problem\n",
    "        \n",
    "        readname = aln.query_name\n",
    "        readname2freq_so_far[readname] += 1\n",
    "        new_readname = f\"{readname}_{readname2freq_so_far[readname]}\"\n",
    "        \n",
    "        # should never happen\n",
    "        if new_readname in smoothed_aln_buffer:\n",
    "            raise ValueError(\"This exact read alignment has already been smoothed? Weird.\")\n",
    "            \n",
    "        # Figure out where on the MAG this alignment \"hits.\" These are 0-indexed positions from Pysam.\n",
    "        # (reference_end points to the position after the actual final position, since these are designed to\n",
    "        # be interoperable with Python's half-open intervals.)\n",
    "        #\n",
    "        # Of course, there likely will be indels within this range: we're purposefully ignoring those here.\n",
    "        ref_start = aln.reference_start\n",
    "        ref_end = aln.reference_end - 1\n",
    "        \n",
    "        # This should never happen (TM)\n",
    "        if ref_start >= ref_end:\n",
    "            # Du sollst jetzt mit Gott sprechen.\n",
    "            raise ValueError(\n",
    "                f\"Ref start {ref_start:,} >= ref end {ref_end:,} for read {new_readname}?\"\n",
    "            )\n",
    "            \n",
    "        # Smoothed sequence; we'll edit this so that if this read has (mis)matches to any called mutated\n",
    "        # positions, these positions are updated with the read's aligned nucleotides at these positions.\n",
    "        smoothed_aln_seq = fasta[ref_start: ref_end + 1]\n",
    "        \n",
    "        if actually_include_mutations_in_the_smoothed_reads:\n",
    "            # just for debugging: track the exact edits made to smoothed_aln_seq\n",
    "            replacements_made = {}\n",
    "\n",
    "            # We may choose to ignore this linear alignment, if we think it is error-prone or\n",
    "            # otherwise not useful. If this gets set to True in the loop below, then we'll notice this\n",
    "            # and ignore this alignment.\n",
    "            ignoring_this_aln = False\n",
    "            \n",
    "            # Notably, include skips -- this way, we can figure out if the aln has a deletion at a mutated\n",
    "            # position, and if so ignore this aln\n",
    "            ap = aln.get_aligned_pairs(matches_only=False)\n",
    "\n",
    "            # Iterating through the aligned pairs is expensive. Since read lengths are generally in the thousands\n",
    "            # to tens of thousands of bp (which is much less than the > 1 million bp length of any bacterial genome),\n",
    "            # we set things up so that we only iterate through the aligned pairs once. We maintain an integer, mpi,\n",
    "            # that is a poor man's \"pointer\" to an index in mutated_positions.\n",
    "\n",
    "            mpi = 0\n",
    "\n",
    "            # Go through this aln's aligned pairs. As we see each pair, compare the pair's reference position\n",
    "            # (refpos) to the mpi-th mutated position (herein referred to as \"mutpos\").\n",
    "            #\n",
    "            # If refpos >  mutpos, increment mpi until refpos <= mutpos (stopping as early as possible).\n",
    "            # If refpos == mutpos, we have a match! Update readname2mutpos2ismutated[mutpos] based on\n",
    "            #                      comparing the read to the reference at the aligned positions.\n",
    "            # If refpos <  mutpos, continue to the next pair.\n",
    "\n",
    "            for pair in ap:\n",
    "\n",
    "                refpos = pair[1]\n",
    "                \n",
    "                # Since we set matches_only (for get_aligned_pairs()) to False, the alignment could include\n",
    "                # insertions (which are encoded as the reference pos being set to None). We inherently ignore\n",
    "                # these insertions as part of the read smoothing process.\n",
    "                if refpos is None:\n",
    "                    continue\n",
    "                    \n",
    "                mutpos = mutated_positions[mpi]\n",
    "\n",
    "                no_mutations_to_right_of_here = False\n",
    "\n",
    "                # Increment mpi until we get to the next mutated position at or after the reference pos for this\n",
    "                # aligned pair (or until we run out of mutated positions).\n",
    "                while refpos > mutpos:\n",
    "                    mpi += 1\n",
    "                    if mpi < len(mutated_positions):\n",
    "                        mutpos = mutated_positions[mpi]\n",
    "                    else:\n",
    "                        no_mutations_to_right_of_here = True\n",
    "                        break\n",
    "\n",
    "                # I expect this should happen only for reads aligned near the right end of the genome.\n",
    "                if no_mutations_to_right_of_here:\n",
    "                    break\n",
    "\n",
    "                # If the next mutation occurs after this aligned pair, continue on to a later pair.\n",
    "                if refpos < mutpos:\n",
    "                    continue\n",
    "\n",
    "                # If we've made it here, refpos == mutpos!\n",
    "                # (...unless I messed something up in how I designed this code.)\n",
    "                if refpos != mutpos:\n",
    "                    raise ValueError(\"This should never happen!\")\n",
    "\n",
    "                # Finally, get the nucleotide aligned to this mutated position from this read.\n",
    "                readpos = pair[0]\n",
    "                \n",
    "                # Since we set matches_only (for get_aligned_pairs()) to False, there's a chance a read\n",
    "                # contains deletions aligned to mutated positions. This is accounted for by this case.\n",
    "                # If this happens, we ignore this alignment (the same as if it would contain a nt that\n",
    "                # isn't the first or second most common).\n",
    "                if readpos is None:\n",
    "                    if no_indoor_voice:\n",
    "                        print(\n",
    "                            f\"Read {new_readname} has deletion at mutpos {mutpos + 1:,}: \"\n",
    "                            f\"pileup = {mutpos2pileup[mutpos]}\"\n",
    "                        )\n",
    "                    ignoring_this_aln = True\n",
    "                    break\n",
    "                    \n",
    "                read_nt = aln.query_sequence[readpos]\n",
    "\n",
    "                # If this aln doesn't have match the first or second most common nucleotide at this position,\n",
    "                # ignore it. In the future, when we perform read  smoothing based on an arbitrary set of\n",
    "                # SNV calls, we can be more careful about this; but for now we make the simplifiying assumption\n",
    "                # that a mutation likely only has one alternate nucleotide, and that the 3rd and 4th most\n",
    "                # common nucleotides indicate errors. (Also, note that we break ties here arbitrarily.)\n",
    "                nt2ct = dict(zip(\"ACGT\", mutpos2pileup[mutpos][0]))\n",
    "                nt1 = max(nt2ct, key=nt2ct.get)\n",
    "                del nt2ct[nt1]\n",
    "                nt2 = max(nt2ct, key=nt2ct.get)\n",
    "                \n",
    "                if read_nt != nt1 and read_nt != nt2:\n",
    "                    if no_indoor_voice:\n",
    "                        print(\n",
    "                            f\"Read {new_readname} has 3rd or 4th most common nt at mutpos {mutpos + 1:,}: \"\n",
    "                            f\"pileup = {mutpos2pileup[mutpos]}, read = {read_nt}\"\n",
    "                        )\n",
    "                    ignoring_this_aln = True\n",
    "                    break\n",
    "                    \n",
    "                # Notably, the nucleotide at a mutated position in a smoothed read will always be the first\n",
    "                # or second most common nucleotide at this position. So \"unreasonable\" positions, in which\n",
    "                # the ref nt != the consensus nt, will not be treated as you might expect -- we ignore the\n",
    "                # reference in this particular case. Shouldn't make a big difference, since in most cases\n",
    "                # the ref and consensus nt agree.\n",
    "                relative_pos_on_aln = mutpos - ref_start\n",
    "                smoothed_aln_seq = smoothed_aln_seq.replace([relative_pos_on_aln], read_nt)\n",
    "                replacements_made[relative_pos_on_aln] = read_nt\n",
    "                if no_indoor_voice:\n",
    "                    print(\n",
    "                        f\"Read {new_readname} mismatches ref at mutpos {mutpos + 1:,}: \"\n",
    "                        f\"ref = {ref_nt}, read = {read_nt}\"\n",
    "                    )\n",
    "\n",
    "            if no_indoor_voice:\n",
    "                print(f\"Read {new_readname} required {len(replacements_made):,} replacements!\")\n",
    "        \n",
    "        if ignoring_this_aln:\n",
    "            num_ignored_alns += 1\n",
    "        else:\n",
    "            # Now that we've finished processing all called mutations that this alignment spans, prepare it\n",
    "            # to be written out to a FASTA file. See comments above on smoothed_aln_buffer, and why we don't\n",
    "            # just write everything out as soon as it's ready.\n",
    "            #\n",
    "            # (Also, we've already guaranteed readname isn't already in smoothed_aln_buffer, so no need to worry\n",
    "            # about accidentally overwriting something from earlier.)\n",
    "            smoothed_aln_buffer[new_readname] = smoothed_aln_seq\n",
    "\n",
    "            # Record which positions this read covers (of course, it may not exactly \"cover\" these positions\n",
    "            # originally due to indels, but the smoothed version will cover them).\n",
    "            # We don't update uncovered_positions until *after* we process all aligned pairs of this read, to allow\n",
    "            # us to ignore reads if desired.\n",
    "            uncovered_positions -= set(range(ref_start, ref_end + 1))\n",
    "\n",
    "            if ai % ALN_BUFFER_FREQ == 0:\n",
    "                write_out_reads(output_smoothed_reads_file, smoothed_aln_buffer)\n",
    "                # Clear the buffer\n",
    "                smoothed_aln_buffer = {}\n",
    "        \n",
    "    # We're probably going to have left over smoothed reads that we still haven't written out, unless things\n",
    "    # worked out so that on the final alignment we saw ai was exactly divisible by ALN_BUFFER_FREQ (and that's\n",
    "    # pretty unlikely unless you set the buffer freq to a low number). So make one last dump of the buffer.\n",
    "    if len(smoothed_aln_buffer) > 0:\n",
    "        write_out_reads(output_smoothed_reads_file, smoothed_aln_buffer)\n",
    "        \n",
    "    print(f\"We ignored {num_ignored_alns:,} linear alignments, fyi.\")\n",
    "        \n",
    "    if add_virtual_reads and len(uncovered_positions) > 0:\n",
    "        print(f\"For reference, there are {len(uncovered_positions):,} uncovered positions in {seq2name[seq]}.\")\n",
    "        \n",
    "        sup = sorted(uncovered_positions)\n",
    "        uc_runs = convert_to_runs(sup)\n",
    "        print(f'And there are {len(uc_runs)} \"runs\" of uncovered positions.')\n",
    "        \n",
    "        rounded_meancov = round(seq2meancov[seq])\n",
    "        print(\n",
    "            f'Adding \"virtual reads\" spanning each of these runs, at rounded mean coverage '\n",
    "            f'of {rounded_meancov:,}x, to account for this...'\n",
    "        )\n",
    "        \n",
    "        num_vr = 0\n",
    "        vr_buffer = {}\n",
    "        for run in uc_runs:\n",
    "            # Construct a virtual read that includes this entire run of uncovered positions as well\n",
    "            # as VR_EXTRA_SPAN positions before and after (clamping to the start/end of the seq if needed).\n",
    "            #\n",
    "            # Notably, we could try to make this loop around from end -> start if this is a cyclic MAG, but\n",
    "            # to remain consistent with how we handle supplementary alignments above -- and because implementing\n",
    "            # the loop around would be a lot of work and it's like 3am -- we ignore this for now.\n",
    "            #\n",
    "            # Also, note that run_start can equal run_end, if only a single isolated position is uncovered.\n",
    "            # This is fine -- the code handles this case automatically. (I guess the only potential problem is\n",
    "            # if the length of the MAG is less than VR_EXTRA_SPAN, but... that should never happen. If you have\n",
    "            # like 100bp-long MAGs that's a problem! I guess, TODO, make note of this when generalizing this\n",
    "            # code.)\n",
    "            \n",
    "            run_start = max(run[0] - VR_EXTRA_SPAN, 0)\n",
    "            run_end = min(run[1] + VR_EXTRA_SPAN, seq2len[seq] - 1)\n",
    "            \n",
    "            # Generate a sequence matching the \"reference\" MAG at these positions. We of course don't have\n",
    "            # any info about mutations here, because these positions are uncovered by the real reads!\n",
    "            vr_seq = fasta[run_start: run_end + 1]\n",
    "            \n",
    "            # We need to assign reads unique names, and including the run coordinates here is a nice way\n",
    "            # to preserve uniqueness across runs and also make our smoothed reads files easier to interpret\n",
    "            vr_name_prefix = f\"vr_{run[0]}_{run[1]}\"\n",
    "            \n",
    "            # Add M copies of this virtual read, where M = (rounded mean coverage of this MAG)\n",
    "            for vr_num in range(1, rounded_meancov + 1):\n",
    "                vr_name = f\"{vr_name_prefix}_{vr_num}\"\n",
    "                vr_buffer[vr_name] = vr_seq\n",
    "                num_vr += 1\n",
    "                \n",
    "        write_out_reads(output_smoothed_reads_file, vr_buffer)\n",
    "        print(f\"Wrote out {num_vr:,} virtual reads.\")\n",
    "    \n",
    "    print(f\"Done with {seq}! Took {time.time() - ts1:,.2f} sec.\")\n",
    "        \n",
    "print(f\"Time taken: {time.time() - t1:,} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assemble these smoothed reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T12:49:09.786436Z",
     "iopub.status.busy": "2022-03-21T12:49:09.783653Z",
     "iopub.status.idle": "2022-03-21T13:05:14.241393Z",
     "shell.execute_reply": "2022-03-21T13:05:14.242134Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:00 2Mb  INFO: Hello! You are running La Jolla Assembler (LJA), a tool for genome assembly from PacBio HiFi reads\n",
      "00:00:19 2Mb  INFO: eaeb8812405afe9b8ffa8ded423178299ee296c2\n",
      "00:00:19 5Mb  INFO: LJA pipeline started\n",
      "00:00:19 5Mb  INFO: Performing initial correction with k = 5001\n",
      "00:00:19 0Mb  INFO: Reading reads\n",
      "00:00:19 0Mb  INFO: Extracting minimizers\n",
      "00:00:36 3.4Gb  INFO: Finished read processing\n",
      "00:00:36 3.4Gb  INFO: 5254841 hashs collected. Starting sorting.\n",
      "00:00:36 3.4Gb  INFO: Finished sorting. Total distinct minimizers: 4301\n",
      "00:00:36 3.4Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:36 3.4Gb  INFO: Vertex map constructed.\n",
      "00:00:36 3.4Gb  INFO: Filling edge sequences.\n",
      "00:00:53 4.5Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:00:53 4.5Gb  INFO:  Collecting tips \n",
      "00:00:53 4.5Gb  INFO: Added 49 artificial minimizers from tips.\n",
      "00:00:53 4.5Gb  INFO: Collected 8642 old edges.\n",
      "00:00:53 4.5Gb  INFO: New minimizers added to sparse graph.\n",
      "00:00:53 4.5Gb  INFO: Refilling graph with old edges.\n",
      "00:00:53 4.5Gb  INFO: Filling graph with new edges.\n",
      "00:00:53 4.5Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:00:53 4.5Gb  INFO: Starting to extract disjointigs.\n",
      "00:00:53 4.5Gb  INFO: Finished extracting 127 disjointigs of total size 1725911\n",
      "00:00:54 0Mb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/k5001/disjointigs.fasta\"\n",
      "00:00:54 8Mb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:00:54 9Mb  INFO: Filled 5049531 bits out of 34905088\n",
      "00:00:54 9Mb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:00:55 9Mb  INFO: Collected 534 junctions.\n",
      "00:00:55 9Mb  INFO: Starting DBG construction.\n",
      "00:00:55 9Mb  INFO: Vertices created.\n",
      "00:00:55 9Mb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:00:55 9Mb  INFO: Added 0 hanging vertices\n",
      "00:00:55 9Mb  INFO: Merging unbranching paths\n",
      "00:00:55 9Mb  INFO: Ended merging edges. Resulting size 138\n",
      "00:00:55 9Mb  INFO: Cleaning edge coverages\n",
      "00:00:55 9Mb  INFO: Collecting alignments of sequences to the graph\n",
      "00:01:09 4.5Gb  INFO: Alignment collection finished. Total length of alignments is 496777\n",
      "00:01:09 4.5Gb  INFO: Could not correct 98 reads. They will be removed.\n",
      "00:01:09 4.5Gb  INFO: Uncorrected reads were removed.\n",
      "00:01:09 4.5Gb  INFO: Applying changes to the graph\n",
      "00:01:12 4.6Gb  INFO: Printing reads to fasta file \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/k5001/corrected_reads.fasta\"\n",
      "00:01:42 5Mb  INFO: Performing repeat resolution by transforming de Bruijn graph into Multiplex de Bruijn graph\n",
      "00:01:42 0Mb  INFO: Loading graph from fasta\n",
      "00:01:43 7Mb  INFO: Finished loading graph\n",
      "00:01:43 156Mb  INFO: Looking for unique edges\n",
      "00:01:43 156Mb  INFO: Marked 18 long edges as unique\n",
      "00:01:43 156Mb  INFO: Marking extra edges as unique based on read paths\n",
      "00:01:43 156Mb  INFO: Marked 18 edges as unique\n",
      "00:01:43 156Mb  INFO: Splitting graph with unique edges\n",
      "00:01:43 156Mb  INFO: Processing 11 components\n",
      "00:01:43 156Mb  INFO: Finished unique edges search. Found 68 unique edges\n",
      "00:01:43 156Mb  INFO: Analysing repeats of multiplicity 2 and looking for additional unique edges\n",
      "00:01:43 156Mb  INFO: Finished processing of repeats of multiplicity 2. Found 0 erroneous edges.\n",
      "00:01:43 156Mb  INFO: Resolving repeats\n",
      "00:01:43 156Mb  INFO: Constructing paths\n",
      "00:01:47 314Mb  INFO: Building graph\n",
      "00:01:47 314Mb  INFO: Increasing k\n",
      "00:01:47 314Mb  INFO: Finished increasing k\n",
      "00:01:47 314Mb  INFO: Exporting remaining active transitions\n",
      "00:01:47 314Mb  INFO: Export to Dot\n",
      "00:01:47 314Mb  INFO: Export to GFA and compressed contigs\n",
      "00:01:47 319Mb  INFO: Finished repeat resolution\n",
      "00:01:48 5Mb  INFO: Performing polishing and homopolymer uncompression\n",
      "00:01:48 6Mb  INFO: Aligning reads back to assembly\n",
      "00:02:05 4.1Gb  INFO: Finished alignment.\n",
      "00:02:05 4.1Gb  INFO: Printing alignments to \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/uncompressing/alignments.txt\"\n",
      "00:02:06 4.1Gb  INFO: Reading and processing initial reads from [\"phasing-data/smoothed-reads/edge_6104_smoothed_reads.fasta\"]\n",
      "00:04:48 4.9Gb  INFO: Uncompressing homopolymers in contigs\n",
      "00:04:49 4.9Gb  INFO: Total zero covered nucleotides 0\n",
      "00:04:49 4.9Gb  INFO: Calculating overlaps between adjacent uncompressed edges\n",
      "00:04:49 4.9Gb  INFO: Printing final gfa file to \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/mdbg.gfa\"\n",
      "00:04:49 4.9Gb  INFO: Printing final assembly to \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/assembly.fasta\"\n",
      "00:04:49 5Mb  INFO: Final homopolymer compressed and corrected reads can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/k5001/corrected_reads.fasta\"\n",
      "00:04:49 5Mb  INFO: Final graph with homopolymer compressed edges can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/mdbg/mdbg.hpc.gfa\"\n",
      "00:04:49 5Mb  INFO: Final graph can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/mdbg.gfa\"\n",
      "00:04:49 5Mb  INFO: Final assembly can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/assembly.fasta\"\n",
      "00:04:49 5Mb  INFO: LJA pipeline finished\n",
      "00:00:00 2Mb  INFO: Hello! You are running La Jolla Assembler (LJA), a tool for genome assembly from PacBio HiFi reads\n",
      "00:00:19 2Mb  INFO: eaeb8812405afe9b8ffa8ded423178299ee296c2\n",
      "00:00:19 5Mb  INFO: LJA pipeline started\n",
      "00:00:19 5Mb  INFO: Performing initial correction with k = 5001\n",
      "00:00:19 0Mb  INFO: Reading reads\n",
      "00:00:19 0Mb  INFO: Extracting minimizers\n",
      "00:00:29 3Gb  INFO: Finished read processing\n",
      "00:00:29 3Gb  INFO: 3126630 hashs collected. Starting sorting.\n",
      "00:00:30 3Gb  INFO: Finished sorting. Total distinct minimizers: 41410\n",
      "00:00:30 3Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:30 3Gb  INFO: Vertex map constructed.\n",
      "00:00:30 3Gb  INFO: Filling edge sequences.\n",
      "00:00:41 3.1Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:00:41 3.1Gb  INFO:  Collecting tips \n",
      "00:00:41 3.1Gb  INFO: Added 3489 artificial minimizers from tips.\n",
      "00:00:41 3.1Gb  INFO: Collected 81674 old edges.\n",
      "00:00:41 3.1Gb  INFO: New minimizers added to sparse graph.\n",
      "00:00:41 3.1Gb  INFO: Refilling graph with old edges.\n",
      "00:00:41 3.1Gb  INFO: Filling graph with new edges.\n",
      "00:00:41 3.1Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:00:42 3.1Gb  INFO: Starting to extract disjointigs.\n",
      "00:00:42 3.1Gb  INFO: Finished extracting 5099 disjointigs of total size 36228569\n",
      "00:00:42 0Mb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/k5001/disjointigs.fasta\"\n",
      "00:00:43 55Mb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:00:45 56Mb  INFO: Filled 49640419 bits out of 343311040\n",
      "00:00:45 56Mb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:00:46 56Mb  INFO: Collected 9836 junctions.\n",
      "00:00:46 56Mb  INFO: Starting DBG construction.\n",
      "00:00:46 56Mb  INFO: Vertices created.\n",
      "00:00:46 56Mb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:00:46 56Mb  INFO: Added 0 hanging vertices\n",
      "00:00:46 56Mb  INFO: Merging unbranching paths\n",
      "00:00:46 56Mb  INFO: Ended merging edges. Resulting size 5848\n",
      "00:00:46 56Mb  INFO: Cleaning edge coverages\n",
      "00:00:46 56Mb  INFO: Collecting alignments of sequences to the graph\n",
      "00:00:52 3Gb  INFO: Alignment collection finished. Total length of alignments is 730337\n",
      "00:00:53 3Gb  INFO: Could not correct 6408 reads. They will be removed.\n",
      "00:00:53 3Gb  INFO: Uncorrected reads were removed.\n",
      "00:00:53 3Gb  INFO: Applying changes to the graph\n",
      "00:00:56 3Gb  INFO: Printing reads to fasta file \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/k5001/corrected_reads.fasta\"\n",
      "00:01:13 5Mb  INFO: Performing repeat resolution by transforming de Bruijn graph into Multiplex de Bruijn graph\n",
      "00:01:13 0Mb  INFO: Loading graph from fasta\n",
      "00:01:14 14Mb  INFO: Finished loading graph\n",
      "00:01:14 89Mb  INFO: Looking for unique edges\n",
      "00:01:14 89Mb  INFO: Marked 86 long edges as unique\n",
      "00:01:14 89Mb  INFO: Marking extra edges as unique based on read paths\n",
      "00:01:14 89Mb  INFO: Marked 94 edges as unique\n",
      "00:01:14 89Mb  INFO: Splitting graph with unique edges\n",
      "00:01:14 89Mb  INFO: Processing 140 components\n",
      "00:01:14 89Mb  INFO: Finished unique edges search. Found 110 unique edges\n",
      "00:01:14 89Mb  INFO: Analysing repeats of multiplicity 2 and looking for additional unique edges\n",
      "00:01:14 89Mb  INFO: Finished processing of repeats of multiplicity 2. Found 0 erroneous edges.\n",
      "00:01:14 89Mb  INFO: Resolving repeats\n",
      "00:01:14 89Mb  INFO: Constructing paths\n",
      "00:01:16 172Mb  INFO: Building graph\n",
      "00:01:16 172Mb  INFO: Increasing k\n",
      "00:01:16 174Mb  INFO: Finished increasing k\n",
      "00:01:16 174Mb  INFO: Exporting remaining active transitions\n",
      "00:01:16 174Mb  INFO: Export to Dot\n",
      "00:01:16 174Mb  INFO: Export to GFA and compressed contigs\n",
      "00:01:17 181Mb  INFO: Finished repeat resolution\n",
      "00:01:17 5Mb  INFO: Performing polishing and homopolymer uncompression\n",
      "00:01:17 6Mb  INFO: Aligning reads back to assembly\n",
      "00:01:26 2.1Gb  INFO: Finished alignment.\n",
      "00:01:26 2.1Gb  INFO: Printing alignments to \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/uncompressing/alignments.txt\"\n",
      "00:01:26 2.1Gb  INFO: Reading and processing initial reads from [\"phasing-data/smoothed-reads/edge_1671_smoothed_reads.fasta\"]\n",
      "00:02:10 3.4Gb  INFO: Uncompressing homopolymers in contigs\n",
      "00:02:10 3.4Gb  INFO: Total zero covered nucleotides 0\n",
      "00:02:10 3.4Gb  INFO: Calculating overlaps between adjacent uncompressed edges\n",
      "00:02:10 3.4Gb  INFO: Printing final gfa file to \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/mdbg.gfa\"\n",
      "00:02:10 3.4Gb  INFO: Printing final assembly to \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/assembly.fasta\"\n",
      "00:02:10 5Mb  INFO: Final homopolymer compressed and corrected reads can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/k5001/corrected_reads.fasta\"\n",
      "00:02:10 5Mb  INFO: Final graph with homopolymer compressed edges can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/mdbg/mdbg.hpc.gfa\"\n",
      "00:02:10 5Mb  INFO: Final graph can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/mdbg.gfa\"\n",
      "00:02:10 5Mb  INFO: Final assembly can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/assembly.fasta\"\n",
      "00:02:10 5Mb  INFO: LJA pipeline finished\n",
      "00:00:00 2Mb  INFO: Hello! You are running La Jolla Assembler (LJA), a tool for genome assembly from PacBio HiFi reads\n",
      "00:00:19 2Mb  INFO: eaeb8812405afe9b8ffa8ded423178299ee296c2\n",
      "00:00:19 2Mb  INFO: LJA pipeline started\n",
      "00:00:19 2Mb  INFO: Performing initial correction with k = 5001\n",
      "00:00:19 0Mb  INFO: Reading reads\n",
      "00:00:19 0Mb  INFO: Extracting minimizers\n",
      "00:00:48 4.4Gb  INFO: Finished read processing\n",
      "00:00:48 4.4Gb  INFO: 9383594 hashs collected. Starting sorting.\n",
      "00:00:48 4.4Gb  INFO: Finished sorting. Total distinct minimizers: 10142\n",
      "00:00:48 4.4Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:48 4.4Gb  INFO: Vertex map constructed.\n",
      "00:00:48 4.4Gb  INFO: Filling edge sequences.\n",
      "00:01:18 5.7Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:01:18 5.7Gb  INFO:  Collecting tips \n",
      "00:01:18 5.7Gb  INFO: Added 29 artificial minimizers from tips.\n",
      "00:01:18 5.7Gb  INFO: Collected 20468 old edges.\n",
      "00:01:18 5.7Gb  INFO: New minimizers added to sparse graph.\n",
      "00:01:18 5.7Gb  INFO: Refilling graph with old edges.\n",
      "00:01:18 5.7Gb  INFO: Filling graph with new edges.\n",
      "00:01:18 5.7Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:01:18 5.7Gb  INFO: Starting to extract disjointigs.\n",
      "00:01:19 5.7Gb  INFO: Finished extracting 326 disjointigs of total size 4173388\n",
      "00:01:19 0Mb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/k5001/disjointigs.fasta\"\n",
      "00:01:19 15Mb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:01:20 15Mb  INFO: Filled 11772074 bits out of 81377984\n",
      "00:01:20 15Mb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:01:20 15Mb  INFO: Collected 1187 junctions.\n",
      "00:01:20 15Mb  INFO: Starting DBG construction.\n",
      "00:01:20 15Mb  INFO: Vertices created.\n",
      "00:01:20 15Mb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:01:20 15Mb  INFO: Added 0 hanging vertices\n",
      "00:01:20 15Mb  INFO: Merging unbranching paths\n",
      "00:01:20 15Mb  INFO: Ended merging edges. Resulting size 242\n",
      "00:01:21 15Mb  INFO: Cleaning edge coverages\n",
      "00:01:21 15Mb  INFO: Collecting alignments of sequences to the graph\n",
      "00:01:41 6.7Gb  INFO: Alignment collection finished. Total length of alignments is 883694\n",
      "00:01:41 6.7Gb  INFO: Could not correct 171 reads. They will be removed.\n",
      "00:01:41 6.7Gb  INFO: Uncorrected reads were removed.\n",
      "00:01:41 6.7Gb  INFO: Applying changes to the graph\n",
      "00:01:47 6.9Gb  INFO: Printing reads to fasta file \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/k5001/corrected_reads.fasta\"\n",
      "00:02:41 5Mb  INFO: Performing repeat resolution by transforming de Bruijn graph into Multiplex de Bruijn graph\n",
      "00:02:41 0Mb  INFO: Loading graph from fasta\n",
      "00:02:41 13Mb  INFO: Finished loading graph\n",
      "00:02:42 245Mb  INFO: Looking for unique edges\n",
      "00:02:42 245Mb  INFO: Marked 20 long edges as unique\n",
      "00:02:42 245Mb  INFO: Marking extra edges as unique based on read paths\n",
      "00:02:42 245Mb  INFO: Marked 20 edges as unique\n",
      "00:02:42 245Mb  INFO: Splitting graph with unique edges\n",
      "00:02:42 245Mb  INFO: Processing 12 components\n",
      "00:02:42 245Mb  INFO: Finished unique edges search. Found 208 unique edges\n",
      "00:02:42 245Mb  INFO: Analysing repeats of multiplicity 2 and looking for additional unique edges\n",
      "00:02:42 245Mb  INFO: Finished processing of repeats of multiplicity 2. Found 0 erroneous edges.\n",
      "00:02:42 245Mb  INFO: Resolving repeats\n",
      "00:02:42 245Mb  INFO: Constructing paths\n",
      "00:02:49 0.5Gb  INFO: Building graph\n",
      "00:02:49 0.5Gb  INFO: Increasing k\n",
      "00:02:53 0.5Gb  INFO: Finished increasing k\n",
      "00:02:53 0.5Gb  INFO: Exporting remaining active transitions\n",
      "00:02:53 0.5Gb  INFO: Export to Dot\n",
      "00:02:53 0.5Gb  INFO: Export to GFA and compressed contigs\n",
      "00:02:53 0.5Gb  INFO: Finished repeat resolution\n",
      "00:02:54 5Mb  INFO: Performing polishing and homopolymer uncompression\n",
      "00:02:54 11Mb  INFO: Aligning reads back to assembly\n",
      "00:03:26 7Gb  INFO: Finished alignment.\n",
      "00:03:26 7Gb  INFO: Printing alignments to \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/uncompressing/alignments.txt\"\n",
      "00:03:27 7Gb  INFO: Reading and processing initial reads from [\"phasing-data/smoothed-reads/edge_2358_smoothed_reads.fasta\"]\n",
      "00:09:02 7Gb  INFO: Uncompressing homopolymers in contigs\n",
      "00:09:03 7Gb  INFO: Total zero covered nucleotides 0\n",
      "00:09:03 7Gb  INFO: Calculating overlaps between adjacent uncompressed edges\n",
      "00:09:03 7Gb  INFO: Printing final gfa file to \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/mdbg.gfa\"\n",
      "00:09:03 7Gb  INFO: Printing final assembly to \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/assembly.fasta\"\n",
      "00:09:03 5Mb  INFO: Final homopolymer compressed and corrected reads can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/k5001/corrected_reads.fasta\"\n",
      "00:09:03 5Mb  INFO: Final graph with homopolymer compressed edges can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/mdbg/mdbg.hpc.gfa\"\n",
      "00:09:03 5Mb  INFO: Final graph can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/mdbg.gfa\"\n",
      "00:09:03 5Mb  INFO: Final assembly can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/assembly.fasta\"\n",
      "00:09:03 5Mb  INFO: LJA pipeline finished\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# LJA with no error correction, but with a filter for low-coverage edges\n",
    "\n",
    "OUTDIR=phasing-data/smoothed-reads\n",
    "\n",
    "/home/mfedarko/software/LJA-branch/bin/lja \\\n",
    "    --reads $OUTDIR/edge_6104_smoothed_reads.fasta \\\n",
    "    --simpleec \\\n",
    "    --Cov-threshold 10 \\\n",
    "    --output-dir $OUTDIR/edge_6104_lja_cf_10x_p1pct\n",
    "\n",
    "/home/mfedarko/software/LJA-branch/bin/lja \\\n",
    "    --reads $OUTDIR/edge_1671_smoothed_reads.fasta \\\n",
    "    --simpleec \\\n",
    "    --Cov-threshold 10 \\\n",
    "    --output-dir $OUTDIR/edge_1671_lja_cf_10x_p1pct\n",
    "\n",
    "/home/mfedarko/software/LJA-branch/bin/lja \\\n",
    "    --reads $OUTDIR/edge_2358_smoothed_reads.fasta \\\n",
    "    --simpleec \\\n",
    "    --Cov-threshold 10 \\\n",
    "    --output-dir $OUTDIR/edge_2358_lja_cf_10x_p1pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
