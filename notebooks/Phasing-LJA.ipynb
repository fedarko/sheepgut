{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform read smoothing then assemble with LJA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T04:54:25.851408Z",
     "iopub.status.busy": "2022-03-09T04:54:25.850496Z",
     "iopub.status.idle": "2022-03-09T04:54:26.323869Z",
     "shell.execute_reply": "2022-03-09T04:54:26.324355Z"
    }
   },
   "outputs": [],
   "source": [
    "%run \"Header.ipynb\"\n",
    "%run \"../main-workflow/utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T04:54:26.328414Z",
     "iopub.status.busy": "2022-03-09T04:54:26.327760Z",
     "iopub.status.idle": "2022-03-09T04:54:26.700912Z",
     "shell.execute_reply": "2022-03-09T04:54:26.700177Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pysam\n",
    "import skbio\n",
    "from collections import defaultdict, Counter\n",
    "from linked_mutations_utils import find_mutated_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Quick sanity check: ensure that all $k$-mers ($k$ = 5,001) are unique in each MAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T04:54:26.706973Z",
     "iopub.status.busy": "2022-03-09T04:54:26.706144Z",
     "iopub.status.idle": "2022-03-09T04:56:03.643601Z",
     "shell.execute_reply": "2022-03-09T04:56:03.644247Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On start pos 0 in CAMP.\n",
      "On start pos 1,000,000 in CAMP.\n",
      "The most common k = 5,001-mer in CAMP occurred 1 time(s).\n",
      "On start pos 0 in BACT1.\n",
      "On start pos 1,000,000 in BACT1.\n",
      "On start pos 2,000,000 in BACT1.\n",
      "The most common k = 5,001-mer in BACT1 occurred 1 time(s).\n",
      "On start pos 0 in BACT2.\n",
      "On start pos 1,000,000 in BACT2.\n",
      "On start pos 2,000,000 in BACT2.\n",
      "The most common k = 5,001-mer in BACT2 occurred 1 time(s).\n"
     ]
    }
   ],
   "source": [
    "# I know there are actual k-mer counting tools you can use but no reason to overcomplicate things for now\n",
    "\n",
    "k = 5001\n",
    "\n",
    "for seq in SEQS:\n",
    "    fasta = skbio.DNA.read(f\"../seqs/{seq}.fasta\")\n",
    "    bargain_bin_kmer_counter = Counter()\n",
    "    \n",
    "    # The skbio.DNA object is 0-indexed, so 0 is the leftmost k-mer start position and\n",
    "    # ((seq length) - k) is the rightmost k-mer start position. The + 1 is because python ranges don't include\n",
    "    # the right endpoint.\n",
    "    for start_pos in range(0, seq2len[seq] - k + 1):\n",
    "        \n",
    "        # NOTE: this is a terrible no good very bad way to do this; it's more efficient to use a \"sliding window\"\n",
    "        # approach where you store the entire k-mer and then, with each step, just remove the first character and\n",
    "        # add on a new last character. \"But, uh, this code will only be run on these three MAGs, so I'm gonna\n",
    "        # prioritize clarity over optimization,\" says me, the insane person who just spent like a minute writing\n",
    "        # this comment when I could've been optimizing this code instead look WHATEVER this counts k-mers and it's\n",
    "        # 4am let's not overcomplicate it, look if you're on GitHub right now and you see this inane comment\n",
    "        # we can both just pretend that you were looking at some really optimized code and we'll both walk away\n",
    "        # satisfied, capisce\n",
    "        kmer = fasta[start_pos : start_pos + k]\n",
    "        \n",
    "        bargain_bin_kmer_counter[str(kmer)] += 1\n",
    "        if start_pos % 1000000 == 0: print(f\"On start pos {start_pos:,} in {seq2name[seq]}.\")\n",
    "    \n",
    "    mckc = bargain_bin_kmer_counter.most_common(1)[0][1]\n",
    "    print(f\"The most common k = {k:,}-mer in {seq2name[seq]} occurred {mckc:,} time(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Smooth reads\n",
    "\n",
    "Lots of this code is duplicated from the `Phasing-01-MakeGraph.ipynb` notebook in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T04:56:03.649017Z",
     "iopub.status.busy": "2022-03-09T04:56:03.648213Z",
     "iopub.status.idle": "2022-03-09T04:56:03.650537Z",
     "shell.execute_reply": "2022-03-09T04:56:03.649902Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set this to True to actually generate ordinary smoothed reads that include called mutations;\n",
    "# set this to False to generate \"sanity check\" perfect smoothed reads, where no mutations are included\n",
    "# and the read entirely matches the reference\n",
    "actually_include_mutations_in_the_smoothed_reads = True\n",
    "\n",
    "add_virtual_reads = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T04:56:03.654342Z",
     "iopub.status.busy": "2022-03-09T04:56:03.653560Z",
     "iopub.status.idle": "2022-03-09T04:56:20.792491Z",
     "shell.execute_reply": "2022-03-09T04:56:20.796107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence edge_6104 has average coverage 4,158.57 and median coverage 4,122.00.\n",
      "Sequence edge_1671 has average coverage 1,415.07 and median coverage 1,436.00.\n",
      "Sequence edge_2358 has average coverage 2,993.46 and median coverage 2,936.00.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'edge_6104': 4158.572468826692,\n",
       " 'edge_1671': 1415.072755380576,\n",
       " 'edge_2358': 2993.461913625056}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll need to know the mean coverage of each sequence when computing virtual reads.\n",
    "seq2meancov = get_meancovs()\n",
    "seq2meancov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T04:56:20.816861Z",
     "iopub.status.busy": "2022-03-09T04:56:20.805245Z",
     "iopub.status.idle": "2022-03-09T05:24:49.400711Z",
     "shell.execute_reply": "2022-03-09T05:24:49.400010Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Identifying mutated positions (p = 10%) in genome CAMP...\n",
      "Found 35 mutated positions (p = 10%) in CAMP.\n",
      "Note that this tally is higher than you'd see in e.g. the CP1/2/3 plots, because now we're including both 'rare' and non-rare mutations. Just so you don't waste five minutes sanity-checking this like I did.\n",
      "Going through these positions...\n",
      "\tOn aln 5,000 in seq CAMP. Time spent on CAMP so far: 12.43 sec.\n",
      "\tOn aln 10,000 in seq CAMP. Time spent on CAMP so far: 32.33 sec.\n",
      "\tOn aln 15,000 in seq CAMP. Time spent on CAMP so far: 51.53 sec.\n",
      "\tOn aln 20,000 in seq CAMP. Time spent on CAMP so far: 72.30 sec.\n",
      "\tOn aln 25,000 in seq CAMP. Time spent on CAMP so far: 91.93 sec.\n",
      "\tOn aln 30,000 in seq CAMP. Time spent on CAMP so far: 110.49 sec.\n",
      "\tOn aln 35,000 in seq CAMP. Time spent on CAMP so far: 128.65 sec.\n",
      "\tOn aln 40,000 in seq CAMP. Time spent on CAMP so far: 147.04 sec.\n",
      "\tOn aln 45,000 in seq CAMP. Time spent on CAMP so far: 165.44 sec.\n",
      "\tOn aln 50,000 in seq CAMP. Time spent on CAMP so far: 182.56 sec.\n",
      "\tOn aln 55,000 in seq CAMP. Time spent on CAMP so far: 200.85 sec.\n",
      "\tOn aln 60,000 in seq CAMP. Time spent on CAMP so far: 219.12 sec.\n",
      "\tOn aln 65,000 in seq CAMP. Time spent on CAMP so far: 238.39 sec.\n",
      "\tOn aln 70,000 in seq CAMP. Time spent on CAMP so far: 259.77 sec.\n",
      "\tOn aln 75,000 in seq CAMP. Time spent on CAMP so far: 279.36 sec.\n",
      "\tOn aln 80,000 in seq CAMP. Time spent on CAMP so far: 299.28 sec.\n",
      "\tOn aln 85,000 in seq CAMP. Time spent on CAMP so far: 319.63 sec.\n",
      "\tOn aln 90,000 in seq CAMP. Time spent on CAMP so far: 338.78 sec.\n",
      "\tOn aln 95,000 in seq CAMP. Time spent on CAMP so far: 357.16 sec.\n",
      "\tOn aln 100,000 in seq CAMP. Time spent on CAMP so far: 375.71 sec.\n",
      "\tOn aln 105,000 in seq CAMP. Time spent on CAMP so far: 393.23 sec.\n",
      "\tOn aln 110,000 in seq CAMP. Time spent on CAMP so far: 411.13 sec.\n",
      "\tOn aln 115,000 in seq CAMP. Time spent on CAMP so far: 428.77 sec.\n",
      "\tOn aln 120,000 in seq CAMP. Time spent on CAMP so far: 446.00 sec.\n",
      "\tOn aln 125,000 in seq CAMP. Time spent on CAMP so far: 463.29 sec.\n",
      "\tOn aln 130,000 in seq CAMP. Time spent on CAMP so far: 480.73 sec.\n",
      "\tOn aln 135,000 in seq CAMP. Time spent on CAMP so far: 498.91 sec.\n",
      "\tOn aln 140,000 in seq CAMP. Time spent on CAMP so far: 517.05 sec.\n",
      "\tOn aln 145,000 in seq CAMP. Time spent on CAMP so far: 534.55 sec.\n",
      "\tOn aln 150,000 in seq CAMP. Time spent on CAMP so far: 551.96 sec.\n",
      "\tOn aln 155,000 in seq CAMP. Time spent on CAMP so far: 569.38 sec.\n",
      "\tOn aln 160,000 in seq CAMP. Time spent on CAMP so far: 586.76 sec.\n",
      "\tOn aln 165,000 in seq CAMP. Time spent on CAMP so far: 604.88 sec.\n",
      "\tOn aln 170,000 in seq CAMP. Time spent on CAMP so far: 622.85 sec.\n",
      "\tOn aln 175,000 in seq CAMP. Time spent on CAMP so far: 641.06 sec.\n",
      "\tOn aln 180,000 in seq CAMP. Time spent on CAMP so far: 659.20 sec.\n",
      "\tOn aln 185,000 in seq CAMP. Time spent on CAMP so far: 677.69 sec.\n",
      "\tOn aln 190,000 in seq CAMP. Time spent on CAMP so far: 697.08 sec.\n",
      "\tOn aln 195,000 in seq CAMP. Time spent on CAMP so far: 715.01 sec.\n",
      "\tOn aln 200,000 in seq CAMP. Time spent on CAMP so far: 732.45 sec.\n",
      "\tOn aln 205,000 in seq CAMP. Time spent on CAMP so far: 749.88 sec.\n",
      "\tOn aln 210,000 in seq CAMP. Time spent on CAMP so far: 767.26 sec.\n",
      "\tOn aln 215,000 in seq CAMP. Time spent on CAMP so far: 785.12 sec.\n",
      "\tOn aln 220,000 in seq CAMP. Time spent on CAMP so far: 802.80 sec.\n",
      "\tOn aln 225,000 in seq CAMP. Time spent on CAMP so far: 821.22 sec.\n",
      "\tOn aln 230,000 in seq CAMP. Time spent on CAMP so far: 838.77 sec.\n",
      "\tOn aln 235,000 in seq CAMP. Time spent on CAMP so far: 856.33 sec.\n",
      "\tOn aln 240,000 in seq CAMP. Time spent on CAMP so far: 873.84 sec.\n",
      "\tOn aln 245,000 in seq CAMP. Time spent on CAMP so far: 891.50 sec.\n",
      "\tOn aln 250,000 in seq CAMP. Time spent on CAMP so far: 909.00 sec.\n",
      "\tOn aln 255,000 in seq CAMP. Time spent on CAMP so far: 926.45 sec.\n",
      "\tOn aln 260,000 in seq CAMP. Time spent on CAMP so far: 943.88 sec.\n",
      "\tOn aln 265,000 in seq CAMP. Time spent on CAMP so far: 961.49 sec.\n",
      "\tOn aln 270,000 in seq CAMP. Time spent on CAMP so far: 979.09 sec.\n",
      "\tOn aln 275,000 in seq CAMP. Time spent on CAMP so far: 997.25 sec.\n",
      "\tOn aln 280,000 in seq CAMP. Time spent on CAMP so far: 1,015.22 sec.\n",
      "\tOn aln 285,000 in seq CAMP. Time spent on CAMP so far: 1,033.34 sec.\n",
      "\tOn aln 290,000 in seq CAMP. Time spent on CAMP so far: 1,051.69 sec.\n",
      "\tOn aln 295,000 in seq CAMP. Time spent on CAMP so far: 1,069.85 sec.\n",
      "\tOn aln 300,000 in seq CAMP. Time spent on CAMP so far: 1,090.74 sec.\n",
      "\tOn aln 305,000 in seq CAMP. Time spent on CAMP so far: 1,111.78 sec.\n",
      "\tOn aln 310,000 in seq CAMP. Time spent on CAMP so far: 1,132.45 sec.\n",
      "\tOn aln 315,000 in seq CAMP. Time spent on CAMP so far: 1,153.03 sec.\n",
      "\tOn aln 320,000 in seq CAMP. Time spent on CAMP so far: 1,172.25 sec.\n",
      "\tOn aln 325,000 in seq CAMP. Time spent on CAMP so far: 1,190.92 sec.\n",
      "\tOn aln 330,000 in seq CAMP. Time spent on CAMP so far: 1,210.21 sec.\n",
      "\tOn aln 335,000 in seq CAMP. Time spent on CAMP so far: 1,229.20 sec.\n",
      "\tOn aln 340,000 in seq CAMP. Time spent on CAMP so far: 1,248.29 sec.\n",
      "\tOn aln 345,000 in seq CAMP. Time spent on CAMP so far: 1,267.15 sec.\n",
      "\tOn aln 350,000 in seq CAMP. Time spent on CAMP so far: 1,285.77 sec.\n",
      "\tOn aln 355,000 in seq CAMP. Time spent on CAMP so far: 1,304.90 sec.\n",
      "\tOn aln 360,000 in seq CAMP. Time spent on CAMP so far: 1,324.89 sec.\n",
      "\tOn aln 365,000 in seq CAMP. Time spent on CAMP so far: 1,345.72 sec.\n",
      "\tOn aln 370,000 in seq CAMP. Time spent on CAMP so far: 1,365.98 sec.\n",
      "\tOn aln 375,000 in seq CAMP. Time spent on CAMP so far: 1,384.93 sec.\n",
      "\tOn aln 380,000 in seq CAMP. Time spent on CAMP so far: 1,403.80 sec.\n",
      "\tOn aln 385,000 in seq CAMP. Time spent on CAMP so far: 1,423.87 sec.\n",
      "\tOn aln 390,000 in seq CAMP. Time spent on CAMP so far: 1,443.73 sec.\n",
      "\tOn aln 395,000 in seq CAMP. Time spent on CAMP so far: 1,463.45 sec.\n",
      "\tOn aln 400,000 in seq CAMP. Time spent on CAMP so far: 1,481.11 sec.\n",
      "\tOn aln 405,000 in seq CAMP. Time spent on CAMP so far: 1,498.71 sec.\n",
      "\tOn aln 410,000 in seq CAMP. Time spent on CAMP so far: 1,516.51 sec.\n",
      "\tOn aln 415,000 in seq CAMP. Time spent on CAMP so far: 1,534.09 sec.\n",
      "\tOn aln 420,000 in seq CAMP. Time spent on CAMP so far: 1,553.64 sec.\n",
      "\tOn aln 425,000 in seq CAMP. Time spent on CAMP so far: 1,573.28 sec.\n",
      "\tOn aln 430,000 in seq CAMP. Time spent on CAMP so far: 1,590.74 sec.\n",
      "\tOn aln 435,000 in seq CAMP. Time spent on CAMP so far: 1,609.32 sec.\n",
      "\tOn aln 440,000 in seq CAMP. Time spent on CAMP so far: 1,627.94 sec.\n",
      "\tOn aln 445,000 in seq CAMP. Time spent on CAMP so far: 1,653.92 sec.\n",
      "\tOn aln 450,000 in seq CAMP. Time spent on CAMP so far: 1,662.29 sec.\n",
      "\tOn aln 455,000 in seq CAMP. Time spent on CAMP so far: 1,669.70 sec.\n",
      "\tOn aln 460,000 in seq CAMP. Time spent on CAMP so far: 1,676.03 sec.\n",
      "\tOn aln 465,000 in seq CAMP. Time spent on CAMP so far: 1,682.49 sec.\n",
      "\tOn aln 470,000 in seq CAMP. Time spent on CAMP so far: 1,688.90 sec.\n",
      "\tOn aln 475,000 in seq CAMP. Time spent on CAMP so far: 1,695.43 sec.\n",
      "We ignored 41 linear alignments, fyi.\n",
      "Done with edge_6104! Took 1,696.78 sec.\n",
      "Time taken: 1,708.5227966308594 sec.\n"
     ]
    }
   ],
   "source": [
    "bf = pysam.AlignmentFile(\"../main-workflow/output/fully-filtered-and-sorted-aln.bam\", \"rb\")\n",
    "output_dir = \"phasing-data/smoothed-reads/\"\n",
    "\n",
    "# verbose?\n",
    "no_indoor_voice = False\n",
    "\n",
    "def write_out_reads(filepath, readname2seq):\n",
    "    # Notably, this uses the \"a\" (append) method in order to add to the end of a file\n",
    "    with open(filepath, \"a\") as of:\n",
    "        for readname in readname2seq:\n",
    "            # Write out both the header and the sequence for each read\n",
    "            of.write(f\">{readname}\\n{str(readname2seq[readname])}\\n\")\n",
    "            \n",
    "ALN_UPDATE_FREQ = 5000\n",
    "ALN_BUFFER_FREQ = 1000\n",
    "VR_EXTRA_SPAN = 100\n",
    "\n",
    "P = 10\n",
    "            \n",
    "t1 = time.time()\n",
    "for seq in [SEQS[0]]:\n",
    "    \n",
    "    # Record which positions (0-indexed) aren't covered by any smoothed reads in this MAG.\n",
    "    # We'll add \"virtual reads\" that span these positions.\n",
    "    uncovered_positions = set(range(0, seq2len[seq]))\n",
    "    \n",
    "    fasta = skbio.DNA.read(f\"../seqs/{seq}.fasta\")\n",
    "    \n",
    "    output_smoothed_reads_file = os.path.join(output_dir, f\"{seq}_smoothed_reads.fasta\")\n",
    "    \n",
    "    # Identify all (0-indexed, so compatible with skbio / pysam!)\n",
    "    # mutated positions in this genome up front to save time.\n",
    "    #\n",
    "    # Equivalently, we could also just take in an arbitrary VCF as input\n",
    "    # (e.g. one produced from another variant calling tool), although we'd\n",
    "    # need to be careful to only include SNVs and not indels/etc...\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Identifying mutated positions (p = {P}%) in genome {seq2name[seq]}...\")\n",
    "    mutpos2pileup = find_mutated_positions(seq, p_to_use=P, incl_pileup=True)\n",
    "    # We sort because the code below relies on these being in ascending order\n",
    "    mutated_positions = sorted(mutpos2pileup.keys())\n",
    "    print(f\"Found {len(mutated_positions):,} mutated positions (p = {P}%) in {seq2name[seq]}.\")\n",
    "    print(\n",
    "        f\"Note that this tally is higher than you'd see in e.g. the CP1/2/3 plots, because now we're including \"\n",
    "        \"both 'rare' and non-rare mutations. Just so you don't waste five minutes sanity-checking this like I did.\"\n",
    "    )\n",
    "    \n",
    "    print(\"Going through these positions...\")\n",
    "    \n",
    "    num_ignored_alns = 0\n",
    "    \n",
    "    # Instead of just writing out every smoothed alignment as soon as we generate it, we build up a \"buffer\"\n",
    "    # of these alignments and then write a bunch out at once. This way we limit slowdown due to constantly\n",
    "    # having to open/close files. I don't really have a good source for this as best practice, but I remembered\n",
    "    # to do it while writing this code, so somewhere in College Park the CS faculty at Maryland are smiling\n",
    "    #\n",
    "    # Also fyi this maps read name to smoothed alignment (well, at this point, just read) sequence. The read name\n",
    "    # is useful to preserve in fasta files so we have some idea of provenance (where smoothed reads came from)\n",
    "    smoothed_aln_buffer = {}\n",
    "    \n",
    "    # The first time we see an alignment of a read, it's 1; if we see a supp aln of this read, it's 2; etc.\n",
    "    # Lets us distinguish alignments with different names\n",
    "    readname2freq_so_far = defaultdict(int)\n",
    "    \n",
    "    # Go through all linear alignments of each read to this genome, focusing (for now) on just the primary\n",
    "    # alignments...\n",
    "    ts1 = time.time()\n",
    "    for ai, aln in enumerate(bf.fetch(seq), 1):\n",
    "        \n",
    "        if ai % ALN_UPDATE_FREQ == 0:\n",
    "            print(\n",
    "                f\"\\tOn aln {ai:,} in seq {seq2name[seq]}. \"\n",
    "                f\"Time spent on {seq2name[seq]} so far: {time.time() - ts1:,.2f} sec.\"\n",
    "            )\n",
    "            \n",
    "        if aln.is_secondary:\n",
    "            raise ValueError(\n",
    "                \"Not to get political or anything, but you should've already filtered secondary alns out\"\n",
    "            )\n",
    "            \n",
    "        # Note that supplementary alignments are ok, though! We implicitly handle these here.\n",
    "        #\n",
    "        # Different alignments of the same read will have different new_readnames, because we're gonna\n",
    "        # be treating them as distinct \"reads\". We should have already filtered reference-overlapping\n",
    "        # supp alns so this shouldn't be a problem\n",
    "        \n",
    "        readname = aln.query_name\n",
    "        readname2freq_so_far[readname] += 1\n",
    "        new_readname = f\"{readname}_{readname2freq_so_far[readname]}\"\n",
    "        \n",
    "        # should never happen\n",
    "        if new_readname in smoothed_aln_buffer:\n",
    "            raise ValueError(\"This exact read alignment has already been smoothed? Weird.\")\n",
    "            \n",
    "        # Figure out where on the MAG this alignment \"hits.\" These are 0-indexed positions from Pysam.\n",
    "        # (reference_end points to the position after the actual final position, since these are designed to\n",
    "        # be interoperable with Python's half-open intervals.)\n",
    "        #\n",
    "        # Of course, there likely will be indels within this range: we're purposefully ignoring those here.\n",
    "        ref_start = aln.reference_start\n",
    "        ref_end = aln.reference_end - 1\n",
    "        \n",
    "        # This should never happen (TM)\n",
    "        if ref_start >= ref_end:\n",
    "            # Du sollst jetzt mit Gott sprechen.\n",
    "            raise ValueError(\n",
    "                f\"Ref start {ref_start:,} >= ref end {ref_end:,} for read {new_readname}?\"\n",
    "            )\n",
    "            \n",
    "        # Smoothed sequence; we'll edit this so that if this read has (mis)matches to any called mutated\n",
    "        # positions, these positions are updated with the read's aligned nucleotides at these positions.\n",
    "        smoothed_aln_seq = fasta[ref_start: ref_end + 1]\n",
    "        \n",
    "        if actually_include_mutations_in_the_smoothed_reads:\n",
    "            # just for debugging: track the exact edits made to smoothed_aln_seq\n",
    "            replacements_made = {}\n",
    "\n",
    "            # We may choose to ignore this linear alignment, if we think it is error-prone or\n",
    "            # otherwise not useful. If this gets set to True in the loop below, then we'll notice this\n",
    "            # and ignore this alignment.\n",
    "            ignoring_this_aln = False\n",
    "            \n",
    "            ap = aln.get_aligned_pairs(matches_only=True)\n",
    "\n",
    "            # Iterating through the aligned pairs is expensive. Since read lengths are generally in the thousands\n",
    "            # to tens of thousands of bp (which is much less than the > 1 million bp length of any bacterial genome),\n",
    "            # we set things up so that we only iterate through the aligned pairs once. We maintain an integer, mpi,\n",
    "            # that is a poor man's \"pointer\" to an index in mutated_positions.\n",
    "\n",
    "            mpi = 0\n",
    "\n",
    "            # Go through this aln's aligned pairs. As we see each pair, compare the pair's reference position\n",
    "            # (refpos) to the mpi-th mutated position (herein referred to as \"mutpos\").\n",
    "            #\n",
    "            # If refpos >  mutpos, increment mpi until refpos <= mutpos (stopping as early as possible).\n",
    "            # If refpos == mutpos, we have a match! Update readname2mutpos2ismutated[mutpos] based on\n",
    "            #                      comparing the read to the reference at the aligned positions.\n",
    "            # If refpos <  mutpos, continue to the next pair.\n",
    "\n",
    "            for pair in ap:\n",
    "\n",
    "                refpos = pair[1]\n",
    "                mutpos = mutated_positions[mpi]\n",
    "\n",
    "                no_mutations_to_right_of_here = False\n",
    "\n",
    "                # Increment mpi until we get to the next mutated position at or after the reference pos for this\n",
    "                # aligned pair (or until we run out of mutated positions).\n",
    "                while refpos > mutpos:\n",
    "                    mpi += 1\n",
    "                    if mpi < len(mutated_positions):\n",
    "                        mutpos = mutated_positions[mpi]\n",
    "                    else:\n",
    "                        no_mutations_to_right_of_here = True\n",
    "                        break\n",
    "\n",
    "                # I expect this should happen only for reads aligned near the right end of the genome.\n",
    "                if no_mutations_to_right_of_here:\n",
    "                    break\n",
    "\n",
    "                # If the next mutation occurs after this aligned pair, continue on to a later pair.\n",
    "                if refpos < mutpos:\n",
    "                    continue\n",
    "\n",
    "                # If we've made it here, refpos == mutpos!\n",
    "                # (...unless I messed something up in how I designed this code.)\n",
    "                if refpos != mutpos:\n",
    "                    raise ValueError(\"This should never happen!\")\n",
    "\n",
    "                # Finally, get the nucleotide aligned to this mutated position from this read.\n",
    "                readpos = pair[0]\n",
    "                read_nt = aln.query_sequence[readpos]\n",
    "\n",
    "                # If this read doesn't have match the first or second most common nucleotide at this position,\n",
    "                # ignore this read. In the future, when we perform read\n",
    "                # smoothing based on an arbitrary set of SNV calls, we can be more careful about this; but for now\n",
    "                # we make the simplifiying assumption that a mutation likely only has one alternate nucleotide,\n",
    "                # and that the 3rd and 4th most common nucleotides indicate errors.\n",
    "                # (Also, note that we break ties here arbitrarily.)\n",
    "                nt2ct = dict(zip(\"ACGT\", mutpos2pileup[mutpos][0]))\n",
    "                nt1 = max(nt2ct, key=nt2ct.get)\n",
    "                del nt2ct[nt1]\n",
    "                nt2 = max(nt2ct, key=nt2ct.get)\n",
    "                \n",
    "                if read_nt != nt1 and read_nt != nt2:\n",
    "                    if no_indoor_voice:\n",
    "                        print(\n",
    "                            f\"Read {new_readname} has 3rd or 4th most common nt at mutpos {mutpos + 1:,}: \"\n",
    "                            f\"pileup = {mutpos2pileup[mutpos]}, read = {read_nt}\"\n",
    "                        )\n",
    "                    ignoring_this_aln = True\n",
    "                    break\n",
    "                    \n",
    "                # Notably, the nucleotide at a mutated position in a smoothed read will always be the first\n",
    "                # or second most common nucleotide at this position. So \"unreasonable\" positions, in which\n",
    "                # the ref nt != the consensus nt, will not be treated as you might expect -- we ignore the\n",
    "                # reference in this particular case. Shouldn't make a big difference, since in most cases\n",
    "                # the ref and consensus nt agree.\n",
    "                relative_pos_on_aln = mutpos - ref_start\n",
    "                smoothed_aln_seq = smoothed_aln_seq.replace([relative_pos_on_aln], read_nt)\n",
    "                replacements_made[relative_pos_on_aln] = read_nt\n",
    "                if no_indoor_voice:\n",
    "                    print(\n",
    "                        f\"Read {new_readname} mismatches ref at mutpos {mutpos + 1:,}: \"\n",
    "                        f\"ref = {ref_nt}, read = {read_nt}\"\n",
    "                    )\n",
    "\n",
    "            if no_indoor_voice:\n",
    "                print(f\"Read {new_readname} required {len(replacements_made):,} replacements!\")\n",
    "        \n",
    "        if ignoring_this_aln:\n",
    "            num_ignored_alns += 1\n",
    "        else:\n",
    "            # Now that we've finished processing all called mutations that this alignment spans, prepare it\n",
    "            # to be written out to a FASTA file. See comments above on smoothed_aln_buffer, and why we don't\n",
    "            # just write everything out as soon as it's ready.\n",
    "            #\n",
    "            # (Also, we've already guaranteed readname isn't already in smoothed_aln_buffer, so no need to worry\n",
    "            # about accidentally overwriting something from earlier.)\n",
    "            smoothed_aln_buffer[new_readname] = smoothed_aln_seq\n",
    "\n",
    "            # Record which positions this read covers (of course, it may not exactly \"cover\" these positions\n",
    "            # originally due to indels, but the smoothed version will cover them).\n",
    "            # We don't update uncovered_positions until *after* we process all aligned pairs of this read, to allow\n",
    "            # us to ignore reads if desired.\n",
    "            uncovered_positions -= set(range(ref_start, ref_end + 1))\n",
    "\n",
    "            if ai % ALN_BUFFER_FREQ == 0:\n",
    "                write_out_reads(output_smoothed_reads_file, smoothed_aln_buffer)\n",
    "                # Clear the buffer\n",
    "                smoothed_aln_buffer = {}\n",
    "        \n",
    "    # We're probably going to have left over smoothed reads that we still haven't written out, unless things\n",
    "    # worked out so that on the final alignment we saw ai was exactly divisible by ALN_BUFFER_FREQ (and that's\n",
    "    # pretty unlikely unless you set the buffer freq to a low number). So make one last dump of the buffer.\n",
    "    if len(smoothed_aln_buffer) > 0:\n",
    "        write_out_reads(output_smoothed_reads_file, smoothed_aln_buffer)\n",
    "        \n",
    "    print(f\"We ignored {num_ignored_alns:,} linear alignments, fyi.\")\n",
    "        \n",
    "    if add_virtual_reads and len(uncovered_positions) > 0:\n",
    "        print(f\"For reference, there are {len(uncovered_positions):,} uncovered positions in {seq2name[seq]}.\")\n",
    "        \n",
    "        sup = sorted(uncovered_positions)\n",
    "        uc_runs = convert_to_runs(sup)\n",
    "        print(f'And there are {len(uc_runs)} \"runs\" of uncovered positions.')\n",
    "        \n",
    "        rounded_meancov = round(seq2meancov[seq])\n",
    "        print(\n",
    "            f'Adding \"virtual reads\" spanning each of these runs, at rounded mean coverage '\n",
    "            f'of {rounded_meancov:,}x, to account for this...'\n",
    "        )\n",
    "        \n",
    "        num_vr = 0\n",
    "        vr_buffer = {}\n",
    "        for run in uc_runs:\n",
    "            # Construct a virtual read that includes this entire run of uncovered positions as well\n",
    "            # as VR_EXTRA_SPAN positions before and after (clamping to the start/end of the seq if needed).\n",
    "            #\n",
    "            # Notably, we could try to make this loop around from end -> start if this is a cyclic MAG, but\n",
    "            # to remain consistent with how we handle supplementary alignments above -- and because implementing\n",
    "            # the loop around would be a lot of work and it's like 3am -- we ignore this for now.\n",
    "            #\n",
    "            # Also, note that run_start can equal run_end, if only a single isolated position is uncovered.\n",
    "            # This is fine -- the code handles this case automatically. (I guess the only potential problem is\n",
    "            # if the length of the MAG is less than VR_EXTRA_SPAN, but... that should never happen. If you have\n",
    "            # like 100bp-long MAGs that's a problem! I guess, TODO, make note of this when generalizing this\n",
    "            # code.)\n",
    "            \n",
    "            run_start = max(run[0] - VR_EXTRA_SPAN, 0)\n",
    "            run_end = min(run[1] + VR_EXTRA_SPAN, seq2len[seq] - 1)\n",
    "            \n",
    "            # Generate a sequence matching the \"reference\" MAG at these positions. We of course don't have\n",
    "            # any info about mutations here, because these positions are uncovered by the real reads!\n",
    "            vr_seq = fasta[run_start: run_end + 1]\n",
    "            \n",
    "            # We need to assign reads unique names, and including the run coordinates here is a nice way\n",
    "            # to preserve uniqueness across runs and also make our smoothed reads files easier to interpret\n",
    "            vr_name_prefix = f\"vr_{run[0]}_{run[1]}\"\n",
    "            \n",
    "            # Add M copies of this virtual read, where M = (rounded mean coverage of this MAG)\n",
    "            for vr_num in range(1, rounded_meancov + 1):\n",
    "                vr_name = f\"{vr_name_prefix}_{vr_num}\"\n",
    "                vr_buffer[vr_name] = vr_seq\n",
    "                num_vr += 1\n",
    "                \n",
    "        write_out_reads(output_smoothed_reads_file, vr_buffer)\n",
    "        print(f\"Wrote out {num_vr:,} virtual reads.\")\n",
    "    \n",
    "    print(f\"Done with {seq}! Took {time.time() - ts1:,.2f} sec.\")\n",
    "        \n",
    "print(f\"Time taken: {time.time() - t1:,} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assemble these smoothed reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T05:24:49.405920Z",
     "iopub.status.busy": "2022-03-09T05:24:49.405125Z",
     "iopub.status.idle": "2022-03-09T05:45:50.475344Z",
     "shell.execute_reply": "2022-03-09T05:45:50.475969Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:00 2Mb  INFO: Hello! You are running La Jolla Assembler (LJA), a tool for genome assembly from PacBio HiFi reads\n",
      "00:00:11 2Mb  INFO: c04bae31911a636f112dee9c63ce1a6b8e834710\n",
      "00:00:11 2Mb  INFO: LJA pipeline started\n",
      "00:00:11 2Mb  INFO: Performing initial correction with k = 5001\n",
      "00:00:11 0Mb  INFO: Reading reads\n",
      "00:00:11 0Mb  INFO: Extracting minimizers\n",
      "00:00:46 5.9Gb  INFO: Finished read processing\n",
      "00:00:46 5.9Gb  INFO: 10513236 hashs collected. Starting sorting.\n",
      "00:00:47 5.9Gb  INFO: Finished sorting. Total distinct minimizers: 4475\n",
      "00:00:47 5.9Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:47 5.9Gb  INFO: Vertex map constructed.\n",
      "00:00:47 5.9Gb  INFO: Filling edge sequences.\n",
      "00:01:21 6.9Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:01:21 6.9Gb  INFO:  Collecting tips \n",
      "00:01:21 6.9Gb  INFO: Added 60 artificial minimizers from tips.\n",
      "00:01:21 6.9Gb  INFO: Collected 8998 old edges.\n",
      "00:01:21 6.9Gb  INFO: New minimizers added to sparse graph.\n",
      "00:01:21 6.9Gb  INFO: Refilling graph with old edges.\n",
      "00:01:21 6.9Gb  INFO: Filling graph with new edges.\n",
      "00:01:21 6.9Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:01:21 6.9Gb  INFO: Starting to extract disjointigs.\n",
      "00:01:21 6.9Gb  INFO: Finished extracting 150 disjointigs of total size 1886270\n",
      "00:01:22 0Mb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x/k5001/disjointigs.fasta\"\n",
      "00:01:22 8Mb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:01:22 9Mb  INFO: Filled 5259018 bits out of 36355840\n",
      "00:01:22 9Mb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:01:23 9Mb  INFO: Collected 575 junctions.\n",
      "00:01:23 9Mb  INFO: Starting DBG construction.\n",
      "00:01:23 9Mb  INFO: Vertices created.\n",
      "00:01:23 9Mb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:01:23 9Mb  INFO: Added 1 hanging vertices\n",
      "00:01:23 9Mb  INFO: Merging unbranching paths\n",
      "00:01:23 9Mb  INFO: Ended merging edges. Resulting size 168\n",
      "00:01:23 9Mb  INFO: Cleaning edge coverages\n",
      "00:01:23 9Mb  INFO: Collecting alignments of sequences to the graph\n",
      "00:01:50 8.1Gb  INFO: Alignment collection finished. Total length of alignments is 997958\n",
      "00:01:50 8.1Gb  INFO: Could not correct 180 reads. They will be removed.\n",
      "00:01:50 8.1Gb  INFO: Uncorrected reads were removed.\n",
      "00:01:50 8.1Gb  INFO: Applying changes to the graph\n",
      "00:01:57 8.4Gb  INFO: Printing reads to fasta file \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x/k5001/corrected_reads.fasta\"\n",
      "00:03:04 5Mb  INFO: Performing repeat resolution by transforming de Bruijn graph into Multiplex de Bruijn graph\n",
      "00:03:04 0Mb  INFO: Loading graph from fasta\n",
      "00:03:04 7Mb  INFO: Finished loading graph\n",
      "00:03:06 303Mb  INFO: Looking for unique edges\n",
      "00:03:06 303Mb  INFO: Marked 18 long edges as unique\n",
      "00:03:06 303Mb  INFO: Marking extra edges as unique based on read paths\n",
      "00:03:06 303Mb  INFO: Marked 18 edges as unique\n",
      "00:03:06 303Mb  INFO: Splitting graph with unique edges\n",
      "00:03:06 303Mb  INFO: Processing 11 components\n",
      "00:03:06 303Mb  INFO: Finished unique edges search. Found 68 unique edges\n",
      "00:03:06 303Mb  INFO: Analysing repeats of multiplicity 2 and looking for additional unique edges\n",
      "00:03:06 303Mb  INFO: Finished processing of repeats of multiplicity 2. Found 0 erroneous edges.\n",
      "00:03:06 303Mb  INFO: Resolving repeats\n",
      "00:03:06 303Mb  INFO: Constructing paths\n",
      "00:03:14 0.6Gb  INFO: Building graph\n",
      "00:03:14 0.6Gb  INFO: Increasing k\n",
      "00:03:14 0.6Gb  INFO: Finished increasing k\n",
      "00:03:14 0.6Gb  INFO: Exporting remaining active transitions\n",
      "00:03:14 0.6Gb  INFO: Export to Dot\n",
      "00:03:14 0.6Gb  INFO: Export to GFA and compressed contigs\n",
      "00:03:14 0.6Gb  INFO: Finished repeat resolution\n",
      "00:03:14 5Mb  INFO: Performing polishing and homopolymer uncompression\n",
      "00:03:14 6Mb  INFO: Aligning reads back to assembly\n",
      "00:03:49 7.7Gb  INFO: Finished alignment.\n",
      "00:03:49 7.7Gb  INFO: Printing alignments to \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x/uncompressing/alignments.txt\"\n",
      "00:03:50 7.7Gb  INFO: Reading and processing initial reads from [\"phasing-data/smoothed-reads/edge_6104_smoothed_reads.fasta\"]\n",
      "00:09:34 7.7Gb  INFO: Uncompressing homopolymers in contigs\n",
      "00:09:35 7.7Gb  INFO: Total zero covered nucleotides 0\n",
      "00:09:35 7.7Gb  INFO: Calculating overlaps between adjacent uncompressed edges\n",
      "00:09:35 7.7Gb  INFO: Printing final gfa file to \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x/mdbg.gfa\"\n",
      "00:09:35 7.7Gb  INFO: Printing final assembly to \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x/assembly.fasta\"\n",
      "00:09:35 5Mb  INFO: Final homopolymer compressed and corrected reads can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x/k5001/corrected_reads.fasta\"\n",
      "00:09:35 5Mb  INFO: Final graph with homopolymer compressed edges can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x/mdbg/mdbg.hpc.gfa\"\n",
      "00:09:35 5Mb  INFO: Final graph can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x/mdbg.gfa\"\n",
      "00:09:35 5Mb  INFO: Final assembly can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x/assembly.fasta\"\n",
      "00:09:35 5Mb  INFO: LJA pipeline finished\n",
      "00:00:00 2Mb  INFO: Hello! You are running La Jolla Assembler (LJA), a tool for genome assembly from PacBio HiFi reads\n",
      "00:00:11 2Mb  INFO: c04bae31911a636f112dee9c63ce1a6b8e834710\n",
      "00:00:11 2Mb  INFO: LJA pipeline started\n",
      "00:00:11 2Mb  INFO: Performing initial correction with k = 5001\n",
      "00:00:11 0Mb  INFO: Reading reads\n",
      "00:00:11 0Mb  INFO: Extracting minimizers\n",
      "00:00:22 3.1Gb  INFO: Finished read processing\n",
      "00:00:22 3.1Gb  INFO: 3259426 hashs collected. Starting sorting.\n",
      "00:00:22 3.1Gb  INFO: Finished sorting. Total distinct minimizers: 66366\n",
      "00:00:22 3.1Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:22 3.1Gb  INFO: Vertex map constructed.\n",
      "00:00:22 3.1Gb  INFO: Filling edge sequences.\n",
      "00:00:34 3.3Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:00:34 3.3Gb  INFO:  Collecting tips \n",
      "00:00:34 3.3Gb  INFO: Added 6140 artificial minimizers from tips.\n",
      "00:00:34 3.3Gb  INFO: Collected 130682 old edges.\n",
      "00:00:34 3.3Gb  INFO: New minimizers added to sparse graph.\n",
      "00:00:34 3.3Gb  INFO: Refilling graph with old edges.\n",
      "00:00:35 3.3Gb  INFO: Filling graph with new edges.\n",
      "00:00:35 3.3Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:00:35 3.3Gb  INFO: Starting to extract disjointigs.\n",
      "00:00:35 3.3Gb  INFO: Finished extracting 8690 disjointigs of total size 60720845\n",
      "00:00:36 0Mb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x/k5001/disjointigs.fasta\"\n",
      "00:00:37 87Mb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:00:42 88Mb  INFO: Filled 79862907 bits out of 552388960\n",
      "00:00:42 88Mb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:00:43 88Mb  INFO: Collected 16889 junctions.\n",
      "00:00:43 88Mb  INFO: Starting DBG construction.\n",
      "00:00:43 88Mb  INFO: Vertices created.\n",
      "00:00:44 88Mb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:00:44 88Mb  INFO: Added 5 hanging vertices\n",
      "00:00:44 88Mb  INFO: Merging unbranching paths\n",
      "00:00:44 88Mb  INFO: Ended merging edges. Resulting size 10274\n",
      "00:00:44 88Mb  INFO: Cleaning edge coverages\n",
      "00:00:44 88Mb  INFO: Collecting alignments of sequences to the graph\n",
      "00:00:50 1.3Gb  INFO: Alignment collection finished. Total length of alignments is 954076\n",
      "00:00:50 1.3Gb  INFO: Could not correct 8959 reads. They will be removed.\n",
      "00:00:50 1.3Gb  INFO: Uncorrected reads were removed.\n",
      "00:00:50 1.3Gb  INFO: Applying changes to the graph\n",
      "00:00:55 1.3Gb  INFO: Printing reads to fasta file \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x/k5001/corrected_reads.fasta\"\n",
      "00:01:14 5Mb  INFO: Performing repeat resolution by transforming de Bruijn graph into Multiplex de Bruijn graph\n",
      "00:01:14 0Mb  INFO: Loading graph from fasta\n",
      "00:01:14 19Mb  INFO: Finished loading graph\n",
      "00:01:15 137Mb  INFO: Looking for unique edges\n",
      "00:01:15 137Mb  INFO: Marked 102 long edges as unique\n",
      "00:01:15 137Mb  INFO: Marking extra edges as unique based on read paths\n",
      "00:01:15 137Mb  INFO: Marked 114 edges as unique\n",
      "00:01:15 137Mb  INFO: Splitting graph with unique edges\n",
      "00:01:15 137Mb  INFO: Processing 194 components\n",
      "00:01:15 137Mb  INFO: Finished unique edges search. Found 140 unique edges\n",
      "00:01:15 137Mb  INFO: Analysing repeats of multiplicity 2 and looking for additional unique edges\n",
      "00:01:15 137Mb  INFO: Finished processing of repeats of multiplicity 2. Found 0 erroneous edges.\n",
      "00:01:15 137Mb  INFO: Resolving repeats\n",
      "00:01:15 137Mb  INFO: Constructing paths\n",
      "00:01:17 188Mb  INFO: Building graph\n",
      "00:01:17 188Mb  INFO: Increasing k\n",
      "00:01:17 191Mb  INFO: Finished increasing k\n",
      "00:01:17 191Mb  INFO: Exporting remaining active transitions\n",
      "00:01:17 191Mb  INFO: Export to Dot\n",
      "00:01:17 191Mb  INFO: Export to GFA and compressed contigs\n",
      "00:01:17 197Mb  INFO: Finished repeat resolution\n",
      "00:01:17 5Mb  INFO: Performing polishing and homopolymer uncompression\n",
      "00:01:18 6Mb  INFO: Aligning reads back to assembly\n",
      "00:01:27 2.1Gb  INFO: Finished alignment.\n",
      "00:01:27 2.1Gb  INFO: Printing alignments to \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x/uncompressing/alignments.txt\"\n",
      "00:01:27 2.2Gb  INFO: Reading and processing initial reads from [\"phasing-data/smoothed-reads/edge_1671_smoothed_reads.fasta\"]\n",
      "00:02:14 3.5Gb  INFO: Uncompressing homopolymers in contigs\n",
      "00:02:15 3.5Gb  INFO: Total zero covered nucleotides 0\n",
      "00:02:15 3.5Gb  INFO: Calculating overlaps between adjacent uncompressed edges\n",
      "00:02:15 3.5Gb  INFO: Printing final gfa file to \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x/mdbg.gfa\"\n",
      "00:02:15 3.5Gb  INFO: Printing final assembly to \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x/assembly.fasta\"\n",
      "00:02:15 5Mb  INFO: Final homopolymer compressed and corrected reads can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x/k5001/corrected_reads.fasta\"\n",
      "00:02:15 5Mb  INFO: Final graph with homopolymer compressed edges can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x/mdbg/mdbg.hpc.gfa\"\n",
      "00:02:15 5Mb  INFO: Final graph can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x/mdbg.gfa\"\n",
      "00:02:15 5Mb  INFO: Final assembly can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x/assembly.fasta\"\n",
      "00:02:15 5Mb  INFO: LJA pipeline finished\n",
      "00:00:00 2Mb  INFO: Hello! You are running La Jolla Assembler (LJA), a tool for genome assembly from PacBio HiFi reads\n",
      "00:00:12 2Mb  INFO: c04bae31911a636f112dee9c63ce1a6b8e834710\n",
      "00:00:12 2Mb  INFO: LJA pipeline started\n",
      "00:00:12 2Mb  INFO: Performing initial correction with k = 5001\n",
      "00:00:12 0Mb  INFO: Reading reads\n",
      "00:00:12 0Mb  INFO: Extracting minimizers\n",
      "00:00:42 6.4Gb  INFO: Finished read processing\n",
      "00:00:42 6.4Gb  INFO: 9394310 hashs collected. Starting sorting.\n",
      "00:00:42 6.4Gb  INFO: Finished sorting. Total distinct minimizers: 10161\n",
      "00:00:42 6.4Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:42 6.4Gb  INFO: Vertex map constructed.\n",
      "00:00:42 6.4Gb  INFO: Filling edge sequences.\n",
      "00:01:12 6.5Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:01:12 6.5Gb  INFO:  Collecting tips \n",
      "00:01:12 6.5Gb  INFO: Added 29 artificial minimizers from tips.\n",
      "00:01:12 6.5Gb  INFO: Collected 20508 old edges.\n",
      "00:01:12 6.5Gb  INFO: New minimizers added to sparse graph.\n",
      "00:01:12 6.5Gb  INFO: Refilling graph with old edges.\n",
      "00:01:12 6.5Gb  INFO: Filling graph with new edges.\n",
      "00:01:12 6.5Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:01:12 6.5Gb  INFO: Starting to extract disjointigs.\n",
      "00:01:12 6.5Gb  INFO: Finished extracting 327 disjointigs of total size 4183486\n",
      "00:01:13 0Mb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x/k5001/disjointigs.fasta\"\n",
      "00:01:13 14Mb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:01:14 15Mb  INFO: Filled 11795859 bits out of 81541088\n",
      "00:01:14 15Mb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:01:14 15Mb  INFO: Collected 1242 junctions.\n",
      "00:01:14 15Mb  INFO: Starting DBG construction.\n",
      "00:01:14 15Mb  INFO: Vertices created.\n",
      "00:01:14 15Mb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:01:14 15Mb  INFO: Added 0 hanging vertices\n",
      "00:01:14 15Mb  INFO: Merging unbranching paths\n",
      "00:01:14 15Mb  INFO: Ended merging edges. Resulting size 244\n",
      "00:01:15 15Mb  INFO: Cleaning edge coverages\n",
      "00:01:15 15Mb  INFO: Collecting alignments of sequences to the graph\n",
      "00:01:36 6.8Gb  INFO: Alignment collection finished. Total length of alignments is 887388\n",
      "00:01:36 6.8Gb  INFO: Could not correct 181 reads. They will be removed.\n",
      "00:01:36 6.8Gb  INFO: Uncorrected reads were removed.\n",
      "00:01:36 6.8Gb  INFO: Applying changes to the graph\n",
      "00:01:42 7Gb  INFO: Printing reads to fasta file \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x/k5001/corrected_reads.fasta\"\n",
      "00:02:40 5Mb  INFO: Performing repeat resolution by transforming de Bruijn graph into Multiplex de Bruijn graph\n",
      "00:02:40 0Mb  INFO: Loading graph from fasta\n",
      "00:02:41 12Mb  INFO: Finished loading graph\n",
      "00:02:42 245Mb  INFO: Looking for unique edges\n",
      "00:02:42 245Mb  INFO: Marked 20 long edges as unique\n",
      "00:02:42 245Mb  INFO: Marking extra edges as unique based on read paths\n",
      "00:02:42 245Mb  INFO: Marked 20 edges as unique\n",
      "00:02:42 245Mb  INFO: Splitting graph with unique edges\n",
      "00:02:42 245Mb  INFO: Processing 12 components\n",
      "00:02:42 245Mb  INFO: Finished unique edges search. Found 208 unique edges\n",
      "00:02:42 245Mb  INFO: Analysing repeats of multiplicity 2 and looking for additional unique edges\n",
      "00:02:42 245Mb  INFO: Finished processing of repeats of multiplicity 2. Found 0 erroneous edges.\n",
      "00:02:42 245Mb  INFO: Resolving repeats\n",
      "00:02:42 245Mb  INFO: Constructing paths\n",
      "00:02:49 0.5Gb  INFO: Building graph\n",
      "00:02:49 0.5Gb  INFO: Increasing k\n",
      "00:02:52 0.5Gb  INFO: Finished increasing k\n",
      "00:02:52 0.5Gb  INFO: Exporting remaining active transitions\n",
      "00:02:52 0.5Gb  INFO: Export to Dot\n",
      "00:02:52 0.5Gb  INFO: Export to GFA and compressed contigs\n",
      "00:02:53 0.5Gb  INFO: Finished repeat resolution\n",
      "00:02:54 5Mb  INFO: Performing polishing and homopolymer uncompression\n",
      "00:02:54 11Mb  INFO: Aligning reads back to assembly\n",
      "00:03:26 5.9Gb  INFO: Finished alignment.\n",
      "00:03:26 5.9Gb  INFO: Printing alignments to \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x/uncompressing/alignments.txt\"\n",
      "00:03:27 5.9Gb  INFO: Reading and processing initial reads from [\"phasing-data/smoothed-reads/edge_2358_smoothed_reads.fasta\"]\n",
      "00:09:07 5.9Gb  INFO: Uncompressing homopolymers in contigs\n",
      "00:09:08 5.9Gb  INFO: Total zero covered nucleotides 0\n",
      "00:09:08 5.9Gb  INFO: Calculating overlaps between adjacent uncompressed edges\n",
      "00:09:08 5.9Gb  INFO: Printing final gfa file to \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x/mdbg.gfa\"\n",
      "00:09:08 5.9Gb  INFO: Printing final assembly to \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x/assembly.fasta\"\n",
      "00:09:09 5Mb  INFO: Final homopolymer compressed and corrected reads can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x/k5001/corrected_reads.fasta\"\n",
      "00:09:09 5Mb  INFO: Final graph with homopolymer compressed edges can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x/mdbg/mdbg.hpc.gfa\"\n",
      "00:09:09 5Mb  INFO: Final graph can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x/mdbg.gfa\"\n",
      "00:09:09 5Mb  INFO: Final assembly can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x/assembly.fasta\"\n",
      "00:09:09 5Mb  INFO: LJA pipeline finished\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# LJA with no error correction, but with a filter for low-coverage edges\n",
    "\n",
    "OUTDIR=phasing-data/smoothed-reads\n",
    "\n",
    "/home/mfedarko/software/LJA-branch/bin/lja \\\n",
    "    --reads $OUTDIR/edge_6104_smoothed_reads.fasta \\\n",
    "    --simpleec \\\n",
    "    --Cov-threshold 10 \\\n",
    "    --output-dir $OUTDIR/edge_6104_lja_cf_10x\n",
    "\n",
    "/home/mfedarko/software/LJA-branch/bin/lja \\\n",
    "    --reads $OUTDIR/edge_1671_smoothed_reads.fasta \\\n",
    "    --simpleec \\\n",
    "    --Cov-threshold 10 \\\n",
    "    --output-dir $OUTDIR/edge_1671_lja_cf_10x\n",
    "\n",
    "/home/mfedarko/software/LJA-branch/bin/lja \\\n",
    "    --reads $OUTDIR/edge_2358_smoothed_reads.fasta \\\n",
    "    --simpleec \\\n",
    "    --Cov-threshold 10 \\\n",
    "    --output-dir $OUTDIR/edge_2358_lja_cf_10x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
