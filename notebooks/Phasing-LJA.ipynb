{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform read smoothing then assemble with LJA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T02:19:10.898532Z",
     "iopub.status.busy": "2022-04-11T02:19:10.897631Z",
     "iopub.status.idle": "2022-04-11T02:19:11.395955Z",
     "shell.execute_reply": "2022-04-11T02:19:11.395483Z"
    }
   },
   "outputs": [],
   "source": [
    "%run \"Header.ipynb\"\n",
    "%run \"../main-workflow/utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T02:19:11.399126Z",
     "iopub.status.busy": "2022-04-11T02:19:11.398730Z",
     "iopub.status.idle": "2022-04-11T02:19:11.794310Z",
     "shell.execute_reply": "2022-04-11T02:19:11.794929Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pysam\n",
    "import skbio\n",
    "from collections import defaultdict, Counter\n",
    "from linked_mutations_utils import find_mutated_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Quick sanity check: ensure that all $k$-mers ($k$ = 5,001) are unique in each MAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T02:19:11.804973Z",
     "iopub.status.busy": "2022-04-11T02:19:11.804143Z",
     "iopub.status.idle": "2022-04-11T02:21:08.046642Z",
     "shell.execute_reply": "2022-04-11T02:21:08.047424Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On start pos 0 in CAMP.\n",
      "On start pos 1,000,000 in CAMP.\n",
      "The most common k = 5,001-mer in CAMP occurred 1 time(s).\n",
      "On start pos 0 in BACT1.\n",
      "On start pos 1,000,000 in BACT1.\n",
      "On start pos 2,000,000 in BACT1.\n",
      "The most common k = 5,001-mer in BACT1 occurred 1 time(s).\n",
      "On start pos 0 in BACT2.\n",
      "On start pos 1,000,000 in BACT2.\n",
      "On start pos 2,000,000 in BACT2.\n",
      "The most common k = 5,001-mer in BACT2 occurred 1 time(s).\n"
     ]
    }
   ],
   "source": [
    "# I know there are actual k-mer counting tools you can use but no reason to overcomplicate things for now\n",
    "\n",
    "k = 5001\n",
    "\n",
    "for seq in SEQS:\n",
    "    fasta = skbio.DNA.read(f\"../seqs/{seq}.fasta\")\n",
    "    bargain_bin_kmer_counter = Counter()\n",
    "    \n",
    "    # The skbio.DNA object is 0-indexed, so 0 is the leftmost k-mer start position and\n",
    "    # ((seq length) - k) is the rightmost k-mer start position. The + 1 is because python ranges don't include\n",
    "    # the right endpoint.\n",
    "    for start_pos in range(0, seq2len[seq] - k + 1):\n",
    "        \n",
    "        # NOTE: this is a terrible no good very bad way to do this; it's more efficient to use a \"sliding window\"\n",
    "        # approach where you store the entire k-mer and then, with each step, just remove the first character and\n",
    "        # add on a new last character. \"But, uh, this code will only be run on these three MAGs, so I'm gonna\n",
    "        # prioritize clarity over optimization,\" says me, the insane person who just spent like a minute writing\n",
    "        # this comment when I could've been optimizing this code instead look WHATEVER this counts k-mers and it's\n",
    "        # 4am let's not overcomplicate it, look if you're on GitHub right now and you see this inane comment\n",
    "        # we can both just pretend that you were looking at some really optimized code and we'll both walk away\n",
    "        # satisfied, capisce\n",
    "        kmer = fasta[start_pos : start_pos + k]\n",
    "        \n",
    "        bargain_bin_kmer_counter[str(kmer)] += 1\n",
    "        if start_pos % 1000000 == 0: print(f\"On start pos {start_pos:,} in {seq2name[seq]}.\")\n",
    "    \n",
    "    mckc = bargain_bin_kmer_counter.most_common(1)[0][1]\n",
    "    print(f\"The most common k = {k:,}-mer in {seq2name[seq]} occurred {mckc:,} time(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Smooth reads\n",
    "\n",
    "Lots of this code is duplicated from the `Phasing-01-MakeGraph.ipynb` notebook in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T02:21:08.054642Z",
     "iopub.status.busy": "2022-04-11T02:21:08.053804Z",
     "iopub.status.idle": "2022-04-11T02:21:08.056555Z",
     "shell.execute_reply": "2022-04-11T02:21:08.055864Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set this to True to actually generate ordinary smoothed reads that include called mutations;\n",
    "# set this to False to generate \"sanity check\" perfect smoothed reads, where no mutations are included\n",
    "# and the read entirely matches the reference\n",
    "actually_include_mutations_in_the_smoothed_reads = True\n",
    "\n",
    "add_virtual_reads = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T02:21:08.090957Z",
     "iopub.status.busy": "2022-04-11T02:21:08.061031Z",
     "iopub.status.idle": "2022-04-11T02:21:25.427176Z",
     "shell.execute_reply": "2022-04-11T02:21:25.427825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence edge_6104 has average coverage 4,158.57 and median coverage 4,122.00.\n",
      "Sequence edge_1671 has average coverage 1,415.07 and median coverage 1,436.00.\n",
      "Sequence edge_2358 has average coverage 2,993.46 and median coverage 2,936.00.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'edge_6104': 4158.572468826692,\n",
       " 'edge_1671': 1415.072755380576,\n",
       " 'edge_2358': 2993.461913625056}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll need to know the mean coverage of each sequence when computing virtual reads.\n",
    "seq2meancov = get_meancovs()\n",
    "seq2meancov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T02:21:25.448026Z",
     "iopub.status.busy": "2022-04-11T02:21:25.430782Z",
     "iopub.status.idle": "2022-04-11T05:01:06.365336Z",
     "shell.execute_reply": "2022-04-11T05:01:06.364510Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Identifying mutated positions (p = 1%) in genome CAMP...\n",
      "Found 83 mutated positions (p = 1%) in CAMP.\n",
      "Note that this tally is higher than you'd see in e.g. the CP1/2/3 plots, because now we're including both 'rare' and non-rare mutations. Just so you don't waste five minutes sanity-checking this like I did.\n",
      "Going through these positions...\n",
      "\tOn aln 5,000 in seq CAMP. Time spent on CAMP so far: 13.78 sec.\n",
      "\tOn aln 10,000 in seq CAMP. Time spent on CAMP so far: 34.37 sec.\n",
      "\tOn aln 15,000 in seq CAMP. Time spent on CAMP so far: 54.41 sec.\n",
      "\tOn aln 20,000 in seq CAMP. Time spent on CAMP so far: 75.94 sec.\n",
      "\tOn aln 25,000 in seq CAMP. Time spent on CAMP so far: 96.40 sec.\n",
      "\tOn aln 30,000 in seq CAMP. Time spent on CAMP so far: 115.78 sec.\n",
      "\tOn aln 35,000 in seq CAMP. Time spent on CAMP so far: 135.05 sec.\n",
      "\tOn aln 40,000 in seq CAMP. Time spent on CAMP so far: 154.97 sec.\n",
      "\tOn aln 45,000 in seq CAMP. Time spent on CAMP so far: 175.16 sec.\n",
      "\tOn aln 50,000 in seq CAMP. Time spent on CAMP so far: 193.84 sec.\n",
      "\tOn aln 55,000 in seq CAMP. Time spent on CAMP so far: 213.35 sec.\n",
      "\tOn aln 60,000 in seq CAMP. Time spent on CAMP so far: 232.73 sec.\n",
      "\tOn aln 65,000 in seq CAMP. Time spent on CAMP so far: 252.51 sec.\n",
      "\tOn aln 70,000 in seq CAMP. Time spent on CAMP so far: 274.24 sec.\n",
      "\tOn aln 75,000 in seq CAMP. Time spent on CAMP so far: 294.92 sec.\n",
      "\tOn aln 80,000 in seq CAMP. Time spent on CAMP so far: 316.04 sec.\n",
      "\tOn aln 85,000 in seq CAMP. Time spent on CAMP so far: 337.55 sec.\n",
      "\tOn aln 90,000 in seq CAMP. Time spent on CAMP so far: 358.84 sec.\n",
      "\tOn aln 95,000 in seq CAMP. Time spent on CAMP so far: 378.45 sec.\n",
      "\tOn aln 100,000 in seq CAMP. Time spent on CAMP so far: 398.12 sec.\n",
      "\tOn aln 105,000 in seq CAMP. Time spent on CAMP so far: 416.97 sec.\n",
      "\tOn aln 110,000 in seq CAMP. Time spent on CAMP so far: 435.53 sec.\n",
      "\tOn aln 115,000 in seq CAMP. Time spent on CAMP so far: 454.48 sec.\n",
      "\tOn aln 120,000 in seq CAMP. Time spent on CAMP so far: 473.55 sec.\n",
      "\tOn aln 125,000 in seq CAMP. Time spent on CAMP so far: 492.09 sec.\n",
      "\tOn aln 130,000 in seq CAMP. Time spent on CAMP so far: 510.65 sec.\n",
      "\tOn aln 135,000 in seq CAMP. Time spent on CAMP so far: 529.52 sec.\n",
      "\tOn aln 140,000 in seq CAMP. Time spent on CAMP so far: 548.98 sec.\n",
      "\tOn aln 145,000 in seq CAMP. Time spent on CAMP so far: 569.52 sec.\n",
      "\tOn aln 150,000 in seq CAMP. Time spent on CAMP so far: 588.20 sec.\n",
      "\tOn aln 155,000 in seq CAMP. Time spent on CAMP so far: 607.13 sec.\n",
      "\tOn aln 160,000 in seq CAMP. Time spent on CAMP so far: 627.16 sec.\n",
      "\tOn aln 165,000 in seq CAMP. Time spent on CAMP so far: 647.08 sec.\n",
      "\tOn aln 170,000 in seq CAMP. Time spent on CAMP so far: 666.55 sec.\n",
      "\tOn aln 175,000 in seq CAMP. Time spent on CAMP so far: 686.35 sec.\n",
      "\tOn aln 180,000 in seq CAMP. Time spent on CAMP so far: 705.82 sec.\n",
      "\tOn aln 185,000 in seq CAMP. Time spent on CAMP so far: 725.18 sec.\n",
      "\tOn aln 190,000 in seq CAMP. Time spent on CAMP so far: 743.48 sec.\n",
      "\tOn aln 195,000 in seq CAMP. Time spent on CAMP so far: 759.29 sec.\n",
      "\tOn aln 200,000 in seq CAMP. Time spent on CAMP so far: 778.16 sec.\n",
      "\tOn aln 205,000 in seq CAMP. Time spent on CAMP so far: 797.30 sec.\n",
      "\tOn aln 210,000 in seq CAMP. Time spent on CAMP so far: 816.87 sec.\n",
      "\tOn aln 215,000 in seq CAMP. Time spent on CAMP so far: 836.91 sec.\n",
      "\tOn aln 220,000 in seq CAMP. Time spent on CAMP so far: 856.08 sec.\n",
      "\tOn aln 225,000 in seq CAMP. Time spent on CAMP so far: 875.44 sec.\n",
      "\tOn aln 230,000 in seq CAMP. Time spent on CAMP so far: 894.54 sec.\n",
      "\tOn aln 235,000 in seq CAMP. Time spent on CAMP so far: 914.27 sec.\n",
      "\tOn aln 240,000 in seq CAMP. Time spent on CAMP so far: 933.77 sec.\n",
      "\tOn aln 245,000 in seq CAMP. Time spent on CAMP so far: 952.93 sec.\n",
      "\tOn aln 250,000 in seq CAMP. Time spent on CAMP so far: 972.34 sec.\n",
      "\tOn aln 255,000 in seq CAMP. Time spent on CAMP so far: 991.32 sec.\n",
      "\tOn aln 260,000 in seq CAMP. Time spent on CAMP so far: 1,010.27 sec.\n",
      "\tOn aln 265,000 in seq CAMP. Time spent on CAMP so far: 1,030.28 sec.\n",
      "\tOn aln 270,000 in seq CAMP. Time spent on CAMP so far: 1,050.80 sec.\n",
      "\tOn aln 275,000 in seq CAMP. Time spent on CAMP so far: 1,070.87 sec.\n",
      "\tOn aln 280,000 in seq CAMP. Time spent on CAMP so far: 1,089.97 sec.\n",
      "\tOn aln 285,000 in seq CAMP. Time spent on CAMP so far: 1,109.00 sec.\n",
      "\tOn aln 290,000 in seq CAMP. Time spent on CAMP so far: 1,127.55 sec.\n",
      "\tOn aln 295,000 in seq CAMP. Time spent on CAMP so far: 1,146.18 sec.\n",
      "\tOn aln 300,000 in seq CAMP. Time spent on CAMP so far: 1,167.59 sec.\n",
      "\tOn aln 305,000 in seq CAMP. Time spent on CAMP so far: 1,188.95 sec.\n",
      "\tOn aln 310,000 in seq CAMP. Time spent on CAMP so far: 1,210.89 sec.\n",
      "\tOn aln 315,000 in seq CAMP. Time spent on CAMP so far: 1,232.99 sec.\n",
      "\tOn aln 320,000 in seq CAMP. Time spent on CAMP so far: 1,253.94 sec.\n",
      "\tOn aln 325,000 in seq CAMP. Time spent on CAMP so far: 1,273.87 sec.\n",
      "\tOn aln 330,000 in seq CAMP. Time spent on CAMP so far: 1,295.65 sec.\n",
      "\tOn aln 335,000 in seq CAMP. Time spent on CAMP so far: 1,316.22 sec.\n",
      "\tOn aln 340,000 in seq CAMP. Time spent on CAMP so far: 1,336.67 sec.\n",
      "\tOn aln 345,000 in seq CAMP. Time spent on CAMP so far: 1,356.52 sec.\n",
      "\tOn aln 350,000 in seq CAMP. Time spent on CAMP so far: 1,376.13 sec.\n",
      "\tOn aln 355,000 in seq CAMP. Time spent on CAMP so far: 1,396.31 sec.\n",
      "\tOn aln 360,000 in seq CAMP. Time spent on CAMP so far: 1,418.86 sec.\n",
      "\tOn aln 365,000 in seq CAMP. Time spent on CAMP so far: 1,441.45 sec.\n",
      "\tOn aln 370,000 in seq CAMP. Time spent on CAMP so far: 1,462.82 sec.\n",
      "\tOn aln 375,000 in seq CAMP. Time spent on CAMP so far: 1,482.86 sec.\n",
      "\tOn aln 380,000 in seq CAMP. Time spent on CAMP so far: 1,506.43 sec.\n",
      "\tOn aln 385,000 in seq CAMP. Time spent on CAMP so far: 1,531.25 sec.\n",
      "\tOn aln 390,000 in seq CAMP. Time spent on CAMP so far: 1,552.02 sec.\n",
      "\tOn aln 395,000 in seq CAMP. Time spent on CAMP so far: 1,572.88 sec.\n",
      "\tOn aln 400,000 in seq CAMP. Time spent on CAMP so far: 1,591.62 sec.\n",
      "\tOn aln 405,000 in seq CAMP. Time spent on CAMP so far: 1,610.20 sec.\n",
      "\tOn aln 410,000 in seq CAMP. Time spent on CAMP so far: 1,629.05 sec.\n",
      "\tOn aln 415,000 in seq CAMP. Time spent on CAMP so far: 1,648.11 sec.\n",
      "\tOn aln 420,000 in seq CAMP. Time spent on CAMP so far: 1,668.98 sec.\n",
      "\tOn aln 425,000 in seq CAMP. Time spent on CAMP so far: 1,690.04 sec.\n",
      "\tOn aln 430,000 in seq CAMP. Time spent on CAMP so far: 1,709.22 sec.\n",
      "\tOn aln 435,000 in seq CAMP. Time spent on CAMP so far: 1,729.20 sec.\n",
      "\tOn aln 440,000 in seq CAMP. Time spent on CAMP so far: 1,749.83 sec.\n",
      "\tOn aln 445,000 in seq CAMP. Time spent on CAMP so far: 1,778.26 sec.\n",
      "\tOn aln 450,000 in seq CAMP. Time spent on CAMP so far: 1,787.33 sec.\n",
      "\tOn aln 455,000 in seq CAMP. Time spent on CAMP so far: 1,795.28 sec.\n",
      "\tOn aln 460,000 in seq CAMP. Time spent on CAMP so far: 1,802.32 sec.\n",
      "\tOn aln 465,000 in seq CAMP. Time spent on CAMP so far: 1,809.34 sec.\n",
      "\tOn aln 470,000 in seq CAMP. Time spent on CAMP so far: 1,816.56 sec.\n",
      "\tOn aln 475,000 in seq CAMP. Time spent on CAMP so far: 1,823.61 sec.\n",
      "We ignored 3,742 linear alignments, fyi.\n",
      "Done with edge_6104! Took 1,825.28 sec.\n",
      "======================================================================\n",
      "Identifying mutated positions (p = 1%) in genome BACT1...\n",
      "Found 22,144 mutated positions (p = 1%) in BACT1.\n",
      "Note that this tally is higher than you'd see in e.g. the CP1/2/3 plots, because now we're including both 'rare' and non-rare mutations. Just so you don't waste five minutes sanity-checking this like I did.\n",
      "Going through these positions...\n",
      "\tOn aln 5,000 in seq BACT1. Time spent on BACT1 so far: 50.12 sec.\n",
      "\tOn aln 10,000 in seq BACT1. Time spent on BACT1 so far: 122.53 sec.\n",
      "\tOn aln 15,000 in seq BACT1. Time spent on BACT1 so far: 211.98 sec.\n",
      "\tOn aln 20,000 in seq BACT1. Time spent on BACT1 so far: 283.91 sec.\n",
      "\tOn aln 25,000 in seq BACT1. Time spent on BACT1 so far: 381.89 sec.\n",
      "\tOn aln 30,000 in seq BACT1. Time spent on BACT1 so far: 477.10 sec.\n",
      "\tOn aln 35,000 in seq BACT1. Time spent on BACT1 so far: 560.86 sec.\n",
      "\tOn aln 40,000 in seq BACT1. Time spent on BACT1 so far: 627.34 sec.\n",
      "\tOn aln 45,000 in seq BACT1. Time spent on BACT1 so far: 722.20 sec.\n",
      "\tOn aln 50,000 in seq BACT1. Time spent on BACT1 so far: 794.67 sec.\n",
      "\tOn aln 55,000 in seq BACT1. Time spent on BACT1 so far: 869.66 sec.\n",
      "\tOn aln 60,000 in seq BACT1. Time spent on BACT1 so far: 969.83 sec.\n",
      "\tOn aln 65,000 in seq BACT1. Time spent on BACT1 so far: 1,067.46 sec.\n",
      "\tOn aln 70,000 in seq BACT1. Time spent on BACT1 so far: 1,134.18 sec.\n",
      "\tOn aln 75,000 in seq BACT1. Time spent on BACT1 so far: 1,233.82 sec.\n",
      "\tOn aln 80,000 in seq BACT1. Time spent on BACT1 so far: 1,303.64 sec.\n",
      "\tOn aln 85,000 in seq BACT1. Time spent on BACT1 so far: 1,371.10 sec.\n",
      "\tOn aln 90,000 in seq BACT1. Time spent on BACT1 so far: 1,443.69 sec.\n",
      "\tOn aln 95,000 in seq BACT1. Time spent on BACT1 so far: 1,522.10 sec.\n",
      "\tOn aln 100,000 in seq BACT1. Time spent on BACT1 so far: 1,619.72 sec.\n",
      "\tOn aln 105,000 in seq BACT1. Time spent on BACT1 so far: 1,700.02 sec.\n",
      "\tOn aln 110,000 in seq BACT1. Time spent on BACT1 so far: 1,808.85 sec.\n",
      "\tOn aln 115,000 in seq BACT1. Time spent on BACT1 so far: 1,889.06 sec.\n",
      "\tOn aln 120,000 in seq BACT1. Time spent on BACT1 so far: 1,970.09 sec.\n",
      "\tOn aln 125,000 in seq BACT1. Time spent on BACT1 so far: 2,078.19 sec.\n",
      "\tOn aln 130,000 in seq BACT1. Time spent on BACT1 so far: 2,197.55 sec.\n",
      "\tOn aln 135,000 in seq BACT1. Time spent on BACT1 so far: 2,277.24 sec.\n",
      "\tOn aln 140,000 in seq BACT1. Time spent on BACT1 so far: 2,338.50 sec.\n",
      "\tOn aln 145,000 in seq BACT1. Time spent on BACT1 so far: 2,402.74 sec.\n",
      "\tOn aln 150,000 in seq BACT1. Time spent on BACT1 so far: 2,473.18 sec.\n",
      "\tOn aln 155,000 in seq BACT1. Time spent on BACT1 so far: 2,566.35 sec.\n",
      "\tOn aln 160,000 in seq BACT1. Time spent on BACT1 so far: 2,688.67 sec.\n",
      "\tOn aln 165,000 in seq BACT1. Time spent on BACT1 so far: 2,775.93 sec.\n",
      "\tOn aln 170,000 in seq BACT1. Time spent on BACT1 so far: 2,902.36 sec.\n",
      "\tOn aln 175,000 in seq BACT1. Time spent on BACT1 so far: 3,003.16 sec.\n",
      "\tOn aln 180,000 in seq BACT1. Time spent on BACT1 so far: 3,082.60 sec.\n",
      "\tOn aln 185,000 in seq BACT1. Time spent on BACT1 so far: 3,170.89 sec.\n",
      "\tOn aln 190,000 in seq BACT1. Time spent on BACT1 so far: 3,263.63 sec.\n",
      "\tOn aln 195,000 in seq BACT1. Time spent on BACT1 so far: 3,362.37 sec.\n",
      "\tOn aln 200,000 in seq BACT1. Time spent on BACT1 so far: 3,479.58 sec.\n",
      "\tOn aln 205,000 in seq BACT1. Time spent on BACT1 so far: 3,555.89 sec.\n",
      "\tOn aln 210,000 in seq BACT1. Time spent on BACT1 so far: 3,644.66 sec.\n",
      "\tOn aln 215,000 in seq BACT1. Time spent on BACT1 so far: 3,773.44 sec.\n",
      "\tOn aln 220,000 in seq BACT1. Time spent on BACT1 so far: 3,869.20 sec.\n",
      "\tOn aln 225,000 in seq BACT1. Time spent on BACT1 so far: 3,971.35 sec.\n",
      "\tOn aln 230,000 in seq BACT1. Time spent on BACT1 so far: 4,056.25 sec.\n",
      "\tOn aln 235,000 in seq BACT1. Time spent on BACT1 so far: 4,130.77 sec.\n",
      "\tOn aln 240,000 in seq BACT1. Time spent on BACT1 so far: 4,237.35 sec.\n",
      "\tOn aln 245,000 in seq BACT1. Time spent on BACT1 so far: 4,356.81 sec.\n",
      "\tOn aln 250,000 in seq BACT1. Time spent on BACT1 so far: 4,468.94 sec.\n",
      "\tOn aln 255,000 in seq BACT1. Time spent on BACT1 so far: 4,550.56 sec.\n",
      "\tOn aln 260,000 in seq BACT1. Time spent on BACT1 so far: 4,670.77 sec.\n",
      "We ignored 30,472 linear alignments, fyi.\n",
      "For reference, there are 2,035 uncovered positions in BACT1.\n",
      "And there are 2 \"runs\" of uncovered positions.\n",
      "Adding \"virtual reads\" spanning each of these runs, at rounded mean coverage of 1,415x, to account for this...\n",
      "Wrote out 2,830 virtual reads.\n",
      "Done with edge_1671! Took 4,701.62 sec.\n",
      "======================================================================\n",
      "Identifying mutated positions (p = 1%) in genome BACT2...\n",
      "Found 372 mutated positions (p = 1%) in BACT2.\n",
      "Note that this tally is higher than you'd see in e.g. the CP1/2/3 plots, because now we're including both 'rare' and non-rare mutations. Just so you don't waste five minutes sanity-checking this like I did.\n",
      "Going through these positions...\n",
      "\tOn aln 5,000 in seq BACT2. Time spent on BACT2 so far: 22.37 sec.\n",
      "\tOn aln 10,000 in seq BACT2. Time spent on BACT2 so far: 48.30 sec.\n",
      "\tOn aln 15,000 in seq BACT2. Time spent on BACT2 so far: 69.78 sec.\n",
      "\tOn aln 20,000 in seq BACT2. Time spent on BACT2 so far: 92.83 sec.\n",
      "\tOn aln 25,000 in seq BACT2. Time spent on BACT2 so far: 119.11 sec.\n",
      "\tOn aln 30,000 in seq BACT2. Time spent on BACT2 so far: 143.59 sec.\n",
      "\tOn aln 35,000 in seq BACT2. Time spent on BACT2 so far: 165.86 sec.\n",
      "\tOn aln 40,000 in seq BACT2. Time spent on BACT2 so far: 191.47 sec.\n",
      "\tOn aln 45,000 in seq BACT2. Time spent on BACT2 so far: 213.40 sec.\n",
      "\tOn aln 50,000 in seq BACT2. Time spent on BACT2 so far: 232.89 sec.\n",
      "\tOn aln 55,000 in seq BACT2. Time spent on BACT2 so far: 259.05 sec.\n",
      "\tOn aln 60,000 in seq BACT2. Time spent on BACT2 so far: 282.07 sec.\n",
      "\tOn aln 65,000 in seq BACT2. Time spent on BACT2 so far: 304.55 sec.\n",
      "\tOn aln 70,000 in seq BACT2. Time spent on BACT2 so far: 330.05 sec.\n",
      "\tOn aln 75,000 in seq BACT2. Time spent on BACT2 so far: 355.89 sec.\n",
      "\tOn aln 80,000 in seq BACT2. Time spent on BACT2 so far: 380.59 sec.\n",
      "\tOn aln 85,000 in seq BACT2. Time spent on BACT2 so far: 405.29 sec.\n",
      "\tOn aln 90,000 in seq BACT2. Time spent on BACT2 so far: 435.67 sec.\n",
      "\tOn aln 95,000 in seq BACT2. Time spent on BACT2 so far: 473.10 sec.\n",
      "\tOn aln 100,000 in seq BACT2. Time spent on BACT2 so far: 495.34 sec.\n",
      "\tOn aln 105,000 in seq BACT2. Time spent on BACT2 so far: 518.11 sec.\n",
      "\tOn aln 110,000 in seq BACT2. Time spent on BACT2 so far: 543.77 sec.\n",
      "\tOn aln 115,000 in seq BACT2. Time spent on BACT2 so far: 564.97 sec.\n",
      "\tOn aln 120,000 in seq BACT2. Time spent on BACT2 so far: 587.59 sec.\n",
      "\tOn aln 125,000 in seq BACT2. Time spent on BACT2 so far: 607.84 sec.\n",
      "\tOn aln 130,000 in seq BACT2. Time spent on BACT2 so far: 639.53 sec.\n",
      "\tOn aln 135,000 in seq BACT2. Time spent on BACT2 so far: 661.36 sec.\n",
      "\tOn aln 140,000 in seq BACT2. Time spent on BACT2 so far: 679.57 sec.\n",
      "\tOn aln 145,000 in seq BACT2. Time spent on BACT2 so far: 699.38 sec.\n",
      "\tOn aln 150,000 in seq BACT2. Time spent on BACT2 so far: 719.47 sec.\n",
      "\tOn aln 155,000 in seq BACT2. Time spent on BACT2 so far: 740.05 sec.\n",
      "\tOn aln 160,000 in seq BACT2. Time spent on BACT2 so far: 763.02 sec.\n",
      "\tOn aln 165,000 in seq BACT2. Time spent on BACT2 so far: 784.57 sec.\n",
      "\tOn aln 170,000 in seq BACT2. Time spent on BACT2 so far: 806.45 sec.\n",
      "\tOn aln 175,000 in seq BACT2. Time spent on BACT2 so far: 827.76 sec.\n",
      "\tOn aln 180,000 in seq BACT2. Time spent on BACT2 so far: 850.42 sec.\n",
      "\tOn aln 185,000 in seq BACT2. Time spent on BACT2 so far: 871.81 sec.\n",
      "\tOn aln 190,000 in seq BACT2. Time spent on BACT2 so far: 892.96 sec.\n",
      "\tOn aln 195,000 in seq BACT2. Time spent on BACT2 so far: 910.68 sec.\n",
      "\tOn aln 200,000 in seq BACT2. Time spent on BACT2 so far: 927.64 sec.\n",
      "\tOn aln 205,000 in seq BACT2. Time spent on BACT2 so far: 948.60 sec.\n",
      "\tOn aln 210,000 in seq BACT2. Time spent on BACT2 so far: 969.16 sec.\n",
      "\tOn aln 215,000 in seq BACT2. Time spent on BACT2 so far: 990.09 sec.\n",
      "\tOn aln 220,000 in seq BACT2. Time spent on BACT2 so far: 1,010.65 sec.\n",
      "\tOn aln 225,000 in seq BACT2. Time spent on BACT2 so far: 1,029.72 sec.\n",
      "\tOn aln 230,000 in seq BACT2. Time spent on BACT2 so far: 1,050.34 sec.\n",
      "\tOn aln 235,000 in seq BACT2. Time spent on BACT2 so far: 1,069.34 sec.\n",
      "\tOn aln 240,000 in seq BACT2. Time spent on BACT2 so far: 1,089.26 sec.\n",
      "\tOn aln 245,000 in seq BACT2. Time spent on BACT2 so far: 1,105.33 sec.\n",
      "\tOn aln 250,000 in seq BACT2. Time spent on BACT2 so far: 1,119.01 sec.\n",
      "\tOn aln 255,000 in seq BACT2. Time spent on BACT2 so far: 1,134.91 sec.\n",
      "\tOn aln 260,000 in seq BACT2. Time spent on BACT2 so far: 1,151.13 sec.\n",
      "\tOn aln 265,000 in seq BACT2. Time spent on BACT2 so far: 1,168.99 sec.\n",
      "\tOn aln 270,000 in seq BACT2. Time spent on BACT2 so far: 1,188.73 sec.\n",
      "\tOn aln 275,000 in seq BACT2. Time spent on BACT2 so far: 1,207.00 sec.\n",
      "\tOn aln 280,000 in seq BACT2. Time spent on BACT2 so far: 1,226.40 sec.\n",
      "\tOn aln 285,000 in seq BACT2. Time spent on BACT2 so far: 1,247.32 sec.\n",
      "\tOn aln 290,000 in seq BACT2. Time spent on BACT2 so far: 1,268.82 sec.\n",
      "\tOn aln 295,000 in seq BACT2. Time spent on BACT2 so far: 1,290.02 sec.\n",
      "\tOn aln 300,000 in seq BACT2. Time spent on BACT2 so far: 1,310.27 sec.\n",
      "\tOn aln 305,000 in seq BACT2. Time spent on BACT2 so far: 1,331.04 sec.\n",
      "\tOn aln 310,000 in seq BACT2. Time spent on BACT2 so far: 1,349.91 sec.\n",
      "\tOn aln 315,000 in seq BACT2. Time spent on BACT2 so far: 1,367.00 sec.\n",
      "\tOn aln 320,000 in seq BACT2. Time spent on BACT2 so far: 1,384.58 sec.\n",
      "\tOn aln 325,000 in seq BACT2. Time spent on BACT2 so far: 1,404.11 sec.\n",
      "\tOn aln 330,000 in seq BACT2. Time spent on BACT2 so far: 1,420.06 sec.\n",
      "\tOn aln 335,000 in seq BACT2. Time spent on BACT2 so far: 1,439.24 sec.\n",
      "\tOn aln 340,000 in seq BACT2. Time spent on BACT2 so far: 1,458.66 sec.\n",
      "\tOn aln 345,000 in seq BACT2. Time spent on BACT2 so far: 1,478.14 sec.\n",
      "\tOn aln 350,000 in seq BACT2. Time spent on BACT2 so far: 1,497.84 sec.\n",
      "\tOn aln 355,000 in seq BACT2. Time spent on BACT2 so far: 1,515.92 sec.\n",
      "\tOn aln 360,000 in seq BACT2. Time spent on BACT2 so far: 1,532.62 sec.\n",
      "\tOn aln 365,000 in seq BACT2. Time spent on BACT2 so far: 1,548.99 sec.\n",
      "\tOn aln 370,000 in seq BACT2. Time spent on BACT2 so far: 1,567.86 sec.\n",
      "\tOn aln 375,000 in seq BACT2. Time spent on BACT2 so far: 1,583.00 sec.\n",
      "\tOn aln 380,000 in seq BACT2. Time spent on BACT2 so far: 1,603.23 sec.\n",
      "\tOn aln 385,000 in seq BACT2. Time spent on BACT2 so far: 1,623.64 sec.\n",
      "\tOn aln 390,000 in seq BACT2. Time spent on BACT2 so far: 1,642.92 sec.\n",
      "\tOn aln 395,000 in seq BACT2. Time spent on BACT2 so far: 1,663.88 sec.\n",
      "\tOn aln 400,000 in seq BACT2. Time spent on BACT2 so far: 1,682.20 sec.\n",
      "\tOn aln 405,000 in seq BACT2. Time spent on BACT2 so far: 1,694.65 sec.\n",
      "\tOn aln 410,000 in seq BACT2. Time spent on BACT2 so far: 1,710.81 sec.\n",
      "\tOn aln 415,000 in seq BACT2. Time spent on BACT2 so far: 1,732.65 sec.\n",
      "\tOn aln 420,000 in seq BACT2. Time spent on BACT2 so far: 1,756.97 sec.\n",
      "\tOn aln 425,000 in seq BACT2. Time spent on BACT2 so far: 1,776.40 sec.\n",
      "\tOn aln 430,000 in seq BACT2. Time spent on BACT2 so far: 1,799.17 sec.\n",
      "\tOn aln 435,000 in seq BACT2. Time spent on BACT2 so far: 1,815.78 sec.\n",
      "\tOn aln 440,000 in seq BACT2. Time spent on BACT2 so far: 1,834.13 sec.\n",
      "\tOn aln 445,000 in seq BACT2. Time spent on BACT2 so far: 1,856.06 sec.\n",
      "\tOn aln 450,000 in seq BACT2. Time spent on BACT2 so far: 1,879.69 sec.\n",
      "\tOn aln 455,000 in seq BACT2. Time spent on BACT2 so far: 1,901.96 sec.\n",
      "\tOn aln 460,000 in seq BACT2. Time spent on BACT2 so far: 1,922.63 sec.\n",
      "\tOn aln 465,000 in seq BACT2. Time spent on BACT2 so far: 1,940.29 sec.\n",
      "\tOn aln 470,000 in seq BACT2. Time spent on BACT2 so far: 1,960.27 sec.\n",
      "\tOn aln 475,000 in seq BACT2. Time spent on BACT2 so far: 1,981.48 sec.\n",
      "\tOn aln 480,000 in seq BACT2. Time spent on BACT2 so far: 2,001.46 sec.\n",
      "\tOn aln 485,000 in seq BACT2. Time spent on BACT2 so far: 2,018.31 sec.\n",
      "\tOn aln 490,000 in seq BACT2. Time spent on BACT2 so far: 2,039.45 sec.\n",
      "\tOn aln 495,000 in seq BACT2. Time spent on BACT2 so far: 2,059.74 sec.\n",
      "\tOn aln 500,000 in seq BACT2. Time spent on BACT2 so far: 2,079.99 sec.\n",
      "\tOn aln 505,000 in seq BACT2. Time spent on BACT2 so far: 2,102.09 sec.\n",
      "\tOn aln 510,000 in seq BACT2. Time spent on BACT2 so far: 2,123.34 sec.\n",
      "\tOn aln 515,000 in seq BACT2. Time spent on BACT2 so far: 2,138.95 sec.\n",
      "\tOn aln 520,000 in seq BACT2. Time spent on BACT2 so far: 2,161.19 sec.\n",
      "\tOn aln 525,000 in seq BACT2. Time spent on BACT2 so far: 2,184.33 sec.\n",
      "\tOn aln 530,000 in seq BACT2. Time spent on BACT2 so far: 2,204.45 sec.\n",
      "\tOn aln 535,000 in seq BACT2. Time spent on BACT2 so far: 2,225.28 sec.\n",
      "\tOn aln 540,000 in seq BACT2. Time spent on BACT2 so far: 2,246.76 sec.\n",
      "\tOn aln 545,000 in seq BACT2. Time spent on BACT2 so far: 2,268.34 sec.\n",
      "\tOn aln 550,000 in seq BACT2. Time spent on BACT2 so far: 2,288.91 sec.\n",
      "\tOn aln 555,000 in seq BACT2. Time spent on BACT2 so far: 2,306.69 sec.\n",
      "\tOn aln 560,000 in seq BACT2. Time spent on BACT2 so far: 2,322.85 sec.\n",
      "\tOn aln 565,000 in seq BACT2. Time spent on BACT2 so far: 2,341.32 sec.\n",
      "\tOn aln 570,000 in seq BACT2. Time spent on BACT2 so far: 2,362.61 sec.\n",
      "\tOn aln 575,000 in seq BACT2. Time spent on BACT2 so far: 2,380.31 sec.\n",
      "\tOn aln 580,000 in seq BACT2. Time spent on BACT2 so far: 2,394.43 sec.\n",
      "\tOn aln 585,000 in seq BACT2. Time spent on BACT2 so far: 2,409.68 sec.\n",
      "\tOn aln 590,000 in seq BACT2. Time spent on BACT2 so far: 2,428.81 sec.\n",
      "\tOn aln 595,000 in seq BACT2. Time spent on BACT2 so far: 2,445.90 sec.\n",
      "\tOn aln 600,000 in seq BACT2. Time spent on BACT2 so far: 2,461.83 sec.\n",
      "\tOn aln 605,000 in seq BACT2. Time spent on BACT2 so far: 2,483.67 sec.\n",
      "\tOn aln 610,000 in seq BACT2. Time spent on BACT2 so far: 2,505.20 sec.\n",
      "\tOn aln 615,000 in seq BACT2. Time spent on BACT2 so far: 2,523.14 sec.\n",
      "\tOn aln 620,000 in seq BACT2. Time spent on BACT2 so far: 2,543.95 sec.\n",
      "\tOn aln 625,000 in seq BACT2. Time spent on BACT2 so far: 2,564.40 sec.\n",
      "\tOn aln 630,000 in seq BACT2. Time spent on BACT2 so far: 2,583.39 sec.\n",
      "\tOn aln 635,000 in seq BACT2. Time spent on BACT2 so far: 2,604.49 sec.\n",
      "\tOn aln 640,000 in seq BACT2. Time spent on BACT2 so far: 2,624.58 sec.\n",
      "\tOn aln 645,000 in seq BACT2. Time spent on BACT2 so far: 2,641.19 sec.\n",
      "\tOn aln 650,000 in seq BACT2. Time spent on BACT2 so far: 2,660.73 sec.\n",
      "\tOn aln 655,000 in seq BACT2. Time spent on BACT2 so far: 2,677.78 sec.\n",
      "\tOn aln 660,000 in seq BACT2. Time spent on BACT2 so far: 2,701.13 sec.\n",
      "\tOn aln 665,000 in seq BACT2. Time spent on BACT2 so far: 2,723.01 sec.\n",
      "\tOn aln 670,000 in seq BACT2. Time spent on BACT2 so far: 2,742.15 sec.\n",
      "\tOn aln 675,000 in seq BACT2. Time spent on BACT2 so far: 2,763.37 sec.\n",
      "\tOn aln 680,000 in seq BACT2. Time spent on BACT2 so far: 2,782.34 sec.\n",
      "\tOn aln 685,000 in seq BACT2. Time spent on BACT2 so far: 2,803.68 sec.\n",
      "\tOn aln 690,000 in seq BACT2. Time spent on BACT2 so far: 2,824.72 sec.\n",
      "\tOn aln 695,000 in seq BACT2. Time spent on BACT2 so far: 2,849.56 sec.\n",
      "\tOn aln 700,000 in seq BACT2. Time spent on BACT2 so far: 2,869.90 sec.\n",
      "\tOn aln 705,000 in seq BACT2. Time spent on BACT2 so far: 2,886.21 sec.\n",
      "\tOn aln 710,000 in seq BACT2. Time spent on BACT2 so far: 2,902.52 sec.\n",
      "\tOn aln 715,000 in seq BACT2. Time spent on BACT2 so far: 2,921.27 sec.\n",
      "\tOn aln 720,000 in seq BACT2. Time spent on BACT2 so far: 2,939.93 sec.\n",
      "\tOn aln 725,000 in seq BACT2. Time spent on BACT2 so far: 2,960.98 sec.\n",
      "\tOn aln 730,000 in seq BACT2. Time spent on BACT2 so far: 2,981.29 sec.\n",
      "\tOn aln 735,000 in seq BACT2. Time spent on BACT2 so far: 2,996.72 sec.\n",
      "\tOn aln 740,000 in seq BACT2. Time spent on BACT2 so far: 3,004.94 sec.\n",
      "We ignored 147,492 linear alignments, fyi.\n",
      "Done with edge_2358! Took 3,007.65 sec.\n",
      "Time taken: 9,580.85502743721 sec.\n"
     ]
    }
   ],
   "source": [
    "bf = pysam.AlignmentFile(\"../main-workflow/output/fully-filtered-and-sorted-aln.bam\", \"rb\")\n",
    "output_dir = \"phasing-data/smoothed-reads/\"\n",
    "\n",
    "# verbose?\n",
    "no_indoor_voice = False\n",
    "\n",
    "def write_out_reads(filepath, readname2seq):\n",
    "    # Notably, this uses the \"a\" (append) method in order to add to the end of a file.\n",
    "    # Be careful -- if you're rerunning this, you'll \n",
    "    with open(filepath, \"a\") as of:\n",
    "        for readname in readname2seq:\n",
    "            # Write out both the header and the sequence for each read\n",
    "            of.write(f\">{readname}\\n{str(readname2seq[readname])}\\n\")\n",
    "            \n",
    "ALN_UPDATE_FREQ = 5000\n",
    "ALN_BUFFER_FREQ = 1000\n",
    "VR_EXTRA_SPAN = 100\n",
    "\n",
    "P = 1\n",
    "            \n",
    "t1 = time.time()\n",
    "for seq in SEQS:\n",
    "    \n",
    "    # Record which positions (0-indexed) aren't covered by any smoothed reads in this MAG.\n",
    "    # We'll add \"virtual reads\" that span these positions.\n",
    "    uncovered_positions = set(range(0, seq2len[seq]))\n",
    "    \n",
    "    fasta = skbio.DNA.read(f\"../seqs/{seq}.fasta\")\n",
    "    \n",
    "    output_smoothed_reads_file = os.path.join(output_dir, f\"{seq}_smoothed_reads_delignore.fasta\")\n",
    "    try:\n",
    "        os.remove(output_smoothed_reads_file)\n",
    "    except FileNotFoundError:\n",
    "        # it's fine if this file doesn't exist yet. what we DON'T want to happen is this file to already exist after\n",
    "        # this point in the code; since we only use \"append\" file-writing to write out smoothed reads, if we don't\n",
    "        # manually remove this file then running this script twice will cause weird behavior where the file contains\n",
    "        # the same smoothed reads multiple times, etc.\n",
    "        pass\n",
    "    \n",
    "    # Identify all (0-indexed, so compatible with skbio / pysam!)\n",
    "    # mutated positions in this genome up front to save time.\n",
    "    #\n",
    "    # Equivalently, we could also just take in an arbitrary VCF as input\n",
    "    # (e.g. one produced from another variant calling tool), although we'd\n",
    "    # need to be careful to only include SNVs and not indels/etc...\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Identifying mutated positions (p = {P}%) in genome {seq2name[seq]}...\")\n",
    "    mutpos2pileup = find_mutated_positions(seq, p_to_use=P, incl_pileup=True)\n",
    "    # We sort because the code below relies on these being in ascending order\n",
    "    mutated_positions = sorted(mutpos2pileup.keys())\n",
    "    print(f\"Found {len(mutated_positions):,} mutated positions (p = {P}%) in {seq2name[seq]}.\")\n",
    "    print(\n",
    "        f\"Note that this tally is higher than you'd see in e.g. the CP1/2/3 plots, because now we're including \"\n",
    "        \"both 'rare' and non-rare mutations. Just so you don't waste five minutes sanity-checking this like I did.\"\n",
    "    )\n",
    "    \n",
    "    print(\"Going through these positions...\")\n",
    "    \n",
    "    num_ignored_alns = 0\n",
    "    \n",
    "    # Instead of just writing out every smoothed alignment as soon as we generate it, we build up a \"buffer\"\n",
    "    # of these alignments and then write a bunch out at once. This way we limit slowdown due to constantly\n",
    "    # having to open/close files. I don't really have a good source for this as best practice, but I remembered\n",
    "    # to do it while writing this code, so somewhere in College Park the CS faculty at Maryland are smiling\n",
    "    #\n",
    "    # Also fyi this maps read name to smoothed alignment (well, at this point, just read) sequence. The read name\n",
    "    # is useful to preserve in fasta files so we have some idea of provenance (where smoothed reads came from)\n",
    "    smoothed_aln_buffer = {}\n",
    "    \n",
    "    # The first time we see an alignment of a read, it's 1; if we see a supp aln of this read, it's 2; etc.\n",
    "    # Lets us distinguish alignments with different names\n",
    "    readname2freq_so_far = defaultdict(int)\n",
    "    \n",
    "    # Go through all linear alignments of each read to this genome, focusing (for now) on just the primary\n",
    "    # alignments...\n",
    "    ts1 = time.time()\n",
    "    for ai, aln in enumerate(bf.fetch(seq), 1):\n",
    "        \n",
    "        if ai % ALN_UPDATE_FREQ == 0:\n",
    "            print(\n",
    "                f\"\\tOn aln {ai:,} in seq {seq2name[seq]}. \"\n",
    "                f\"Time spent on {seq2name[seq]} so far: {time.time() - ts1:,.2f} sec.\"\n",
    "            )\n",
    "            \n",
    "        if aln.is_secondary:\n",
    "            raise ValueError(\n",
    "                \"Not to get political or anything, but you should've already filtered secondary alns out\"\n",
    "            )\n",
    "            \n",
    "        # Note that supplementary alignments are ok, though! We implicitly handle these here.\n",
    "        #\n",
    "        # Different alignments of the same read will have different new_readnames, because we're gonna\n",
    "        # be treating them as distinct \"reads\". We should have already filtered reference-overlapping\n",
    "        # supp alns so this shouldn't be a problem\n",
    "        \n",
    "        readname = aln.query_name\n",
    "        readname2freq_so_far[readname] += 1\n",
    "        new_readname = f\"{readname}_{readname2freq_so_far[readname]}\"\n",
    "        \n",
    "        # should never happen\n",
    "        if new_readname in smoothed_aln_buffer:\n",
    "            raise ValueError(\"This exact read alignment has already been smoothed? Weird.\")\n",
    "            \n",
    "        # Figure out where on the MAG this alignment \"hits.\" These are 0-indexed positions from Pysam.\n",
    "        # (reference_end points to the position after the actual final position, since these are designed to\n",
    "        # be interoperable with Python's half-open intervals.)\n",
    "        #\n",
    "        # Of course, there likely will be indels within this range: we're purposefully ignoring those here.\n",
    "        ref_start = aln.reference_start\n",
    "        ref_end = aln.reference_end - 1\n",
    "        \n",
    "        # This should never happen (TM)\n",
    "        if ref_start >= ref_end:\n",
    "            # Du sollst jetzt mit Gott sprechen.\n",
    "            raise ValueError(\n",
    "                f\"Ref start {ref_start:,} >= ref end {ref_end:,} for read {new_readname}?\"\n",
    "            )\n",
    "            \n",
    "        # Smoothed sequence; we'll edit this so that if this read has (mis)matches to any called mutated\n",
    "        # positions, these positions are updated with the read's aligned nucleotides at these positions.\n",
    "        smoothed_aln_seq = fasta[ref_start: ref_end + 1]\n",
    "        \n",
    "        if actually_include_mutations_in_the_smoothed_reads:\n",
    "            # just for debugging: track the exact edits made to smoothed_aln_seq\n",
    "            replacements_made = {}\n",
    "\n",
    "            # We may choose to ignore this linear alignment, if we think it is error-prone or\n",
    "            # otherwise not useful. If this gets set to True in the loop below, then we'll notice this\n",
    "            # and ignore this alignment.\n",
    "            ignoring_this_aln = False\n",
    "            \n",
    "            # Notably, include skips -- this way, we can figure out if the aln has a deletion at a mutated\n",
    "            # position, and if so ignore this aln\n",
    "            ap = aln.get_aligned_pairs(matches_only=False)\n",
    "\n",
    "            # Iterating through the aligned pairs is expensive. Since read lengths are generally in the thousands\n",
    "            # to tens of thousands of bp (which is much less than the > 1 million bp length of any bacterial genome),\n",
    "            # we set things up so that we only iterate through the aligned pairs once. We maintain an integer, mpi,\n",
    "            # that is a poor man's \"pointer\" to an index in mutated_positions.\n",
    "\n",
    "            mpi = 0\n",
    "\n",
    "            # Go through this aln's aligned pairs. As we see each pair, compare the pair's reference position\n",
    "            # (refpos) to the mpi-th mutated position (herein referred to as \"mutpos\").\n",
    "            #\n",
    "            # If refpos >  mutpos, increment mpi until refpos <= mutpos (stopping as early as possible).\n",
    "            # If refpos == mutpos, we have a match! Update readname2mutpos2ismutated[mutpos] based on\n",
    "            #                      comparing the read to the reference at the aligned positions.\n",
    "            # If refpos <  mutpos, continue to the next pair.\n",
    "\n",
    "            for pair in ap:\n",
    "\n",
    "                refpos = pair[1]\n",
    "                \n",
    "                # Since we set matches_only (for get_aligned_pairs()) to False, the alignment could include\n",
    "                # insertions (which are encoded as the reference pos being set to None). We inherently ignore\n",
    "                # these insertions as part of the read smoothing process.\n",
    "                if refpos is None:\n",
    "                    continue\n",
    "                    \n",
    "                mutpos = mutated_positions[mpi]\n",
    "\n",
    "                no_mutations_to_right_of_here = False\n",
    "\n",
    "                # Increment mpi until we get to the next mutated position at or after the reference pos for this\n",
    "                # aligned pair (or until we run out of mutated positions).\n",
    "                while refpos > mutpos:\n",
    "                    mpi += 1\n",
    "                    if mpi < len(mutated_positions):\n",
    "                        mutpos = mutated_positions[mpi]\n",
    "                    else:\n",
    "                        no_mutations_to_right_of_here = True\n",
    "                        break\n",
    "\n",
    "                # I expect this should happen only for reads aligned near the right end of the genome.\n",
    "                if no_mutations_to_right_of_here:\n",
    "                    break\n",
    "\n",
    "                # If the next mutation occurs after this aligned pair, continue on to a later pair.\n",
    "                if refpos < mutpos:\n",
    "                    continue\n",
    "\n",
    "                # If we've made it here, refpos == mutpos!\n",
    "                # (...unless I messed something up in how I designed this code.)\n",
    "                if refpos != mutpos:\n",
    "                    raise ValueError(\"This should never happen!\")\n",
    "\n",
    "                # Finally, get the nucleotide aligned to this mutated position from this read.\n",
    "                readpos = pair[0]\n",
    "                \n",
    "                # Since we set matches_only (for get_aligned_pairs()) to False, there's a chance a read\n",
    "                # contains deletions aligned to mutated positions. This is accounted for by this case.\n",
    "                # If this happens, we ignore this alignment (the same as if it would contain a nt that\n",
    "                # isn't the first or second most common).\n",
    "                if readpos is None:\n",
    "                    if no_indoor_voice:\n",
    "                        print(\n",
    "                            f\"Read {new_readname} has deletion at mutpos {mutpos + 1:,}: \"\n",
    "                            f\"pileup = {mutpos2pileup[mutpos]}\"\n",
    "                        )\n",
    "                    ignoring_this_aln = True\n",
    "                    break\n",
    "                    \n",
    "                read_nt = aln.query_sequence[readpos]\n",
    "\n",
    "                # If this aln doesn't have match the first or second most common nucleotide at this position,\n",
    "                # ignore it. In the future, when we perform read  smoothing based on an arbitrary set of\n",
    "                # SNV calls, we can be more careful about this; but for now we make the simplifiying assumption\n",
    "                # that a mutation likely only has one alternate nucleotide, and that the 3rd and 4th most\n",
    "                # common nucleotides indicate errors. (Also, note that we break ties here arbitrarily.)\n",
    "                nt2ct = dict(zip(\"ACGT\", mutpos2pileup[mutpos][0]))\n",
    "                nt1 = max(nt2ct, key=nt2ct.get)\n",
    "                del nt2ct[nt1]\n",
    "                nt2 = max(nt2ct, key=nt2ct.get)\n",
    "                \n",
    "                if read_nt != nt1 and read_nt != nt2:\n",
    "                    if no_indoor_voice:\n",
    "                        print(\n",
    "                            f\"Read {new_readname} has 3rd or 4th most common nt at mutpos {mutpos + 1:,}: \"\n",
    "                            f\"pileup = {mutpos2pileup[mutpos]}, read = {read_nt}\"\n",
    "                        )\n",
    "                    ignoring_this_aln = True\n",
    "                    break\n",
    "                    \n",
    "                # Notably, the nucleotide at a mutated position in a smoothed read will always be the first\n",
    "                # or second most common nucleotide at this position. So \"unreasonable\" positions, in which\n",
    "                # the ref nt != the consensus nt, will not be treated as you might expect -- we ignore the\n",
    "                # reference in this particular case. Shouldn't make a big difference, since in most cases\n",
    "                # the ref and consensus nt agree.\n",
    "                #\n",
    "                # Note that we aren't even looking at the reference nt at this point -- in many cases\n",
    "                # the .replace() operation done below won't change anything. Shouldn't matter.\n",
    "                relative_pos_on_aln = mutpos - ref_start\n",
    "                smoothed_aln_seq = smoothed_aln_seq.replace([relative_pos_on_aln], read_nt)\n",
    "                replacements_made[relative_pos_on_aln] = read_nt\n",
    "                if no_indoor_voice:\n",
    "                    print(\n",
    "                        f\"Updated read {new_readname} re: nt at mutpos {mutpos + 1:,}: \"\n",
    "                        f\"pileup = {mutpos2pileup[mutpos]}, read = {read_nt}\"\n",
    "                    )\n",
    "\n",
    "            if no_indoor_voice:\n",
    "                print(f\"Read {new_readname} involved {len(replacements_made):,} replacements!\")\n",
    "        \n",
    "        if ignoring_this_aln:\n",
    "            num_ignored_alns += 1\n",
    "        else:\n",
    "            # Now that we've finished processing all called mutations that this alignment spans, prepare it\n",
    "            # to be written out to a FASTA file. See comments above on smoothed_aln_buffer, and why we don't\n",
    "            # just write everything out as soon as it's ready.\n",
    "            #\n",
    "            # (Also, we've already guaranteed readname isn't already in smoothed_aln_buffer, so no need to worry\n",
    "            # about accidentally overwriting something from earlier.)\n",
    "            smoothed_aln_buffer[new_readname] = smoothed_aln_seq\n",
    "\n",
    "            # Record which positions this read covers (of course, it may not exactly \"cover\" these positions\n",
    "            # originally due to indels, but the smoothed version will cover them).\n",
    "            # We don't update uncovered_positions until *after* we process all aligned pairs of this read, to allow\n",
    "            # us to ignore reads if desired.\n",
    "            uncovered_positions -= set(range(ref_start, ref_end + 1))\n",
    "\n",
    "            if ai % ALN_BUFFER_FREQ == 0:\n",
    "                write_out_reads(output_smoothed_reads_file, smoothed_aln_buffer)\n",
    "                # Clear the buffer\n",
    "                smoothed_aln_buffer = {}\n",
    "        \n",
    "    # We're probably going to have left over smoothed reads that we still haven't written out, unless things\n",
    "    # worked out so that on the final alignment we saw ai was exactly divisible by ALN_BUFFER_FREQ (and that's\n",
    "    # pretty unlikely unless you set the buffer freq to a low number). So make one last dump of the buffer.\n",
    "    if len(smoothed_aln_buffer) > 0:\n",
    "        write_out_reads(output_smoothed_reads_file, smoothed_aln_buffer)\n",
    "        \n",
    "    print(f\"We ignored {num_ignored_alns:,} linear alignments, fyi.\")\n",
    "        \n",
    "    if add_virtual_reads and len(uncovered_positions) > 0:\n",
    "        print(f\"For reference, there are {len(uncovered_positions):,} uncovered positions in {seq2name[seq]}.\")\n",
    "        \n",
    "        sup = sorted(uncovered_positions)\n",
    "        uc_runs = convert_to_runs(sup)\n",
    "        print(f'And there are {len(uc_runs)} \"runs\" of uncovered positions.')\n",
    "        \n",
    "        rounded_meancov = round(seq2meancov[seq])\n",
    "        print(\n",
    "            f'Adding \"virtual reads\" spanning each of these runs, at rounded mean coverage '\n",
    "            f'of {rounded_meancov:,}x, to account for this...'\n",
    "        )\n",
    "        \n",
    "        num_vr = 0\n",
    "        vr_buffer = {}\n",
    "        for run in uc_runs:\n",
    "            # Construct a virtual read that includes this entire run of uncovered positions as well\n",
    "            # as VR_EXTRA_SPAN positions before and after (clamping to the start/end of the seq if needed).\n",
    "            #\n",
    "            # Notably, we could try to make this loop around from end -> start if this is a cyclic MAG, but\n",
    "            # to remain consistent with how we handle supplementary alignments above -- and because implementing\n",
    "            # the loop around would be a lot of work and it's like 3am -- we ignore this for now.\n",
    "            #\n",
    "            # Also, note that run_start can equal run_end, if only a single isolated position is uncovered.\n",
    "            # This is fine -- the code handles this case automatically. (I guess the only potential problem is\n",
    "            # if the length of the MAG is less than VR_EXTRA_SPAN, but... that should never happen. If you have\n",
    "            # like 100bp-long MAGs that's a problem! I guess, TODO, make note of this when generalizing this\n",
    "            # code.)\n",
    "            \n",
    "            run_start = max(run[0] - VR_EXTRA_SPAN, 0)\n",
    "            run_end = min(run[1] + VR_EXTRA_SPAN, seq2len[seq] - 1)\n",
    "            \n",
    "            # Generate a sequence matching the \"reference\" MAG at these positions. We of course don't have\n",
    "            # any info about mutations here, because these positions are uncovered by the real reads!\n",
    "            vr_seq = fasta[run_start: run_end + 1]\n",
    "            \n",
    "            # We need to assign reads unique names, and including the run coordinates here is a nice way\n",
    "            # to preserve uniqueness across runs and also make our smoothed reads files easier to interpret\n",
    "            vr_name_prefix = f\"vr_{run[0]}_{run[1]}\"\n",
    "            \n",
    "            # Add M copies of this virtual read, where M = (rounded mean coverage of this MAG)\n",
    "            for vr_num in range(1, rounded_meancov + 1):\n",
    "                vr_name = f\"{vr_name_prefix}_{vr_num}\"\n",
    "                vr_buffer[vr_name] = vr_seq\n",
    "                num_vr += 1\n",
    "                \n",
    "        write_out_reads(output_smoothed_reads_file, vr_buffer)\n",
    "        print(f\"Wrote out {num_vr:,} virtual reads.\")\n",
    "    \n",
    "    print(f\"Done with {seq}! Took {time.time() - ts1:,.2f} sec.\")\n",
    "        \n",
    "print(f\"Time taken: {time.time() - t1:,} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assemble these smoothed reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T05:01:06.712451Z",
     "iopub.status.busy": "2022-04-11T05:01:06.711530Z",
     "iopub.status.idle": "2022-04-11T05:14:02.379024Z",
     "shell.execute_reply": "2022-04-11T05:14:02.379653Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:00 2Mb  INFO: Hello! You are running La Jolla Assembler (LJA), a tool for genome assembly from PacBio HiFi reads\n",
      "00:00:11 2Mb  INFO: 007ba931cbbf0218b4524edafe6c3bd5cac9003f\n",
      "00:00:11 5Mb  INFO: LJA pipeline started\n",
      "00:00:11 5Mb  INFO: Performing initial correction with k = 5001\n",
      "00:00:11 0Mb  INFO: Reading reads\n",
      "00:00:11 0Mb  INFO: Extracting minimizers\n",
      "00:00:28 5.3Gb  INFO: Finished read processing\n",
      "00:00:28 5.3Gb  INFO: 5203371 hashs collected. Starting sorting.\n",
      "00:00:28 5.3Gb  INFO: Finished sorting. Total distinct minimizers: 4165\n",
      "00:00:29 5.3Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:29 5.3Gb  INFO: Vertex map constructed.\n",
      "00:00:29 5.3Gb  INFO: Filling edge sequences.\n",
      "00:00:46 5.5Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:00:46 5.5Gb  INFO:  Collecting tips \n",
      "00:00:46 5.5Gb  INFO: Added 31 artificial minimizers from tips.\n",
      "00:00:46 5.5Gb  INFO: Collected 8366 old edges.\n",
      "00:00:46 5.5Gb  INFO: New minimizers added to sparse graph.\n",
      "00:00:46 5.5Gb  INFO: Refilling graph with old edges.\n",
      "00:00:46 5.5Gb  INFO: Filling graph with new edges.\n",
      "00:00:46 5.5Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:00:46 5.5Gb  INFO: Starting to extract disjointigs.\n",
      "00:00:46 5.5Gb  INFO: Finished extracting 101 disjointigs of total size 1558708\n",
      "00:00:46 0Mb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct_delignore/k5001/disjointigs.fasta\"\n",
      "00:00:46 8Mb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:00:46 8Mb  INFO: Filled 4876839 bits out of 33715424\n",
      "00:00:46 8Mb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:00:47 8Mb  INFO: Collected 501 junctions.\n",
      "00:00:47 8Mb  INFO: Starting DBG construction.\n",
      "00:00:47 8Mb  INFO: Vertices created.\n",
      "00:00:47 8Mb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:00:47 8Mb  INFO: Added 0 hanging vertices\n",
      "00:00:47 8Mb  INFO: Merging unbranching paths\n",
      "00:00:47 8Mb  INFO: Ended merging edges. Resulting size 98\n",
      "00:00:47 8Mb  INFO: Cleaning edge coverages\n",
      "00:00:47 8Mb  INFO: Collecting alignments of sequences to the graph\n",
      "00:01:01 5.6Gb  INFO: Alignment collection finished. Total length of alignments is 487434\n",
      "00:01:01 5.6Gb  INFO: Could not correct 61 reads. They will be removed.\n",
      "00:01:01 5.6Gb  INFO: Uncorrected reads were removed.\n",
      "00:01:01 5.6Gb  INFO: Applying changes to the graph\n",
      "00:01:04 5.8Gb  INFO: Printing reads to fasta file \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct_delignore/k5001/corrected_reads.fasta\"\n",
      "00:01:36 5Mb  INFO: Performing repeat resolution by transforming de Bruijn graph into Multiplex de Bruijn graph\n",
      "00:01:36 0Mb  INFO: Loading graph from fasta\n",
      "00:01:36 7Mb  INFO: Finished loading graph\n",
      "00:01:36 154Mb  INFO: Looking for unique edges\n",
      "00:01:36 154Mb  INFO: Marked 18 long edges as unique\n",
      "00:01:36 154Mb  INFO: Marking extra edges as unique based on read paths\n",
      "00:01:36 154Mb  INFO: Marked 18 edges as unique\n",
      "00:01:36 154Mb  INFO: Splitting graph with unique edges\n",
      "00:01:36 154Mb  INFO: Processing 11 components\n",
      "00:01:36 154Mb  INFO: Finished unique edges search. Found 68 unique edges\n",
      "00:01:36 154Mb  INFO: Analysing repeats of multiplicity 2 and looking for additional unique edges\n",
      "00:01:36 154Mb  INFO: Finished processing of repeats of multiplicity 2. Found 0 erroneous edges.\n",
      "00:01:36 154Mb  INFO: Resolving repeats\n",
      "00:01:36 154Mb  INFO: Constructing paths\n",
      "00:01:40 310Mb  INFO: Building graph\n",
      "00:01:40 310Mb  INFO: Increasing k\n",
      "00:01:40 310Mb  INFO: Finished increasing k\n",
      "00:01:40 310Mb  INFO: Exporting remaining active transitions\n",
      "00:01:40 310Mb  INFO: Export to Dot\n",
      "00:01:40 310Mb  INFO: Export to GFA and compressed contigs\n",
      "00:01:40 315Mb  INFO: Finished repeat resolution\n",
      "00:01:41 5Mb  INFO: Performing polishing and homopolymer uncompression\n",
      "00:01:41 6Mb  INFO: Aligning reads back to assembly\n",
      "00:01:58 4.1Gb  INFO: Finished alignment.\n",
      "00:01:58 4.1Gb  INFO: Printing alignments to \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct_delignore/uncompressing/alignments.txt\"\n",
      "00:01:58 4.1Gb  INFO: Reading and processing initial reads from [\"phasing-data/smoothed-reads/edge_6104_smoothed_reads_delignore.fasta\"]\n",
      "00:04:44 5.2Gb  INFO: Uncompressing homopolymers in contigs\n",
      "00:04:45 5.2Gb  INFO: Total zero covered nucleotides 0\n",
      "00:04:45 5.2Gb  INFO: Calculating overlaps between adjacent uncompressed edges\n",
      "00:04:45 5.2Gb  INFO: Printing final gfa file to \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct_delignore/mdbg.gfa\"\n",
      "00:04:45 5.2Gb  INFO: Printing final assembly to \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct_delignore/assembly.fasta\"\n",
      "00:04:45 5Mb  INFO: Final homopolymer compressed and corrected reads can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct_delignore/k5001/corrected_reads.fasta\"\n",
      "00:04:45 5Mb  INFO: Final graph with homopolymer compressed edges can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct_delignore/mdbg/mdbg.hpc.gfa\"\n",
      "00:04:45 5Mb  INFO: Final graph can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct_delignore/mdbg.gfa\"\n",
      "00:04:45 5Mb  INFO: Final assembly can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct_delignore/assembly.fasta\"\n",
      "00:04:45 5Mb  INFO: LJA pipeline finished\n",
      "00:00:00 2Mb  INFO: Hello! You are running La Jolla Assembler (LJA), a tool for genome assembly from PacBio HiFi reads\n",
      "00:00:10 2Mb  INFO: 007ba931cbbf0218b4524edafe6c3bd5cac9003f\n",
      "00:00:10 5Mb  INFO: LJA pipeline started\n",
      "00:00:10 5Mb  INFO: Performing initial correction with k = 5001\n",
      "00:00:10 0Mb  INFO: Reading reads\n",
      "00:00:10 0Mb  INFO: Extracting minimizers\n",
      "00:00:19 2.7Gb  INFO: Finished read processing\n",
      "00:00:19 2.7Gb  INFO: 2767973 hashs collected. Starting sorting.\n",
      "00:00:19 2.7Gb  INFO: Finished sorting. Total distinct minimizers: 25155\n",
      "00:00:19 2.7Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:19 2.7Gb  INFO: Vertex map constructed.\n",
      "00:00:19 2.7Gb  INFO: Filling edge sequences.\n",
      "00:00:30 2.7Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:00:30 2.7Gb  INFO:  Collecting tips \n",
      "00:00:30 2.7Gb  INFO: Added 1600 artificial minimizers from tips.\n",
      "00:00:30 2.7Gb  INFO: Collected 49656 old edges.\n",
      "00:00:30 2.7Gb  INFO: New minimizers added to sparse graph.\n",
      "00:00:30 2.7Gb  INFO: Refilling graph with old edges.\n",
      "00:00:30 2.7Gb  INFO: Filling graph with new edges.\n",
      "00:00:30 2.7Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:00:30 2.7Gb  INFO: Starting to extract disjointigs.\n",
      "00:00:30 2.7Gb  INFO: Finished extracting 2210 disjointigs of total size 17516883\n",
      "00:00:31 0Mb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct_delignore/k5001/disjointigs.fasta\"\n",
      "00:00:31 33Mb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:00:32 34Mb  INFO: Filled 29922929 bits out of 206869536\n",
      "00:00:32 34Mb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:00:33 34Mb  INFO: Collected 4959 junctions.\n",
      "00:00:33 34Mb  INFO: Starting DBG construction.\n",
      "00:00:33 34Mb  INFO: Vertices created.\n",
      "00:00:33 34Mb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:00:33 34Mb  INFO: Added 1 hanging vertices\n",
      "00:00:33 34Mb  INFO: Merging unbranching paths\n",
      "00:00:33 34Mb  INFO: Ended merging edges. Resulting size 2546\n",
      "00:00:33 34Mb  INFO: Cleaning edge coverages\n",
      "00:00:33 34Mb  INFO: Collecting alignments of sequences to the graph\n",
      "00:00:39 2.7Gb  INFO: Alignment collection finished. Total length of alignments is 483377\n",
      "00:00:39 2.7Gb  INFO: Could not correct 4713 reads. They will be removed.\n",
      "00:00:39 2.7Gb  INFO: Uncorrected reads were removed.\n",
      "00:00:39 2.7Gb  INFO: Applying changes to the graph\n",
      "00:00:42 2.7Gb  INFO: Printing reads to fasta file \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct_delignore/k5001/corrected_reads.fasta\"\n",
      "00:00:59 5Mb  INFO: Performing repeat resolution by transforming de Bruijn graph into Multiplex de Bruijn graph\n",
      "00:00:59 0Mb  INFO: Loading graph from fasta\n",
      "00:00:59 12Mb  INFO: Finished loading graph\n",
      "00:00:59 81Mb  INFO: Looking for unique edges\n",
      "00:00:59 81Mb  INFO: Marked 80 long edges as unique\n",
      "00:00:59 81Mb  INFO: Marking extra edges as unique based on read paths\n",
      "00:00:59 81Mb  INFO: Marked 84 edges as unique\n",
      "00:00:59 81Mb  INFO: Splitting graph with unique edges\n",
      "00:00:59 81Mb  INFO: Processing 104 components\n",
      "00:00:59 81Mb  INFO: Finished unique edges search. Found 92 unique edges\n",
      "00:00:59 81Mb  INFO: Analysing repeats of multiplicity 2 and looking for additional unique edges\n",
      "00:00:59 81Mb  INFO: Finished processing of repeats of multiplicity 2. Found 0 erroneous edges.\n",
      "00:00:59 81Mb  INFO: Resolving repeats\n",
      "00:00:59 81Mb  INFO: Constructing paths\n",
      "00:01:01 156Mb  INFO: Building graph\n",
      "00:01:01 156Mb  INFO: Increasing k\n",
      "00:01:01 158Mb  INFO: Finished increasing k\n",
      "00:01:01 158Mb  INFO: Exporting remaining active transitions\n",
      "00:01:01 158Mb  INFO: Export to Dot\n",
      "00:01:01 158Mb  INFO: Export to GFA and compressed contigs\n",
      "00:01:01 166Mb  INFO: Finished repeat resolution\n",
      "00:01:02 5Mb  INFO: Performing polishing and homopolymer uncompression\n",
      "00:01:02 7Mb  INFO: Aligning reads back to assembly\n",
      "00:01:10 1.9Gb  INFO: Finished alignment.\n",
      "00:01:10 1.9Gb  INFO: Printing alignments to \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct_delignore/uncompressing/alignments.txt\"\n",
      "00:01:10 1.9Gb  INFO: Reading and processing initial reads from [\"phasing-data/smoothed-reads/edge_1671_smoothed_reads_delignore.fasta\"]\n",
      "00:01:49 3.2Gb  INFO: Uncompressing homopolymers in contigs\n",
      "00:01:49 3.2Gb  INFO: Total zero covered nucleotides 0\n",
      "00:01:49 3.2Gb  INFO: Calculating overlaps between adjacent uncompressed edges\n",
      "00:01:49 3.2Gb  INFO: Printing final gfa file to \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct_delignore/mdbg.gfa\"\n",
      "00:01:49 3.2Gb  INFO: Printing final assembly to \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct_delignore/assembly.fasta\"\n",
      "00:01:49 5Mb  INFO: Final homopolymer compressed and corrected reads can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct_delignore/k5001/corrected_reads.fasta\"\n",
      "00:01:49 5Mb  INFO: Final graph with homopolymer compressed edges can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct_delignore/mdbg/mdbg.hpc.gfa\"\n",
      "00:01:49 5Mb  INFO: Final graph can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct_delignore/mdbg.gfa\"\n",
      "00:01:49 5Mb  INFO: Final assembly can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct_delignore/assembly.fasta\"\n",
      "00:01:49 5Mb  INFO: LJA pipeline finished\n",
      "00:00:00 2Mb  INFO: Hello! You are running La Jolla Assembler (LJA), a tool for genome assembly from PacBio HiFi reads\n",
      "00:00:11 2Mb  INFO: 007ba931cbbf0218b4524edafe6c3bd5cac9003f\n",
      "00:00:11 5Mb  INFO: LJA pipeline started\n",
      "00:00:11 5Mb  INFO: Performing initial correction with k = 5001\n",
      "00:00:11 0Mb  INFO: Reading reads\n",
      "00:00:11 0Mb  INFO: Extracting minimizers\n",
      "00:00:33 6.5Gb  INFO: Finished read processing\n",
      "00:00:33 6.5Gb  INFO: 7184040 hashs collected. Starting sorting.\n",
      "00:00:33 6.7Gb  INFO: Finished sorting. Total distinct minimizers: 10087\n",
      "00:00:34 6.7Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:34 6.7Gb  INFO: Vertex map constructed.\n",
      "00:00:34 6.7Gb  INFO: Filling edge sequences.\n",
      "00:00:56 6.7Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:00:56 6.7Gb  INFO:  Collecting tips \n",
      "00:00:56 6.7Gb  INFO: Added 29 artificial minimizers from tips.\n",
      "00:00:56 6.7Gb  INFO: Collected 20348 old edges.\n",
      "00:00:56 6.7Gb  INFO: New minimizers added to sparse graph.\n",
      "00:00:56 6.7Gb  INFO: Refilling graph with old edges.\n",
      "00:00:57 6.7Gb  INFO: Filling graph with new edges.\n",
      "00:00:57 6.7Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:00:57 6.7Gb  INFO: Starting to extract disjointigs.\n",
      "00:00:57 6.7Gb  INFO: Finished extracting 313 disjointigs of total size 4093868\n",
      "00:00:57 0Mb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct_delignore/k5001/disjointigs.fasta\"\n",
      "00:00:57 14Mb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:00:58 15Mb  INFO: Filled 11703639 bits out of 80913760\n",
      "00:00:58 15Mb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:00:58 15Mb  INFO: Collected 1238 junctions.\n",
      "00:00:58 15Mb  INFO: Starting DBG construction.\n",
      "00:00:58 15Mb  INFO: Vertices created.\n",
      "00:00:58 15Mb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:00:58 15Mb  INFO: Added 0 hanging vertices\n",
      "00:00:58 15Mb  INFO: Merging unbranching paths\n",
      "00:00:58 15Mb  INFO: Ended merging edges. Resulting size 232\n",
      "00:00:59 15Mb  INFO: Cleaning edge coverages\n",
      "00:00:59 15Mb  INFO: Collecting alignments of sequences to the graph\n",
      "00:01:16 6.8Gb  INFO: Alignment collection finished. Total length of alignments is 681093\n",
      "00:01:16 6.8Gb  INFO: Could not correct 469 reads. They will be removed.\n",
      "00:01:16 6.8Gb  INFO: Uncorrected reads were removed.\n",
      "00:01:16 6.8Gb  INFO: Applying changes to the graph\n",
      "00:01:20 7.1Gb  INFO: Printing reads to fasta file \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct_delignore/k5001/corrected_reads.fasta\"\n",
      "00:02:04 5Mb  INFO: Performing repeat resolution by transforming de Bruijn graph into Multiplex de Bruijn graph\n",
      "00:02:04 0Mb  INFO: Loading graph from fasta\n",
      "00:02:04 13Mb  INFO: Finished loading graph\n",
      "00:02:06 250Mb  INFO: Looking for unique edges\n",
      "00:02:06 250Mb  INFO: Marked 29 long edges as unique\n",
      "00:02:06 250Mb  INFO: Marking extra edges as unique based on read paths\n",
      "00:02:06 250Mb  INFO: Marked 29 edges as unique\n",
      "00:02:06 250Mb  INFO: Splitting graph with unique edges\n",
      "00:02:06 250Mb  INFO: Processing 16 components\n",
      "00:02:06 250Mb  INFO: Finished unique edges search. Found 177 unique edges\n",
      "00:02:06 250Mb  INFO: Analysing repeats of multiplicity 2 and looking for additional unique edges\n",
      "00:02:06 250Mb  INFO: Finished processing of repeats of multiplicity 2. Found 0 erroneous edges.\n",
      "00:02:06 250Mb  INFO: Resolving repeats\n",
      "00:02:06 250Mb  INFO: Constructing paths\n",
      "00:02:11 408Mb  INFO: Building graph\n",
      "00:02:11 408Mb  INFO: Increasing k\n",
      "00:02:11 408Mb  INFO: Finished increasing k\n",
      "00:02:11 408Mb  INFO: Exporting remaining active transitions\n",
      "00:02:11 408Mb  INFO: Export to Dot\n",
      "00:02:11 408Mb  INFO: Export to GFA and compressed contigs\n",
      "00:02:12 415Mb  INFO: Finished repeat resolution\n",
      "00:02:12 5Mb  INFO: Performing polishing and homopolymer uncompression\n",
      "00:02:13 10Mb  INFO: Aligning reads back to assembly\n",
      "00:02:36 5.3Gb  INFO: Finished alignment.\n",
      "00:02:36 5.3Gb  INFO: Printing alignments to \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct_delignore/uncompressing/alignments.txt\"\n",
      "00:02:37 5.3Gb  INFO: Reading and processing initial reads from [\"phasing-data/smoothed-reads/edge_2358_smoothed_reads_delignore.fasta\"]\n",
      "00:06:19 6.2Gb  INFO: Uncompressing homopolymers in contigs\n",
      "00:06:19 6.2Gb  INFO: Total zero covered nucleotides 0\n",
      "00:06:19 6.2Gb  INFO: Calculating overlaps between adjacent uncompressed edges\n",
      "00:06:19 6.2Gb  INFO: Printing final gfa file to \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct_delignore/mdbg.gfa\"\n",
      "00:06:20 6.2Gb  INFO: Printing final assembly to \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct_delignore/assembly.fasta\"\n",
      "00:06:20 5Mb  INFO: Final homopolymer compressed and corrected reads can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct_delignore/k5001/corrected_reads.fasta\"\n",
      "00:06:20 5Mb  INFO: Final graph with homopolymer compressed edges can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct_delignore/mdbg/mdbg.hpc.gfa\"\n",
      "00:06:20 5Mb  INFO: Final graph can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct_delignore/mdbg.gfa\"\n",
      "00:06:20 5Mb  INFO: Final assembly can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct_delignore/assembly.fasta\"\n",
      "00:06:20 5Mb  INFO: LJA pipeline finished\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# LJA with no error correction, but with a filter for low-coverage edges\n",
    "\n",
    "OUTDIR=phasing-data/smoothed-reads\n",
    "\n",
    "/home/mfedarko/software/LJA-branch/bin/lja \\\n",
    "    --reads $OUTDIR/edge_6104_smoothed_reads_delignore.fasta \\\n",
    "    --simpleec \\\n",
    "    --Cov-threshold 10 \\\n",
    "    --output-dir $OUTDIR/edge_6104_lja_cf_10x_p1pct_delignore\n",
    "\n",
    "/home/mfedarko/software/LJA-branch/bin/lja \\\n",
    "    --reads $OUTDIR/edge_1671_smoothed_reads_delignore.fasta \\\n",
    "    --simpleec \\\n",
    "    --Cov-threshold 10 \\\n",
    "    --output-dir $OUTDIR/edge_1671_lja_cf_10x_p1pct_delignore\n",
    "\n",
    "/home/mfedarko/software/LJA-branch/bin/lja \\\n",
    "    --reads $OUTDIR/edge_2358_smoothed_reads_delignore.fasta \\\n",
    "    --simpleec \\\n",
    "    --Cov-threshold 10 \\\n",
    "    --output-dir $OUTDIR/edge_2358_lja_cf_10x_p1pct_delignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Double-check edge locations in original MAG\n",
    "\n",
    "Assumes the segments in the mdbg.gfa files have been converted to a FASTA file already. This is doable using something like [this small script](https://gist.github.com/fedarko/9fe32014f1e55d80511be0d22dc36830), which is a generalized version of one of the python scripts located in the \"main workflow\" side of things.\n",
    "\n",
    "In LJA's output, the assembly.fasta file doesn't always exactly match up with the segments in the mdbg.gfa file; we are interested here in associating parts of the graph with the original MAG, so we align these segments (which may contain e.g. overlapping or redundant sequences) rather than the actual final contigs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[M::mm_idx_gen::0.057*0.98] collected minimizers\n",
      "[M::mm_idx_gen::0.079*1.47] sorted minimizers\n",
      "[M::main::0.079*1.47] loaded/built the index for 1 target sequence(s)\n",
      "[M::mm_mapopt_update::0.086*1.44] mid_occ = 100\n",
      "[M::mm_idx_stat] kmer size: 19; skip: 10; is_hpc: 0; #seq: 1\n",
      "[M::mm_idx_stat::0.092*1.40] distinct minimizers: 230439 (98.64% are singletons); average occurrences: 1.017; average spacing: 5.499\n",
      "[M::worker_pipeline::0.628*2.56] mapped 53 sequences\n",
      "[M::main] Version: 2.17-r941\n",
      "[M::main] CMD: minimap2 -ax asm20 --secondary=no --MD /Poppy/mfedarko/sheepgut/seqs/edge_6104.fasta phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct_delignore/mdbg-seqs.fasta\n",
      "[M::main] Real time: 0.638 sec; CPU: 1.612 sec; Peak RSS: 0.031 GB\n",
      "[M::mm_idx_gen::0.090*0.97] collected minimizers\n",
      "[M::mm_idx_gen::0.112*1.32] sorted minimizers\n",
      "[M::main::0.113*1.32] loaded/built the index for 1 target sequence(s)\n",
      "[M::mm_mapopt_update::0.124*1.29] mid_occ = 100\n",
      "[M::mm_idx_stat] kmer size: 19; skip: 10; is_hpc: 0; #seq: 1\n",
      "[M::mm_idx_stat::0.130*1.26] distinct minimizers: 384668 (98.91% are singletons); average occurrences: 1.017; average spacing: 5.506\n",
      "[M::worker_pipeline::0.709*2.45] mapped 54 sequences\n",
      "[M::main] Version: 2.17-r941\n",
      "[M::main] CMD: minimap2 -ax asm20 --secondary=no --MD /Poppy/mfedarko/sheepgut/seqs/edge_1671.fasta phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct_delignore/mdbg-seqs.fasta\n",
      "[M::main] Real time: 0.722 sec; CPU: 1.748 sec; Peak RSS: 0.049 GB\n",
      "[M::mm_idx_gen::0.109*0.99] collected minimizers\n",
      "[M::mm_idx_gen::0.141*1.31] sorted minimizers\n",
      "[M::main::0.141*1.31] loaded/built the index for 1 target sequence(s)\n",
      "[M::mm_mapopt_update::0.156*1.28] mid_occ = 100\n",
      "[M::mm_idx_stat] kmer size: 19; skip: 10; is_hpc: 0; #seq: 1\n",
      "[M::mm_idx_stat::0.165*1.26] distinct minimizers: 478571 (97.87% are singletons); average occurrences: 1.067; average spacing: 5.497\n",
      "[M::worker_pipeline::1.676*2.62] mapped 153 sequences\n",
      "[M::main] Version: 2.17-r941\n",
      "[M::main] CMD: minimap2 -ax asm20 --secondary=no --MD /Poppy/mfedarko/sheepgut/seqs/edge_2358.fasta phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct_delignore/mdbg-seqs.fasta\n",
      "[M::main] Real time: 1.692 sec; CPU: 4.412 sec; Peak RSS: 0.065 GB\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUTDIR=phasing-data/smoothed-reads\n",
    "\n",
    "minimap2 -ax asm20 --secondary=no --MD \\\n",
    "    /Poppy/mfedarko/sheepgut/seqs/edge_6104.fasta $OUTDIR/edge_6104_lja_cf_10x_p1pct_delignore/mdbg-seqs.fasta > \\\n",
    "    $OUTDIR/edge_6104_lja_cf_10x_p1pct_delignore/seq2mag.sam\n",
    "    \n",
    "minimap2 -ax asm20 --secondary=no --MD \\\n",
    "    /Poppy/mfedarko/sheepgut/seqs/edge_1671.fasta $OUTDIR/edge_1671_lja_cf_10x_p1pct_delignore/mdbg-seqs.fasta > \\\n",
    "    $OUTDIR/edge_1671_lja_cf_10x_p1pct_delignore/seq2mag.sam\n",
    "    \n",
    "minimap2 -ax asm20 --secondary=no --MD \\\n",
    "    /Poppy/mfedarko/sheepgut/seqs/edge_2358.fasta $OUTDIR/edge_2358_lja_cf_10x_p1pct_delignore/mdbg-seqs.fasta > \\\n",
    "    $OUTDIR/edge_2358_lja_cf_10x_p1pct_delignore/seq2mag.sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
