{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform read smoothing then assemble with LJA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T10:07:59.325361Z",
     "iopub.status.busy": "2022-03-21T10:07:59.324407Z",
     "iopub.status.idle": "2022-03-21T10:07:59.804545Z",
     "shell.execute_reply": "2022-03-21T10:07:59.804979Z"
    }
   },
   "outputs": [],
   "source": [
    "%run \"Header.ipynb\"\n",
    "%run \"../main-workflow/utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T10:07:59.808857Z",
     "iopub.status.busy": "2022-03-21T10:07:59.808185Z",
     "iopub.status.idle": "2022-03-21T10:08:00.193791Z",
     "shell.execute_reply": "2022-03-21T10:08:00.193048Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pysam\n",
    "import skbio\n",
    "from collections import defaultdict, Counter\n",
    "from linked_mutations_utils import find_mutated_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Quick sanity check: ensure that all $k$-mers ($k$ = 5,001) are unique in each MAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T10:08:00.204856Z",
     "iopub.status.busy": "2022-03-21T10:08:00.204009Z",
     "iopub.status.idle": "2022-03-21T10:09:44.206226Z",
     "shell.execute_reply": "2022-03-21T10:09:44.206867Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On start pos 0 in CAMP.\n",
      "On start pos 1,000,000 in CAMP.\n",
      "The most common k = 5,001-mer in CAMP occurred 1 time(s).\n",
      "On start pos 0 in BACT1.\n",
      "On start pos 1,000,000 in BACT1.\n",
      "On start pos 2,000,000 in BACT1.\n",
      "The most common k = 5,001-mer in BACT1 occurred 1 time(s).\n",
      "On start pos 0 in BACT2.\n",
      "On start pos 1,000,000 in BACT2.\n",
      "On start pos 2,000,000 in BACT2.\n",
      "The most common k = 5,001-mer in BACT2 occurred 1 time(s).\n"
     ]
    }
   ],
   "source": [
    "# I know there are actual k-mer counting tools you can use but no reason to overcomplicate things for now\n",
    "\n",
    "k = 5001\n",
    "\n",
    "for seq in SEQS:\n",
    "    fasta = skbio.DNA.read(f\"../seqs/{seq}.fasta\")\n",
    "    bargain_bin_kmer_counter = Counter()\n",
    "    \n",
    "    # The skbio.DNA object is 0-indexed, so 0 is the leftmost k-mer start position and\n",
    "    # ((seq length) - k) is the rightmost k-mer start position. The + 1 is because python ranges don't include\n",
    "    # the right endpoint.\n",
    "    for start_pos in range(0, seq2len[seq] - k + 1):\n",
    "        \n",
    "        # NOTE: this is a terrible no good very bad way to do this; it's more efficient to use a \"sliding window\"\n",
    "        # approach where you store the entire k-mer and then, with each step, just remove the first character and\n",
    "        # add on a new last character. \"But, uh, this code will only be run on these three MAGs, so I'm gonna\n",
    "        # prioritize clarity over optimization,\" says me, the insane person who just spent like a minute writing\n",
    "        # this comment when I could've been optimizing this code instead look WHATEVER this counts k-mers and it's\n",
    "        # 4am let's not overcomplicate it, look if you're on GitHub right now and you see this inane comment\n",
    "        # we can both just pretend that you were looking at some really optimized code and we'll both walk away\n",
    "        # satisfied, capisce\n",
    "        kmer = fasta[start_pos : start_pos + k]\n",
    "        \n",
    "        bargain_bin_kmer_counter[str(kmer)] += 1\n",
    "        if start_pos % 1000000 == 0: print(f\"On start pos {start_pos:,} in {seq2name[seq]}.\")\n",
    "    \n",
    "    mckc = bargain_bin_kmer_counter.most_common(1)[0][1]\n",
    "    print(f\"The most common k = {k:,}-mer in {seq2name[seq]} occurred {mckc:,} time(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Smooth reads\n",
    "\n",
    "Lots of this code is duplicated from the `Phasing-01-MakeGraph.ipynb` notebook in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T10:09:44.214524Z",
     "iopub.status.busy": "2022-03-21T10:09:44.213726Z",
     "iopub.status.idle": "2022-03-21T10:09:44.215822Z",
     "shell.execute_reply": "2022-03-21T10:09:44.216462Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set this to True to actually generate ordinary smoothed reads that include called mutations;\n",
    "# set this to False to generate \"sanity check\" perfect smoothed reads, where no mutations are included\n",
    "# and the read entirely matches the reference\n",
    "actually_include_mutations_in_the_smoothed_reads = True\n",
    "\n",
    "add_virtual_reads = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T10:09:44.550099Z",
     "iopub.status.busy": "2022-03-21T10:09:44.220755Z",
     "iopub.status.idle": "2022-03-21T10:10:01.668777Z",
     "shell.execute_reply": "2022-03-21T10:10:01.669440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence edge_6104 has average coverage 4,158.57 and median coverage 4,122.00.\n",
      "Sequence edge_1671 has average coverage 1,415.07 and median coverage 1,436.00.\n",
      "Sequence edge_2358 has average coverage 2,993.46 and median coverage 2,936.00.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'edge_6104': 4158.572468826692,\n",
       " 'edge_1671': 1415.072755380576,\n",
       " 'edge_2358': 2993.461913625056}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll need to know the mean coverage of each sequence when computing virtual reads.\n",
    "seq2meancov = get_meancovs()\n",
    "seq2meancov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T10:10:01.686105Z",
     "iopub.status.busy": "2022-03-21T10:10:01.683412Z",
     "iopub.status.idle": "2022-03-21T12:49:09.479624Z",
     "shell.execute_reply": "2022-03-21T12:49:09.480481Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Identifying mutated positions (p = 1%) in genome CAMP...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9de74ae72e6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Identifying mutated positions (p = {P}%) in genome {seq2name[seq]}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mmutpos2pileup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_mutated_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_to_use\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincl_pileup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;31m# We sort because the code below relies on these being in ascending order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mmutated_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmutpos2pileup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Poppy/mfedarko/sheepgut/notebooks/linked_mutations_utils.py\u001b[0m in \u001b[0;36mfind_mutated_positions\u001b[0;34m(seq, p_to_use, incl_pileup)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpileup\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \"\"\"\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mseq2pos2pileup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpileup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincl_pileup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mmutated_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Poppy/mfedarko/sheepgut/notebooks/pileup.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(picklepath)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# https://stackoverflow.com/a/18261955\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpicklepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpicklefile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpicklefile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bf = pysam.AlignmentFile(\"../main-workflow/output/fully-filtered-and-sorted-aln.bam\", \"rb\")\n",
    "output_dir = \"phasing-data/smoothed-reads/\"\n",
    "\n",
    "# verbose?\n",
    "no_indoor_voice = False\n",
    "\n",
    "def write_out_reads(filepath, readname2seq):\n",
    "    # Notably, this uses the \"a\" (append) method in order to add to the end of a file\n",
    "    with open(filepath, \"a\") as of:\n",
    "        for readname in readname2seq:\n",
    "            # Write out both the header and the sequence for each read\n",
    "            of.write(f\">{readname}\\n{str(readname2seq[readname])}\\n\")\n",
    "            \n",
    "ALN_UPDATE_FREQ = 5000\n",
    "ALN_BUFFER_FREQ = 1000\n",
    "VR_EXTRA_SPAN = 100\n",
    "\n",
    "P = 1\n",
    "            \n",
    "t1 = time.time()\n",
    "for seq in SEQS:\n",
    "    \n",
    "    # Record which positions (0-indexed) aren't covered by any smoothed reads in this MAG.\n",
    "    # We'll add \"virtual reads\" that span these positions.\n",
    "    uncovered_positions = set(range(0, seq2len[seq]))\n",
    "    \n",
    "    fasta = skbio.DNA.read(f\"../seqs/{seq}.fasta\")\n",
    "    \n",
    "    output_smoothed_reads_file = os.path.join(output_dir, f\"{seq}_smoothed_reads_delignore.fasta\")\n",
    "    \n",
    "    # Identify all (0-indexed, so compatible with skbio / pysam!)\n",
    "    # mutated positions in this genome up front to save time.\n",
    "    #\n",
    "    # Equivalently, we could also just take in an arbitrary VCF as input\n",
    "    # (e.g. one produced from another variant calling tool), although we'd\n",
    "    # need to be careful to only include SNVs and not indels/etc...\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Identifying mutated positions (p = {P}%) in genome {seq2name[seq]}...\")\n",
    "    mutpos2pileup = find_mutated_positions(seq, p_to_use=P, incl_pileup=True)\n",
    "    # We sort because the code below relies on these being in ascending order\n",
    "    mutated_positions = sorted(mutpos2pileup.keys())\n",
    "    print(f\"Found {len(mutated_positions):,} mutated positions (p = {P}%) in {seq2name[seq]}.\")\n",
    "    print(\n",
    "        f\"Note that this tally is higher than you'd see in e.g. the CP1/2/3 plots, because now we're including \"\n",
    "        \"both 'rare' and non-rare mutations. Just so you don't waste five minutes sanity-checking this like I did.\"\n",
    "    )\n",
    "    \n",
    "    print(\"Going through these positions...\")\n",
    "    \n",
    "    num_ignored_alns = 0\n",
    "    \n",
    "    # Instead of just writing out every smoothed alignment as soon as we generate it, we build up a \"buffer\"\n",
    "    # of these alignments and then write a bunch out at once. This way we limit slowdown due to constantly\n",
    "    # having to open/close files. I don't really have a good source for this as best practice, but I remembered\n",
    "    # to do it while writing this code, so somewhere in College Park the CS faculty at Maryland are smiling\n",
    "    #\n",
    "    # Also fyi this maps read name to smoothed alignment (well, at this point, just read) sequence. The read name\n",
    "    # is useful to preserve in fasta files so we have some idea of provenance (where smoothed reads came from)\n",
    "    smoothed_aln_buffer = {}\n",
    "    \n",
    "    # The first time we see an alignment of a read, it's 1; if we see a supp aln of this read, it's 2; etc.\n",
    "    # Lets us distinguish alignments with different names\n",
    "    readname2freq_so_far = defaultdict(int)\n",
    "    \n",
    "    # Go through all linear alignments of each read to this genome, focusing (for now) on just the primary\n",
    "    # alignments...\n",
    "    ts1 = time.time()\n",
    "    for ai, aln in enumerate(bf.fetch(seq), 1):\n",
    "        \n",
    "        if ai % ALN_UPDATE_FREQ == 0:\n",
    "            print(\n",
    "                f\"\\tOn aln {ai:,} in seq {seq2name[seq]}. \"\n",
    "                f\"Time spent on {seq2name[seq]} so far: {time.time() - ts1:,.2f} sec.\"\n",
    "            )\n",
    "            \n",
    "        if aln.is_secondary:\n",
    "            raise ValueError(\n",
    "                \"Not to get political or anything, but you should've already filtered secondary alns out\"\n",
    "            )\n",
    "            \n",
    "        # Note that supplementary alignments are ok, though! We implicitly handle these here.\n",
    "        #\n",
    "        # Different alignments of the same read will have different new_readnames, because we're gonna\n",
    "        # be treating them as distinct \"reads\". We should have already filtered reference-overlapping\n",
    "        # supp alns so this shouldn't be a problem\n",
    "        \n",
    "        readname = aln.query_name\n",
    "        readname2freq_so_far[readname] += 1\n",
    "        new_readname = f\"{readname}_{readname2freq_so_far[readname]}\"\n",
    "        \n",
    "        # should never happen\n",
    "        if new_readname in smoothed_aln_buffer:\n",
    "            raise ValueError(\"This exact read alignment has already been smoothed? Weird.\")\n",
    "            \n",
    "        # Figure out where on the MAG this alignment \"hits.\" These are 0-indexed positions from Pysam.\n",
    "        # (reference_end points to the position after the actual final position, since these are designed to\n",
    "        # be interoperable with Python's half-open intervals.)\n",
    "        #\n",
    "        # Of course, there likely will be indels within this range: we're purposefully ignoring those here.\n",
    "        ref_start = aln.reference_start\n",
    "        ref_end = aln.reference_end - 1\n",
    "        \n",
    "        # This should never happen (TM)\n",
    "        if ref_start >= ref_end:\n",
    "            # Du sollst jetzt mit Gott sprechen.\n",
    "            raise ValueError(\n",
    "                f\"Ref start {ref_start:,} >= ref end {ref_end:,} for read {new_readname}?\"\n",
    "            )\n",
    "            \n",
    "        # Smoothed sequence; we'll edit this so that if this read has (mis)matches to any called mutated\n",
    "        # positions, these positions are updated with the read's aligned nucleotides at these positions.\n",
    "        smoothed_aln_seq = fasta[ref_start: ref_end + 1]\n",
    "        \n",
    "        if actually_include_mutations_in_the_smoothed_reads:\n",
    "            # just for debugging: track the exact edits made to smoothed_aln_seq\n",
    "            replacements_made = {}\n",
    "\n",
    "            # We may choose to ignore this linear alignment, if we think it is error-prone or\n",
    "            # otherwise not useful. If this gets set to True in the loop below, then we'll notice this\n",
    "            # and ignore this alignment.\n",
    "            ignoring_this_aln = False\n",
    "            \n",
    "            # Notably, include skips -- this way, we can figure out if the aln has a deletion at a mutated\n",
    "            # position, and if so ignore this aln\n",
    "            ap = aln.get_aligned_pairs(matches_only=False)\n",
    "\n",
    "            # Iterating through the aligned pairs is expensive. Since read lengths are generally in the thousands\n",
    "            # to tens of thousands of bp (which is much less than the > 1 million bp length of any bacterial genome),\n",
    "            # we set things up so that we only iterate through the aligned pairs once. We maintain an integer, mpi,\n",
    "            # that is a poor man's \"pointer\" to an index in mutated_positions.\n",
    "\n",
    "            mpi = 0\n",
    "\n",
    "            # Go through this aln's aligned pairs. As we see each pair, compare the pair's reference position\n",
    "            # (refpos) to the mpi-th mutated position (herein referred to as \"mutpos\").\n",
    "            #\n",
    "            # If refpos >  mutpos, increment mpi until refpos <= mutpos (stopping as early as possible).\n",
    "            # If refpos == mutpos, we have a match! Update readname2mutpos2ismutated[mutpos] based on\n",
    "            #                      comparing the read to the reference at the aligned positions.\n",
    "            # If refpos <  mutpos, continue to the next pair.\n",
    "\n",
    "            for pair in ap:\n",
    "\n",
    "                refpos = pair[1]\n",
    "                \n",
    "                # Since we set matches_only (for get_aligned_pairs()) to False, the alignment could include\n",
    "                # insertions (which are encoded as the reference pos being set to None). We inherently ignore\n",
    "                # these insertions as part of the read smoothing process.\n",
    "                if refpos is None:\n",
    "                    continue\n",
    "                    \n",
    "                mutpos = mutated_positions[mpi]\n",
    "\n",
    "                no_mutations_to_right_of_here = False\n",
    "\n",
    "                # Increment mpi until we get to the next mutated position at or after the reference pos for this\n",
    "                # aligned pair (or until we run out of mutated positions).\n",
    "                while refpos > mutpos:\n",
    "                    mpi += 1\n",
    "                    if mpi < len(mutated_positions):\n",
    "                        mutpos = mutated_positions[mpi]\n",
    "                    else:\n",
    "                        no_mutations_to_right_of_here = True\n",
    "                        break\n",
    "\n",
    "                # I expect this should happen only for reads aligned near the right end of the genome.\n",
    "                if no_mutations_to_right_of_here:\n",
    "                    break\n",
    "\n",
    "                # If the next mutation occurs after this aligned pair, continue on to a later pair.\n",
    "                if refpos < mutpos:\n",
    "                    continue\n",
    "\n",
    "                # If we've made it here, refpos == mutpos!\n",
    "                # (...unless I messed something up in how I designed this code.)\n",
    "                if refpos != mutpos:\n",
    "                    raise ValueError(\"This should never happen!\")\n",
    "\n",
    "                # Finally, get the nucleotide aligned to this mutated position from this read.\n",
    "                readpos = pair[0]\n",
    "                \n",
    "                # Since we set matches_only (for get_aligned_pairs()) to False, there's a chance a read\n",
    "                # contains deletions aligned to mutated positions. This is accounted for by this case.\n",
    "                # If this happens, we ignore this alignment (the same as if it would contain a nt that\n",
    "                # isn't the first or second most common).\n",
    "                if readpos is None:\n",
    "                    if no_indoor_voice:\n",
    "                        print(\n",
    "                            f\"Read {new_readname} has deletion at mutpos {mutpos + 1:,}: \"\n",
    "                            f\"pileup = {mutpos2pileup[mutpos]}\"\n",
    "                        )\n",
    "                    ignoring_this_aln = True\n",
    "                    break\n",
    "                    \n",
    "                read_nt = aln.query_sequence[readpos]\n",
    "\n",
    "                # If this aln doesn't have match the first or second most common nucleotide at this position,\n",
    "                # ignore it. In the future, when we perform read  smoothing based on an arbitrary set of\n",
    "                # SNV calls, we can be more careful about this; but for now we make the simplifiying assumption\n",
    "                # that a mutation likely only has one alternate nucleotide, and that the 3rd and 4th most\n",
    "                # common nucleotides indicate errors. (Also, note that we break ties here arbitrarily.)\n",
    "                nt2ct = dict(zip(\"ACGT\", mutpos2pileup[mutpos][0]))\n",
    "                nt1 = max(nt2ct, key=nt2ct.get)\n",
    "                del nt2ct[nt1]\n",
    "                nt2 = max(nt2ct, key=nt2ct.get)\n",
    "                \n",
    "                if read_nt != nt1 and read_nt != nt2:\n",
    "                    if no_indoor_voice:\n",
    "                        print(\n",
    "                            f\"Read {new_readname} has 3rd or 4th most common nt at mutpos {mutpos + 1:,}: \"\n",
    "                            f\"pileup = {mutpos2pileup[mutpos]}, read = {read_nt}\"\n",
    "                        )\n",
    "                    ignoring_this_aln = True\n",
    "                    break\n",
    "                    \n",
    "                # Notably, the nucleotide at a mutated position in a smoothed read will always be the first\n",
    "                # or second most common nucleotide at this position. So \"unreasonable\" positions, in which\n",
    "                # the ref nt != the consensus nt, will not be treated as you might expect -- we ignore the\n",
    "                # reference in this particular case. Shouldn't make a big difference, since in most cases\n",
    "                # the ref and consensus nt agree.\n",
    "                #\n",
    "                # Note that we aren't even looking at the reference nt at this point -- in many cases\n",
    "                # the .replace() operation done below won't change anything. Shouldn't matter.\n",
    "                relative_pos_on_aln = mutpos - ref_start\n",
    "                smoothed_aln_seq = smoothed_aln_seq.replace([relative_pos_on_aln], read_nt)\n",
    "                replacements_made[relative_pos_on_aln] = read_nt\n",
    "                if no_indoor_voice:\n",
    "                    print(\n",
    "                        f\"Updated read {new_readname} re: nt at mutpos {mutpos + 1:,}: \"\n",
    "                        f\"pileup = {mutpos2pileup[mutpos]}, read = {read_nt}\"\n",
    "                    )\n",
    "\n",
    "            if no_indoor_voice:\n",
    "                print(f\"Read {new_readname} involved {len(replacements_made):,} replacements!\")\n",
    "        \n",
    "        if ignoring_this_aln:\n",
    "            num_ignored_alns += 1\n",
    "        else:\n",
    "            # Now that we've finished processing all called mutations that this alignment spans, prepare it\n",
    "            # to be written out to a FASTA file. See comments above on smoothed_aln_buffer, and why we don't\n",
    "            # just write everything out as soon as it's ready.\n",
    "            #\n",
    "            # (Also, we've already guaranteed readname isn't already in smoothed_aln_buffer, so no need to worry\n",
    "            # about accidentally overwriting something from earlier.)\n",
    "            smoothed_aln_buffer[new_readname] = smoothed_aln_seq\n",
    "\n",
    "            # Record which positions this read covers (of course, it may not exactly \"cover\" these positions\n",
    "            # originally due to indels, but the smoothed version will cover them).\n",
    "            # We don't update uncovered_positions until *after* we process all aligned pairs of this read, to allow\n",
    "            # us to ignore reads if desired.\n",
    "            uncovered_positions -= set(range(ref_start, ref_end + 1))\n",
    "\n",
    "            if ai % ALN_BUFFER_FREQ == 0:\n",
    "                write_out_reads(output_smoothed_reads_file, smoothed_aln_buffer)\n",
    "                # Clear the buffer\n",
    "                smoothed_aln_buffer = {}\n",
    "        \n",
    "    # We're probably going to have left over smoothed reads that we still haven't written out, unless things\n",
    "    # worked out so that on the final alignment we saw ai was exactly divisible by ALN_BUFFER_FREQ (and that's\n",
    "    # pretty unlikely unless you set the buffer freq to a low number). So make one last dump of the buffer.\n",
    "    if len(smoothed_aln_buffer) > 0:\n",
    "        write_out_reads(output_smoothed_reads_file, smoothed_aln_buffer)\n",
    "        \n",
    "    print(f\"We ignored {num_ignored_alns:,} linear alignments, fyi.\")\n",
    "        \n",
    "    if add_virtual_reads and len(uncovered_positions) > 0:\n",
    "        print(f\"For reference, there are {len(uncovered_positions):,} uncovered positions in {seq2name[seq]}.\")\n",
    "        \n",
    "        sup = sorted(uncovered_positions)\n",
    "        uc_runs = convert_to_runs(sup)\n",
    "        print(f'And there are {len(uc_runs)} \"runs\" of uncovered positions.')\n",
    "        \n",
    "        rounded_meancov = round(seq2meancov[seq])\n",
    "        print(\n",
    "            f'Adding \"virtual reads\" spanning each of these runs, at rounded mean coverage '\n",
    "            f'of {rounded_meancov:,}x, to account for this...'\n",
    "        )\n",
    "        \n",
    "        num_vr = 0\n",
    "        vr_buffer = {}\n",
    "        for run in uc_runs:\n",
    "            # Construct a virtual read that includes this entire run of uncovered positions as well\n",
    "            # as VR_EXTRA_SPAN positions before and after (clamping to the start/end of the seq if needed).\n",
    "            #\n",
    "            # Notably, we could try to make this loop around from end -> start if this is a cyclic MAG, but\n",
    "            # to remain consistent with how we handle supplementary alignments above -- and because implementing\n",
    "            # the loop around would be a lot of work and it's like 3am -- we ignore this for now.\n",
    "            #\n",
    "            # Also, note that run_start can equal run_end, if only a single isolated position is uncovered.\n",
    "            # This is fine -- the code handles this case automatically. (I guess the only potential problem is\n",
    "            # if the length of the MAG is less than VR_EXTRA_SPAN, but... that should never happen. If you have\n",
    "            # like 100bp-long MAGs that's a problem! I guess, TODO, make note of this when generalizing this\n",
    "            # code.)\n",
    "            \n",
    "            run_start = max(run[0] - VR_EXTRA_SPAN, 0)\n",
    "            run_end = min(run[1] + VR_EXTRA_SPAN, seq2len[seq] - 1)\n",
    "            \n",
    "            # Generate a sequence matching the \"reference\" MAG at these positions. We of course don't have\n",
    "            # any info about mutations here, because these positions are uncovered by the real reads!\n",
    "            vr_seq = fasta[run_start: run_end + 1]\n",
    "            \n",
    "            # We need to assign reads unique names, and including the run coordinates here is a nice way\n",
    "            # to preserve uniqueness across runs and also make our smoothed reads files easier to interpret\n",
    "            vr_name_prefix = f\"vr_{run[0]}_{run[1]}\"\n",
    "            \n",
    "            # Add M copies of this virtual read, where M = (rounded mean coverage of this MAG)\n",
    "            for vr_num in range(1, rounded_meancov + 1):\n",
    "                vr_name = f\"{vr_name_prefix}_{vr_num}\"\n",
    "                vr_buffer[vr_name] = vr_seq\n",
    "                num_vr += 1\n",
    "                \n",
    "        write_out_reads(output_smoothed_reads_file, vr_buffer)\n",
    "        print(f\"Wrote out {num_vr:,} virtual reads.\")\n",
    "    \n",
    "    print(f\"Done with {seq}! Took {time.time() - ts1:,.2f} sec.\")\n",
    "        \n",
    "print(f\"Time taken: {time.time() - t1:,} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assemble these smoothed reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T12:49:09.786436Z",
     "iopub.status.busy": "2022-03-21T12:49:09.783653Z",
     "iopub.status.idle": "2022-03-21T13:05:14.241393Z",
     "shell.execute_reply": "2022-03-21T13:05:14.242134Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:00 2Mb  INFO: Hello! You are running La Jolla Assembler (LJA), a tool for genome assembly from PacBio HiFi reads\n",
      "00:00:19 2Mb  INFO: eaeb8812405afe9b8ffa8ded423178299ee296c2\n",
      "00:00:19 5Mb  INFO: LJA pipeline started\n",
      "00:00:19 5Mb  INFO: Performing initial correction with k = 5001\n",
      "00:00:19 0Mb  INFO: Reading reads\n",
      "00:00:19 0Mb  INFO: Extracting minimizers\n",
      "00:00:36 3.4Gb  INFO: Finished read processing\n",
      "00:00:36 3.4Gb  INFO: 5254841 hashs collected. Starting sorting.\n",
      "00:00:36 3.4Gb  INFO: Finished sorting. Total distinct minimizers: 4301\n",
      "00:00:36 3.4Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:36 3.4Gb  INFO: Vertex map constructed.\n",
      "00:00:36 3.4Gb  INFO: Filling edge sequences.\n",
      "00:00:53 4.5Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:00:53 4.5Gb  INFO:  Collecting tips \n",
      "00:00:53 4.5Gb  INFO: Added 49 artificial minimizers from tips.\n",
      "00:00:53 4.5Gb  INFO: Collected 8642 old edges.\n",
      "00:00:53 4.5Gb  INFO: New minimizers added to sparse graph.\n",
      "00:00:53 4.5Gb  INFO: Refilling graph with old edges.\n",
      "00:00:53 4.5Gb  INFO: Filling graph with new edges.\n",
      "00:00:53 4.5Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:00:53 4.5Gb  INFO: Starting to extract disjointigs.\n",
      "00:00:53 4.5Gb  INFO: Finished extracting 127 disjointigs of total size 1725911\n",
      "00:00:54 0Mb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/k5001/disjointigs.fasta\"\n",
      "00:00:54 8Mb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:00:54 9Mb  INFO: Filled 5049531 bits out of 34905088\n",
      "00:00:54 9Mb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:00:55 9Mb  INFO: Collected 534 junctions.\n",
      "00:00:55 9Mb  INFO: Starting DBG construction.\n",
      "00:00:55 9Mb  INFO: Vertices created.\n",
      "00:00:55 9Mb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:00:55 9Mb  INFO: Added 0 hanging vertices\n",
      "00:00:55 9Mb  INFO: Merging unbranching paths\n",
      "00:00:55 9Mb  INFO: Ended merging edges. Resulting size 138\n",
      "00:00:55 9Mb  INFO: Cleaning edge coverages\n",
      "00:00:55 9Mb  INFO: Collecting alignments of sequences to the graph\n",
      "00:01:09 4.5Gb  INFO: Alignment collection finished. Total length of alignments is 496777\n",
      "00:01:09 4.5Gb  INFO: Could not correct 98 reads. They will be removed.\n",
      "00:01:09 4.5Gb  INFO: Uncorrected reads were removed.\n",
      "00:01:09 4.5Gb  INFO: Applying changes to the graph\n",
      "00:01:12 4.6Gb  INFO: Printing reads to fasta file \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/k5001/corrected_reads.fasta\"\n",
      "00:01:42 5Mb  INFO: Performing repeat resolution by transforming de Bruijn graph into Multiplex de Bruijn graph\n",
      "00:01:42 0Mb  INFO: Loading graph from fasta\n",
      "00:01:43 7Mb  INFO: Finished loading graph\n",
      "00:01:43 156Mb  INFO: Looking for unique edges\n",
      "00:01:43 156Mb  INFO: Marked 18 long edges as unique\n",
      "00:01:43 156Mb  INFO: Marking extra edges as unique based on read paths\n",
      "00:01:43 156Mb  INFO: Marked 18 edges as unique\n",
      "00:01:43 156Mb  INFO: Splitting graph with unique edges\n",
      "00:01:43 156Mb  INFO: Processing 11 components\n",
      "00:01:43 156Mb  INFO: Finished unique edges search. Found 68 unique edges\n",
      "00:01:43 156Mb  INFO: Analysing repeats of multiplicity 2 and looking for additional unique edges\n",
      "00:01:43 156Mb  INFO: Finished processing of repeats of multiplicity 2. Found 0 erroneous edges.\n",
      "00:01:43 156Mb  INFO: Resolving repeats\n",
      "00:01:43 156Mb  INFO: Constructing paths\n",
      "00:01:47 314Mb  INFO: Building graph\n",
      "00:01:47 314Mb  INFO: Increasing k\n",
      "00:01:47 314Mb  INFO: Finished increasing k\n",
      "00:01:47 314Mb  INFO: Exporting remaining active transitions\n",
      "00:01:47 314Mb  INFO: Export to Dot\n",
      "00:01:47 314Mb  INFO: Export to GFA and compressed contigs\n",
      "00:01:47 319Mb  INFO: Finished repeat resolution\n",
      "00:01:48 5Mb  INFO: Performing polishing and homopolymer uncompression\n",
      "00:01:48 6Mb  INFO: Aligning reads back to assembly\n",
      "00:02:05 4.1Gb  INFO: Finished alignment.\n",
      "00:02:05 4.1Gb  INFO: Printing alignments to \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/uncompressing/alignments.txt\"\n",
      "00:02:06 4.1Gb  INFO: Reading and processing initial reads from [\"phasing-data/smoothed-reads/edge_6104_smoothed_reads.fasta\"]\n",
      "00:04:48 4.9Gb  INFO: Uncompressing homopolymers in contigs\n",
      "00:04:49 4.9Gb  INFO: Total zero covered nucleotides 0\n",
      "00:04:49 4.9Gb  INFO: Calculating overlaps between adjacent uncompressed edges\n",
      "00:04:49 4.9Gb  INFO: Printing final gfa file to \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/mdbg.gfa\"\n",
      "00:04:49 4.9Gb  INFO: Printing final assembly to \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/assembly.fasta\"\n",
      "00:04:49 5Mb  INFO: Final homopolymer compressed and corrected reads can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/k5001/corrected_reads.fasta\"\n",
      "00:04:49 5Mb  INFO: Final graph with homopolymer compressed edges can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/mdbg/mdbg.hpc.gfa\"\n",
      "00:04:49 5Mb  INFO: Final graph can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/mdbg.gfa\"\n",
      "00:04:49 5Mb  INFO: Final assembly can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_cf_10x_p1pct/assembly.fasta\"\n",
      "00:04:49 5Mb  INFO: LJA pipeline finished\n",
      "00:00:00 2Mb  INFO: Hello! You are running La Jolla Assembler (LJA), a tool for genome assembly from PacBio HiFi reads\n",
      "00:00:19 2Mb  INFO: eaeb8812405afe9b8ffa8ded423178299ee296c2\n",
      "00:00:19 5Mb  INFO: LJA pipeline started\n",
      "00:00:19 5Mb  INFO: Performing initial correction with k = 5001\n",
      "00:00:19 0Mb  INFO: Reading reads\n",
      "00:00:19 0Mb  INFO: Extracting minimizers\n",
      "00:00:29 3Gb  INFO: Finished read processing\n",
      "00:00:29 3Gb  INFO: 3126630 hashs collected. Starting sorting.\n",
      "00:00:30 3Gb  INFO: Finished sorting. Total distinct minimizers: 41410\n",
      "00:00:30 3Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:30 3Gb  INFO: Vertex map constructed.\n",
      "00:00:30 3Gb  INFO: Filling edge sequences.\n",
      "00:00:41 3.1Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:00:41 3.1Gb  INFO:  Collecting tips \n",
      "00:00:41 3.1Gb  INFO: Added 3489 artificial minimizers from tips.\n",
      "00:00:41 3.1Gb  INFO: Collected 81674 old edges.\n",
      "00:00:41 3.1Gb  INFO: New minimizers added to sparse graph.\n",
      "00:00:41 3.1Gb  INFO: Refilling graph with old edges.\n",
      "00:00:41 3.1Gb  INFO: Filling graph with new edges.\n",
      "00:00:41 3.1Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:00:42 3.1Gb  INFO: Starting to extract disjointigs.\n",
      "00:00:42 3.1Gb  INFO: Finished extracting 5099 disjointigs of total size 36228569\n",
      "00:00:42 0Mb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/k5001/disjointigs.fasta\"\n",
      "00:00:43 55Mb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:00:45 56Mb  INFO: Filled 49640419 bits out of 343311040\n",
      "00:00:45 56Mb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:00:46 56Mb  INFO: Collected 9836 junctions.\n",
      "00:00:46 56Mb  INFO: Starting DBG construction.\n",
      "00:00:46 56Mb  INFO: Vertices created.\n",
      "00:00:46 56Mb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:00:46 56Mb  INFO: Added 0 hanging vertices\n",
      "00:00:46 56Mb  INFO: Merging unbranching paths\n",
      "00:00:46 56Mb  INFO: Ended merging edges. Resulting size 5848\n",
      "00:00:46 56Mb  INFO: Cleaning edge coverages\n",
      "00:00:46 56Mb  INFO: Collecting alignments of sequences to the graph\n",
      "00:00:52 3Gb  INFO: Alignment collection finished. Total length of alignments is 730337\n",
      "00:00:53 3Gb  INFO: Could not correct 6408 reads. They will be removed.\n",
      "00:00:53 3Gb  INFO: Uncorrected reads were removed.\n",
      "00:00:53 3Gb  INFO: Applying changes to the graph\n",
      "00:00:56 3Gb  INFO: Printing reads to fasta file \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/k5001/corrected_reads.fasta\"\n",
      "00:01:13 5Mb  INFO: Performing repeat resolution by transforming de Bruijn graph into Multiplex de Bruijn graph\n",
      "00:01:13 0Mb  INFO: Loading graph from fasta\n",
      "00:01:14 14Mb  INFO: Finished loading graph\n",
      "00:01:14 89Mb  INFO: Looking for unique edges\n",
      "00:01:14 89Mb  INFO: Marked 86 long edges as unique\n",
      "00:01:14 89Mb  INFO: Marking extra edges as unique based on read paths\n",
      "00:01:14 89Mb  INFO: Marked 94 edges as unique\n",
      "00:01:14 89Mb  INFO: Splitting graph with unique edges\n",
      "00:01:14 89Mb  INFO: Processing 140 components\n",
      "00:01:14 89Mb  INFO: Finished unique edges search. Found 110 unique edges\n",
      "00:01:14 89Mb  INFO: Analysing repeats of multiplicity 2 and looking for additional unique edges\n",
      "00:01:14 89Mb  INFO: Finished processing of repeats of multiplicity 2. Found 0 erroneous edges.\n",
      "00:01:14 89Mb  INFO: Resolving repeats\n",
      "00:01:14 89Mb  INFO: Constructing paths\n",
      "00:01:16 172Mb  INFO: Building graph\n",
      "00:01:16 172Mb  INFO: Increasing k\n",
      "00:01:16 174Mb  INFO: Finished increasing k\n",
      "00:01:16 174Mb  INFO: Exporting remaining active transitions\n",
      "00:01:16 174Mb  INFO: Export to Dot\n",
      "00:01:16 174Mb  INFO: Export to GFA and compressed contigs\n",
      "00:01:17 181Mb  INFO: Finished repeat resolution\n",
      "00:01:17 5Mb  INFO: Performing polishing and homopolymer uncompression\n",
      "00:01:17 6Mb  INFO: Aligning reads back to assembly\n",
      "00:01:26 2.1Gb  INFO: Finished alignment.\n",
      "00:01:26 2.1Gb  INFO: Printing alignments to \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/uncompressing/alignments.txt\"\n",
      "00:01:26 2.1Gb  INFO: Reading and processing initial reads from [\"phasing-data/smoothed-reads/edge_1671_smoothed_reads.fasta\"]\n",
      "00:02:10 3.4Gb  INFO: Uncompressing homopolymers in contigs\n",
      "00:02:10 3.4Gb  INFO: Total zero covered nucleotides 0\n",
      "00:02:10 3.4Gb  INFO: Calculating overlaps between adjacent uncompressed edges\n",
      "00:02:10 3.4Gb  INFO: Printing final gfa file to \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/mdbg.gfa\"\n",
      "00:02:10 3.4Gb  INFO: Printing final assembly to \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/assembly.fasta\"\n",
      "00:02:10 5Mb  INFO: Final homopolymer compressed and corrected reads can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/k5001/corrected_reads.fasta\"\n",
      "00:02:10 5Mb  INFO: Final graph with homopolymer compressed edges can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/mdbg/mdbg.hpc.gfa\"\n",
      "00:02:10 5Mb  INFO: Final graph can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/mdbg.gfa\"\n",
      "00:02:10 5Mb  INFO: Final assembly can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_cf_10x_p1pct/assembly.fasta\"\n",
      "00:02:10 5Mb  INFO: LJA pipeline finished\n",
      "00:00:00 2Mb  INFO: Hello! You are running La Jolla Assembler (LJA), a tool for genome assembly from PacBio HiFi reads\n",
      "00:00:19 2Mb  INFO: eaeb8812405afe9b8ffa8ded423178299ee296c2\n",
      "00:00:19 2Mb  INFO: LJA pipeline started\n",
      "00:00:19 2Mb  INFO: Performing initial correction with k = 5001\n",
      "00:00:19 0Mb  INFO: Reading reads\n",
      "00:00:19 0Mb  INFO: Extracting minimizers\n",
      "00:00:48 4.4Gb  INFO: Finished read processing\n",
      "00:00:48 4.4Gb  INFO: 9383594 hashs collected. Starting sorting.\n",
      "00:00:48 4.4Gb  INFO: Finished sorting. Total distinct minimizers: 10142\n",
      "00:00:48 4.4Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:48 4.4Gb  INFO: Vertex map constructed.\n",
      "00:00:48 4.4Gb  INFO: Filling edge sequences.\n",
      "00:01:18 5.7Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:01:18 5.7Gb  INFO:  Collecting tips \n",
      "00:01:18 5.7Gb  INFO: Added 29 artificial minimizers from tips.\n",
      "00:01:18 5.7Gb  INFO: Collected 20468 old edges.\n",
      "00:01:18 5.7Gb  INFO: New minimizers added to sparse graph.\n",
      "00:01:18 5.7Gb  INFO: Refilling graph with old edges.\n",
      "00:01:18 5.7Gb  INFO: Filling graph with new edges.\n",
      "00:01:18 5.7Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:01:18 5.7Gb  INFO: Starting to extract disjointigs.\n",
      "00:01:19 5.7Gb  INFO: Finished extracting 326 disjointigs of total size 4173388\n",
      "00:01:19 0Mb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/k5001/disjointigs.fasta\"\n",
      "00:01:19 15Mb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:01:20 15Mb  INFO: Filled 11772074 bits out of 81377984\n",
      "00:01:20 15Mb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:01:20 15Mb  INFO: Collected 1187 junctions.\n",
      "00:01:20 15Mb  INFO: Starting DBG construction.\n",
      "00:01:20 15Mb  INFO: Vertices created.\n",
      "00:01:20 15Mb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:01:20 15Mb  INFO: Added 0 hanging vertices\n",
      "00:01:20 15Mb  INFO: Merging unbranching paths\n",
      "00:01:20 15Mb  INFO: Ended merging edges. Resulting size 242\n",
      "00:01:21 15Mb  INFO: Cleaning edge coverages\n",
      "00:01:21 15Mb  INFO: Collecting alignments of sequences to the graph\n",
      "00:01:41 6.7Gb  INFO: Alignment collection finished. Total length of alignments is 883694\n",
      "00:01:41 6.7Gb  INFO: Could not correct 171 reads. They will be removed.\n",
      "00:01:41 6.7Gb  INFO: Uncorrected reads were removed.\n",
      "00:01:41 6.7Gb  INFO: Applying changes to the graph\n",
      "00:01:47 6.9Gb  INFO: Printing reads to fasta file \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/k5001/corrected_reads.fasta\"\n",
      "00:02:41 5Mb  INFO: Performing repeat resolution by transforming de Bruijn graph into Multiplex de Bruijn graph\n",
      "00:02:41 0Mb  INFO: Loading graph from fasta\n",
      "00:02:41 13Mb  INFO: Finished loading graph\n",
      "00:02:42 245Mb  INFO: Looking for unique edges\n",
      "00:02:42 245Mb  INFO: Marked 20 long edges as unique\n",
      "00:02:42 245Mb  INFO: Marking extra edges as unique based on read paths\n",
      "00:02:42 245Mb  INFO: Marked 20 edges as unique\n",
      "00:02:42 245Mb  INFO: Splitting graph with unique edges\n",
      "00:02:42 245Mb  INFO: Processing 12 components\n",
      "00:02:42 245Mb  INFO: Finished unique edges search. Found 208 unique edges\n",
      "00:02:42 245Mb  INFO: Analysing repeats of multiplicity 2 and looking for additional unique edges\n",
      "00:02:42 245Mb  INFO: Finished processing of repeats of multiplicity 2. Found 0 erroneous edges.\n",
      "00:02:42 245Mb  INFO: Resolving repeats\n",
      "00:02:42 245Mb  INFO: Constructing paths\n",
      "00:02:49 0.5Gb  INFO: Building graph\n",
      "00:02:49 0.5Gb  INFO: Increasing k\n",
      "00:02:53 0.5Gb  INFO: Finished increasing k\n",
      "00:02:53 0.5Gb  INFO: Exporting remaining active transitions\n",
      "00:02:53 0.5Gb  INFO: Export to Dot\n",
      "00:02:53 0.5Gb  INFO: Export to GFA and compressed contigs\n",
      "00:02:53 0.5Gb  INFO: Finished repeat resolution\n",
      "00:02:54 5Mb  INFO: Performing polishing and homopolymer uncompression\n",
      "00:02:54 11Mb  INFO: Aligning reads back to assembly\n",
      "00:03:26 7Gb  INFO: Finished alignment.\n",
      "00:03:26 7Gb  INFO: Printing alignments to \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/uncompressing/alignments.txt\"\n",
      "00:03:27 7Gb  INFO: Reading and processing initial reads from [\"phasing-data/smoothed-reads/edge_2358_smoothed_reads.fasta\"]\n",
      "00:09:02 7Gb  INFO: Uncompressing homopolymers in contigs\n",
      "00:09:03 7Gb  INFO: Total zero covered nucleotides 0\n",
      "00:09:03 7Gb  INFO: Calculating overlaps between adjacent uncompressed edges\n",
      "00:09:03 7Gb  INFO: Printing final gfa file to \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/mdbg.gfa\"\n",
      "00:09:03 7Gb  INFO: Printing final assembly to \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/assembly.fasta\"\n",
      "00:09:03 5Mb  INFO: Final homopolymer compressed and corrected reads can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/k5001/corrected_reads.fasta\"\n",
      "00:09:03 5Mb  INFO: Final graph with homopolymer compressed edges can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/mdbg/mdbg.hpc.gfa\"\n",
      "00:09:03 5Mb  INFO: Final graph can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/mdbg.gfa\"\n",
      "00:09:03 5Mb  INFO: Final assembly can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_cf_10x_p1pct/assembly.fasta\"\n",
      "00:09:03 5Mb  INFO: LJA pipeline finished\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# LJA with no error correction, but with a filter for low-coverage edges\n",
    "\n",
    "OUTDIR=phasing-data/smoothed-reads\n",
    "\n",
    "/home/mfedarko/software/LJA-branch/bin/lja \\\n",
    "    --reads $OUTDIR/edge_6104_smoothed_reads_delignore.fasta \\\n",
    "    --simpleec \\\n",
    "    --Cov-threshold 10 \\\n",
    "    --output-dir $OUTDIR/edge_6104_lja_cf_10x_p1pct_delignore\n",
    "\n",
    "/home/mfedarko/software/LJA-branch/bin/lja \\\n",
    "    --reads $OUTDIR/edge_1671_smoothed_reads_delignore.fasta \\\n",
    "    --simpleec \\\n",
    "    --Cov-threshold 10 \\\n",
    "    --output-dir $OUTDIR/edge_1671_lja_cf_10x_p1pct_delignore\n",
    "\n",
    "/home/mfedarko/software/LJA-branch/bin/lja \\\n",
    "    --reads $OUTDIR/edge_2358_smoothed_reads_delignore.fasta \\\n",
    "    --simpleec \\\n",
    "    --Cov-threshold 10 \\\n",
    "    --output-dir $OUTDIR/edge_2358_lja_cf_10x_p1pct_delignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
