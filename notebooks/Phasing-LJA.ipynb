{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform read smoothing then assemble with LJA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T11:06:12.386442Z",
     "iopub.status.busy": "2022-02-07T11:06:12.385488Z",
     "iopub.status.idle": "2022-02-07T11:06:12.717083Z",
     "shell.execute_reply": "2022-02-07T11:06:12.716149Z"
    }
   },
   "outputs": [],
   "source": [
    "%run \"Header.ipynb\"\n",
    "%run \"../main-workflow/utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T11:06:12.721285Z",
     "iopub.status.busy": "2022-02-07T11:06:12.720458Z",
     "iopub.status.idle": "2022-02-07T11:06:13.238408Z",
     "shell.execute_reply": "2022-02-07T11:06:13.239092Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pysam\n",
    "import skbio\n",
    "from collections import defaultdict, Counter\n",
    "from linked_mutations_utils import find_mutated_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Quick sanity check: ensure that all $k$-mers ($k$ = 5,001) are unique in each MAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On start pos 0 in CAMP.\n",
      "On start pos 1,000,000 in CAMP.\n",
      "The most common k = 5,001-mer in CAMP occurred 1 time(s).\n",
      "On start pos 0 in BACT1.\n",
      "On start pos 1,000,000 in BACT1.\n",
      "On start pos 2,000,000 in BACT1.\n",
      "The most common k = 5,001-mer in BACT1 occurred 1 time(s).\n",
      "On start pos 0 in BACT2.\n",
      "On start pos 1,000,000 in BACT2.\n",
      "On start pos 2,000,000 in BACT2.\n",
      "The most common k = 5,001-mer in BACT2 occurred 1 time(s).\n"
     ]
    }
   ],
   "source": [
    "# I know there are actual k-mer counting tools you can use but no reason to overcomplicate things for now\n",
    "\n",
    "k = 5001\n",
    "\n",
    "for seq in SEQS:\n",
    "    fasta = skbio.DNA.read(f\"../seqs/{seq}.fasta\")\n",
    "    bargain_bin_kmer_counter = Counter()\n",
    "    \n",
    "    # The skbio.DNA object is 0-indexed, so 0 is the leftmost k-mer start position and\n",
    "    # ((seq length) - k) is the rightmost k-mer start position. The + 1 is because python ranges don't include\n",
    "    # the right endpoint.\n",
    "    for start_pos in range(0, seq2len[seq] - k + 1):\n",
    "        \n",
    "        # NOTE: this is a terrible no good very bad way to do this; it's more efficient to use a \"sliding window\"\n",
    "        # approach where you store the entire k-mer and then, with each step, just remove the first character and\n",
    "        # add on a new last character. \"But, uh, this code will only be run on these three MAGs, so I'm gonna\n",
    "        # prioritize clarity over optimization,\" says me, the insane person who just spent like a minute writing\n",
    "        # this comment when I could've been optimizing this code instead look WHATEVER this counts k-mers and it's\n",
    "        # 4am let's not overcomplicate it, look if you're on GitHub right now and you see this inane comment\n",
    "        # we can both just pretend that you were looking at some really optimized code and we'll both walk away\n",
    "        # satisfied, capisce\n",
    "        kmer = fasta[start_pos : start_pos + k]\n",
    "        \n",
    "        bargain_bin_kmer_counter[str(kmer)] += 1\n",
    "        if start_pos % 1000000 == 0: print(f\"On start pos {start_pos:,} in {seq2name[seq]}.\")\n",
    "    \n",
    "    mckc = bargain_bin_kmer_counter.most_common(1)[0][1]\n",
    "    print(f\"The most common k = {k:,}-mer in {seq2name[seq]} occurred {mckc:,} time(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Smooth reads\n",
    "\n",
    "Lots of this code is duplicated from the `Phasing-01-MakeGraph.ipynb` notebook in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True to actually generate ordinary smoothed reads that include called mutations;\n",
    "# set this to False to generate \"sanity check\" perfect smoothed reads, where no mutations are included\n",
    "# and the read entirely matches the reference\n",
    "actually_include_mutations_in_the_smoothed_reads = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence edge_6104 has average coverage 4,158.57 and median coverage 4,122.00.\n",
      "Sequence edge_1671 has average coverage 1,415.07 and median coverage 1,436.00.\n",
      "Sequence edge_2358 has average coverage 2,993.46 and median coverage 2,936.00.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'edge_6104': 4158.572468826692,\n",
       " 'edge_1671': 1415.072755380576,\n",
       " 'edge_2358': 2993.461913625056}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll need to know the mean coverage of each sequence when computing virtual reads.\n",
    "seq2meancov = get_meancovs()\n",
    "seq2meancov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T11:06:13.258951Z",
     "iopub.status.busy": "2022-02-07T11:06:13.257971Z",
     "iopub.status.idle": "2022-02-07T11:08:10.160475Z",
     "shell.execute_reply": "2022-02-07T11:08:10.159569Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Identifying mutated positions (p = 1%) in genome CAMP...\n",
      "Found 83 mutated positions (p = 1%) in CAMP.\n",
      "Note that this tally is higher than you'd see in e.g. the CP1/2/3 plots, because now we're including both 'rare' and non-rare mutations. Just so you don't waste five minutes sanity-checking this like I did.\n",
      "Going through these positions...\n",
      "\tOn aln 5,000 in seq CAMP. Time spent on CAMP so far: 13.85 sec.\n",
      "\tOn aln 10,000 in seq CAMP. Time spent on CAMP so far: 36.32 sec.\n",
      "\tOn aln 15,000 in seq CAMP. Time spent on CAMP so far: 57.60 sec.\n",
      "\tOn aln 20,000 in seq CAMP. Time spent on CAMP so far: 80.53 sec.\n",
      "\tOn aln 25,000 in seq CAMP. Time spent on CAMP so far: 102.50 sec.\n",
      "\tOn aln 30,000 in seq CAMP. Time spent on CAMP so far: 123.05 sec.\n",
      "\tOn aln 35,000 in seq CAMP. Time spent on CAMP so far: 143.75 sec.\n",
      "\tOn aln 40,000 in seq CAMP. Time spent on CAMP so far: 165.21 sec.\n",
      "\tOn aln 45,000 in seq CAMP. Time spent on CAMP so far: 186.74 sec.\n",
      "\tOn aln 50,000 in seq CAMP. Time spent on CAMP so far: 206.11 sec.\n",
      "\tOn aln 55,000 in seq CAMP. Time spent on CAMP so far: 226.67 sec.\n",
      "\tOn aln 60,000 in seq CAMP. Time spent on CAMP so far: 247.07 sec.\n",
      "\tOn aln 65,000 in seq CAMP. Time spent on CAMP so far: 267.58 sec.\n",
      "\tOn aln 70,000 in seq CAMP. Time spent on CAMP so far: 289.99 sec.\n",
      "\tOn aln 75,000 in seq CAMP. Time spent on CAMP so far: 311.85 sec.\n",
      "\tOn aln 80,000 in seq CAMP. Time spent on CAMP so far: 334.16 sec.\n",
      "\tOn aln 85,000 in seq CAMP. Time spent on CAMP so far: 356.33 sec.\n",
      "\tOn aln 90,000 in seq CAMP. Time spent on CAMP so far: 378.05 sec.\n",
      "\tOn aln 95,000 in seq CAMP. Time spent on CAMP so far: 398.06 sec.\n",
      "\tOn aln 100,000 in seq CAMP. Time spent on CAMP so far: 418.52 sec.\n",
      "\tOn aln 105,000 in seq CAMP. Time spent on CAMP so far: 438.45 sec.\n",
      "\tOn aln 110,000 in seq CAMP. Time spent on CAMP so far: 457.65 sec.\n",
      "\tOn aln 115,000 in seq CAMP. Time spent on CAMP so far: 477.57 sec.\n",
      "\tOn aln 120,000 in seq CAMP. Time spent on CAMP so far: 498.10 sec.\n",
      "\tOn aln 125,000 in seq CAMP. Time spent on CAMP so far: 517.37 sec.\n",
      "\tOn aln 130,000 in seq CAMP. Time spent on CAMP so far: 536.50 sec.\n",
      "\tOn aln 135,000 in seq CAMP. Time spent on CAMP so far: 555.61 sec.\n",
      "\tOn aln 140,000 in seq CAMP. Time spent on CAMP so far: 575.52 sec.\n",
      "\tOn aln 145,000 in seq CAMP. Time spent on CAMP so far: 597.12 sec.\n",
      "\tOn aln 150,000 in seq CAMP. Time spent on CAMP so far: 616.49 sec.\n",
      "\tOn aln 155,000 in seq CAMP. Time spent on CAMP so far: 636.21 sec.\n",
      "\tOn aln 160,000 in seq CAMP. Time spent on CAMP so far: 657.30 sec.\n",
      "\tOn aln 165,000 in seq CAMP. Time spent on CAMP so far: 677.25 sec.\n",
      "\tOn aln 170,000 in seq CAMP. Time spent on CAMP so far: 696.63 sec.\n",
      "\tOn aln 175,000 in seq CAMP. Time spent on CAMP so far: 717.02 sec.\n",
      "\tOn aln 180,000 in seq CAMP. Time spent on CAMP so far: 737.06 sec.\n",
      "\tOn aln 185,000 in seq CAMP. Time spent on CAMP so far: 756.66 sec.\n",
      "\tOn aln 190,000 in seq CAMP. Time spent on CAMP so far: 776.41 sec.\n",
      "\tOn aln 195,000 in seq CAMP. Time spent on CAMP so far: 796.57 sec.\n",
      "\tOn aln 200,000 in seq CAMP. Time spent on CAMP so far: 815.94 sec.\n",
      "\tOn aln 205,000 in seq CAMP. Time spent on CAMP so far: 835.40 sec.\n",
      "\tOn aln 210,000 in seq CAMP. Time spent on CAMP so far: 855.00 sec.\n",
      "\tOn aln 215,000 in seq CAMP. Time spent on CAMP so far: 875.63 sec.\n",
      "\tOn aln 220,000 in seq CAMP. Time spent on CAMP so far: 895.32 sec.\n",
      "\tOn aln 225,000 in seq CAMP. Time spent on CAMP so far: 915.28 sec.\n",
      "\tOn aln 230,000 in seq CAMP. Time spent on CAMP so far: 934.38 sec.\n",
      "\tOn aln 235,000 in seq CAMP. Time spent on CAMP so far: 953.70 sec.\n",
      "\tOn aln 240,000 in seq CAMP. Time spent on CAMP so far: 973.01 sec.\n",
      "\tOn aln 245,000 in seq CAMP. Time spent on CAMP so far: 992.69 sec.\n",
      "\tOn aln 250,000 in seq CAMP. Time spent on CAMP so far: 1,012.45 sec.\n",
      "\tOn aln 255,000 in seq CAMP. Time spent on CAMP so far: 1,031.84 sec.\n",
      "\tOn aln 260,000 in seq CAMP. Time spent on CAMP so far: 1,051.18 sec.\n",
      "\tOn aln 265,000 in seq CAMP. Time spent on CAMP so far: 1,071.54 sec.\n",
      "\tOn aln 270,000 in seq CAMP. Time spent on CAMP so far: 1,091.87 sec.\n",
      "\tOn aln 275,000 in seq CAMP. Time spent on CAMP so far: 1,112.52 sec.\n",
      "\tOn aln 280,000 in seq CAMP. Time spent on CAMP so far: 1,132.12 sec.\n",
      "\tOn aln 285,000 in seq CAMP. Time spent on CAMP so far: 1,150.94 sec.\n",
      "\tOn aln 290,000 in seq CAMP. Time spent on CAMP so far: 1,169.98 sec.\n",
      "\tOn aln 295,000 in seq CAMP. Time spent on CAMP so far: 1,189.31 sec.\n",
      "\tOn aln 300,000 in seq CAMP. Time spent on CAMP so far: 1,211.32 sec.\n",
      "\tOn aln 305,000 in seq CAMP. Time spent on CAMP so far: 1,233.62 sec.\n",
      "\tOn aln 310,000 in seq CAMP. Time spent on CAMP so far: 1,256.21 sec.\n",
      "\tOn aln 315,000 in seq CAMP. Time spent on CAMP so far: 1,278.89 sec.\n",
      "\tOn aln 320,000 in seq CAMP. Time spent on CAMP so far: 1,300.79 sec.\n",
      "\tOn aln 325,000 in seq CAMP. Time spent on CAMP so far: 1,320.96 sec.\n",
      "\tOn aln 330,000 in seq CAMP. Time spent on CAMP so far: 1,343.59 sec.\n",
      "\tOn aln 335,000 in seq CAMP. Time spent on CAMP so far: 1,364.59 sec.\n",
      "\tOn aln 340,000 in seq CAMP. Time spent on CAMP so far: 1,385.07 sec.\n",
      "\tOn aln 345,000 in seq CAMP. Time spent on CAMP so far: 1,405.65 sec.\n",
      "\tOn aln 350,000 in seq CAMP. Time spent on CAMP so far: 1,425.97 sec.\n",
      "\tOn aln 355,000 in seq CAMP. Time spent on CAMP so far: 1,446.42 sec.\n",
      "\tOn aln 360,000 in seq CAMP. Time spent on CAMP so far: 1,468.40 sec.\n",
      "\tOn aln 365,000 in seq CAMP. Time spent on CAMP so far: 1,490.85 sec.\n",
      "\tOn aln 370,000 in seq CAMP. Time spent on CAMP so far: 1,512.59 sec.\n",
      "\tOn aln 375,000 in seq CAMP. Time spent on CAMP so far: 1,532.97 sec.\n",
      "\tOn aln 380,000 in seq CAMP. Time spent on CAMP so far: 1,557.24 sec.\n",
      "\tOn aln 385,000 in seq CAMP. Time spent on CAMP so far: 1,582.98 sec.\n",
      "\tOn aln 390,000 in seq CAMP. Time spent on CAMP so far: 1,604.53 sec.\n",
      "\tOn aln 395,000 in seq CAMP. Time spent on CAMP so far: 1,626.19 sec.\n",
      "\tOn aln 400,000 in seq CAMP. Time spent on CAMP so far: 1,645.74 sec.\n",
      "\tOn aln 405,000 in seq CAMP. Time spent on CAMP so far: 1,665.47 sec.\n",
      "\tOn aln 410,000 in seq CAMP. Time spent on CAMP so far: 1,685.25 sec.\n",
      "\tOn aln 415,000 in seq CAMP. Time spent on CAMP so far: 1,704.43 sec.\n",
      "\tOn aln 420,000 in seq CAMP. Time spent on CAMP so far: 1,725.80 sec.\n",
      "\tOn aln 425,000 in seq CAMP. Time spent on CAMP so far: 1,747.44 sec.\n",
      "\tOn aln 430,000 in seq CAMP. Time spent on CAMP so far: 1,767.60 sec.\n",
      "\tOn aln 435,000 in seq CAMP. Time spent on CAMP so far: 1,788.18 sec.\n",
      "\tOn aln 440,000 in seq CAMP. Time spent on CAMP so far: 1,809.33 sec.\n",
      "\tOn aln 445,000 in seq CAMP. Time spent on CAMP so far: 1,838.45 sec.\n",
      "\tOn aln 450,000 in seq CAMP. Time spent on CAMP so far: 1,847.87 sec.\n",
      "\tOn aln 455,000 in seq CAMP. Time spent on CAMP so far: 1,856.12 sec.\n",
      "\tOn aln 460,000 in seq CAMP. Time spent on CAMP so far: 1,863.40 sec.\n",
      "\tOn aln 465,000 in seq CAMP. Time spent on CAMP so far: 1,870.68 sec.\n",
      "\tOn aln 470,000 in seq CAMP. Time spent on CAMP so far: 1,878.21 sec.\n",
      "\tOn aln 475,000 in seq CAMP. Time spent on CAMP so far: 1,885.51 sec.\n",
      "Done with edge_6104! Took 1,887.03 sec.\n",
      "======================================================================\n",
      "Identifying mutated positions (p = 1%) in genome BACT1...\n",
      "Found 22,144 mutated positions (p = 1%) in BACT1.\n",
      "Note that this tally is higher than you'd see in e.g. the CP1/2/3 plots, because now we're including both 'rare' and non-rare mutations. Just so you don't waste five minutes sanity-checking this like I did.\n",
      "Going through these positions...\n",
      "\tOn aln 5,000 in seq BACT1. Time spent on BACT1 so far: 53.80 sec.\n",
      "\tOn aln 10,000 in seq BACT1. Time spent on BACT1 so far: 134.35 sec.\n",
      "\tOn aln 15,000 in seq BACT1. Time spent on BACT1 so far: 236.05 sec.\n",
      "\tOn aln 20,000 in seq BACT1. Time spent on BACT1 so far: 316.00 sec.\n",
      "\tOn aln 25,000 in seq BACT1. Time spent on BACT1 so far: 428.53 sec.\n",
      "\tOn aln 30,000 in seq BACT1. Time spent on BACT1 so far: 538.50 sec.\n",
      "\tOn aln 35,000 in seq BACT1. Time spent on BACT1 so far: 634.59 sec.\n",
      "\tOn aln 40,000 in seq BACT1. Time spent on BACT1 so far: 709.70 sec.\n",
      "\tOn aln 45,000 in seq BACT1. Time spent on BACT1 so far: 822.25 sec.\n",
      "\tOn aln 50,000 in seq BACT1. Time spent on BACT1 so far: 904.35 sec.\n",
      "\tOn aln 55,000 in seq BACT1. Time spent on BACT1 so far: 985.77 sec.\n",
      "\tOn aln 60,000 in seq BACT1. Time spent on BACT1 so far: 1,100.14 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tOn aln 65,000 in seq BACT1. Time spent on BACT1 so far: 1,208.12 sec.\n",
      "\tOn aln 70,000 in seq BACT1. Time spent on BACT1 so far: 1,279.04 sec.\n",
      "\tOn aln 75,000 in seq BACT1. Time spent on BACT1 so far: 1,389.03 sec.\n",
      "\tOn aln 80,000 in seq BACT1. Time spent on BACT1 so far: 1,465.24 sec.\n",
      "\tOn aln 85,000 in seq BACT1. Time spent on BACT1 so far: 1,538.79 sec.\n",
      "\tOn aln 90,000 in seq BACT1. Time spent on BACT1 so far: 1,619.42 sec.\n",
      "\tOn aln 95,000 in seq BACT1. Time spent on BACT1 so far: 1,702.07 sec.\n",
      "\tOn aln 100,000 in seq BACT1. Time spent on BACT1 so far: 1,804.05 sec.\n",
      "\tOn aln 105,000 in seq BACT1. Time spent on BACT1 so far: 1,888.61 sec.\n",
      "\tOn aln 110,000 in seq BACT1. Time spent on BACT1 so far: 2,009.40 sec.\n",
      "\tOn aln 115,000 in seq BACT1. Time spent on BACT1 so far: 2,093.49 sec.\n",
      "\tOn aln 120,000 in seq BACT1. Time spent on BACT1 so far: 2,182.65 sec.\n",
      "\tOn aln 125,000 in seq BACT1. Time spent on BACT1 so far: 2,303.11 sec.\n",
      "\tOn aln 130,000 in seq BACT1. Time spent on BACT1 so far: 2,440.16 sec.\n",
      "\tOn aln 135,000 in seq BACT1. Time spent on BACT1 so far: 2,522.88 sec.\n",
      "\tOn aln 140,000 in seq BACT1. Time spent on BACT1 so far: 2,585.66 sec.\n",
      "\tOn aln 145,000 in seq BACT1. Time spent on BACT1 so far: 2,655.32 sec.\n",
      "\tOn aln 150,000 in seq BACT1. Time spent on BACT1 so far: 2,730.45 sec.\n",
      "\tOn aln 155,000 in seq BACT1. Time spent on BACT1 so far: 2,833.70 sec.\n",
      "\tOn aln 160,000 in seq BACT1. Time spent on BACT1 so far: 2,971.99 sec.\n",
      "\tOn aln 165,000 in seq BACT1. Time spent on BACT1 so far: 3,063.78 sec.\n",
      "\tOn aln 170,000 in seq BACT1. Time spent on BACT1 so far: 3,210.16 sec.\n",
      "\tOn aln 175,000 in seq BACT1. Time spent on BACT1 so far: 3,318.66 sec.\n",
      "\tOn aln 180,000 in seq BACT1. Time spent on BACT1 so far: 3,400.36 sec.\n",
      "\tOn aln 185,000 in seq BACT1. Time spent on BACT1 so far: 3,495.68 sec.\n",
      "\tOn aln 190,000 in seq BACT1. Time spent on BACT1 so far: 3,593.42 sec.\n",
      "\tOn aln 195,000 in seq BACT1. Time spent on BACT1 so far: 3,699.80 sec.\n",
      "\tOn aln 200,000 in seq BACT1. Time spent on BACT1 so far: 3,833.81 sec.\n",
      "\tOn aln 205,000 in seq BACT1. Time spent on BACT1 so far: 3,912.71 sec.\n",
      "\tOn aln 210,000 in seq BACT1. Time spent on BACT1 so far: 4,010.32 sec.\n",
      "\tOn aln 215,000 in seq BACT1. Time spent on BACT1 so far: 4,147.90 sec.\n",
      "\tOn aln 220,000 in seq BACT1. Time spent on BACT1 so far: 4,252.88 sec.\n",
      "\tOn aln 225,000 in seq BACT1. Time spent on BACT1 so far: 4,363.18 sec.\n",
      "\tOn aln 230,000 in seq BACT1. Time spent on BACT1 so far: 4,453.53 sec.\n",
      "\tOn aln 235,000 in seq BACT1. Time spent on BACT1 so far: 4,532.01 sec.\n",
      "\tOn aln 240,000 in seq BACT1. Time spent on BACT1 so far: 4,645.60 sec.\n",
      "\tOn aln 245,000 in seq BACT1. Time spent on BACT1 so far: 4,781.10 sec.\n",
      "\tOn aln 250,000 in seq BACT1. Time spent on BACT1 so far: 4,899.44 sec.\n",
      "\tOn aln 255,000 in seq BACT1. Time spent on BACT1 so far: 4,986.56 sec.\n",
      "\tOn aln 260,000 in seq BACT1. Time spent on BACT1 so far: 5,118.96 sec.\n",
      "For reference, there are 2,035 uncovered positions in BACT1.\n",
      "And there are 2 \"runs\" of uncovered positions.\n",
      "Adding \"virtual reads\" spanning each of these runs, at rounded mean coverage of 1,415x, to account for this...\n",
      "Wrote out 2,830 virtual reads.\n",
      "Done with edge_1671! Took 5,150.24 sec.\n",
      "======================================================================\n",
      "Identifying mutated positions (p = 1%) in genome BACT2...\n",
      "Found 372 mutated positions (p = 1%) in BACT2.\n",
      "Note that this tally is higher than you'd see in e.g. the CP1/2/3 plots, because now we're including both 'rare' and non-rare mutations. Just so you don't waste five minutes sanity-checking this like I did.\n",
      "Going through these positions...\n",
      "\tOn aln 5,000 in seq BACT2. Time spent on BACT2 so far: 22.39 sec.\n",
      "\tOn aln 10,000 in seq BACT2. Time spent on BACT2 so far: 51.60 sec.\n",
      "\tOn aln 15,000 in seq BACT2. Time spent on BACT2 so far: 79.71 sec.\n",
      "\tOn aln 20,000 in seq BACT2. Time spent on BACT2 so far: 109.16 sec.\n",
      "\tOn aln 25,000 in seq BACT2. Time spent on BACT2 so far: 140.02 sec.\n",
      "\tOn aln 30,000 in seq BACT2. Time spent on BACT2 so far: 168.37 sec.\n",
      "\tOn aln 35,000 in seq BACT2. Time spent on BACT2 so far: 194.91 sec.\n",
      "\tOn aln 40,000 in seq BACT2. Time spent on BACT2 so far: 221.92 sec.\n",
      "\tOn aln 45,000 in seq BACT2. Time spent on BACT2 so far: 247.74 sec.\n",
      "\tOn aln 50,000 in seq BACT2. Time spent on BACT2 so far: 275.24 sec.\n",
      "\tOn aln 55,000 in seq BACT2. Time spent on BACT2 so far: 302.11 sec.\n",
      "\tOn aln 60,000 in seq BACT2. Time spent on BACT2 so far: 327.53 sec.\n",
      "\tOn aln 65,000 in seq BACT2. Time spent on BACT2 so far: 352.64 sec.\n",
      "\tOn aln 70,000 in seq BACT2. Time spent on BACT2 so far: 379.01 sec.\n",
      "\tOn aln 75,000 in seq BACT2. Time spent on BACT2 so far: 405.69 sec.\n",
      "\tOn aln 80,000 in seq BACT2. Time spent on BACT2 so far: 431.38 sec.\n",
      "\tOn aln 85,000 in seq BACT2. Time spent on BACT2 so far: 457.00 sec.\n",
      "\tOn aln 90,000 in seq BACT2. Time spent on BACT2 so far: 487.66 sec.\n",
      "\tOn aln 95,000 in seq BACT2. Time spent on BACT2 so far: 525.17 sec.\n",
      "\tOn aln 100,000 in seq BACT2. Time spent on BACT2 so far: 551.20 sec.\n",
      "\tOn aln 105,000 in seq BACT2. Time spent on BACT2 so far: 579.55 sec.\n",
      "\tOn aln 110,000 in seq BACT2. Time spent on BACT2 so far: 607.35 sec.\n",
      "\tOn aln 115,000 in seq BACT2. Time spent on BACT2 so far: 634.57 sec.\n",
      "\tOn aln 120,000 in seq BACT2. Time spent on BACT2 so far: 659.75 sec.\n",
      "\tOn aln 125,000 in seq BACT2. Time spent on BACT2 so far: 685.34 sec.\n",
      "\tOn aln 130,000 in seq BACT2. Time spent on BACT2 so far: 717.45 sec.\n",
      "\tOn aln 135,000 in seq BACT2. Time spent on BACT2 so far: 741.26 sec.\n",
      "\tOn aln 140,000 in seq BACT2. Time spent on BACT2 so far: 765.71 sec.\n",
      "\tOn aln 145,000 in seq BACT2. Time spent on BACT2 so far: 788.74 sec.\n",
      "\tOn aln 150,000 in seq BACT2. Time spent on BACT2 so far: 811.88 sec.\n",
      "\tOn aln 155,000 in seq BACT2. Time spent on BACT2 so far: 834.59 sec.\n",
      "\tOn aln 160,000 in seq BACT2. Time spent on BACT2 so far: 857.78 sec.\n",
      "\tOn aln 165,000 in seq BACT2. Time spent on BACT2 so far: 881.01 sec.\n",
      "\tOn aln 170,000 in seq BACT2. Time spent on BACT2 so far: 903.49 sec.\n",
      "\tOn aln 175,000 in seq BACT2. Time spent on BACT2 so far: 926.50 sec.\n",
      "\tOn aln 180,000 in seq BACT2. Time spent on BACT2 so far: 948.60 sec.\n",
      "\tOn aln 185,000 in seq BACT2. Time spent on BACT2 so far: 969.79 sec.\n",
      "\tOn aln 190,000 in seq BACT2. Time spent on BACT2 so far: 990.99 sec.\n",
      "\tOn aln 195,000 in seq BACT2. Time spent on BACT2 so far: 1,013.85 sec.\n",
      "\tOn aln 200,000 in seq BACT2. Time spent on BACT2 so far: 1,036.17 sec.\n",
      "\tOn aln 205,000 in seq BACT2. Time spent on BACT2 so far: 1,057.28 sec.\n",
      "\tOn aln 210,000 in seq BACT2. Time spent on BACT2 so far: 1,077.93 sec.\n",
      "\tOn aln 215,000 in seq BACT2. Time spent on BACT2 so far: 1,098.53 sec.\n",
      "\tOn aln 220,000 in seq BACT2. Time spent on BACT2 so far: 1,118.65 sec.\n",
      "\tOn aln 225,000 in seq BACT2. Time spent on BACT2 so far: 1,140.15 sec.\n",
      "\tOn aln 230,000 in seq BACT2. Time spent on BACT2 so far: 1,162.04 sec.\n",
      "\tOn aln 235,000 in seq BACT2. Time spent on BACT2 so far: 1,182.72 sec.\n",
      "\tOn aln 240,000 in seq BACT2. Time spent on BACT2 so far: 1,203.09 sec.\n",
      "\tOn aln 245,000 in seq BACT2. Time spent on BACT2 so far: 1,223.59 sec.\n",
      "\tOn aln 250,000 in seq BACT2. Time spent on BACT2 so far: 1,244.63 sec.\n",
      "\tOn aln 255,000 in seq BACT2. Time spent on BACT2 so far: 1,265.15 sec.\n",
      "\tOn aln 260,000 in seq BACT2. Time spent on BACT2 so far: 1,287.10 sec.\n",
      "\tOn aln 265,000 in seq BACT2. Time spent on BACT2 so far: 1,306.70 sec.\n",
      "\tOn aln 270,000 in seq BACT2. Time spent on BACT2 so far: 1,327.75 sec.\n",
      "\tOn aln 275,000 in seq BACT2. Time spent on BACT2 so far: 1,347.42 sec.\n",
      "\tOn aln 280,000 in seq BACT2. Time spent on BACT2 so far: 1,367.43 sec.\n",
      "\tOn aln 285,000 in seq BACT2. Time spent on BACT2 so far: 1,387.70 sec.\n",
      "\tOn aln 290,000 in seq BACT2. Time spent on BACT2 so far: 1,408.75 sec.\n",
      "\tOn aln 295,000 in seq BACT2. Time spent on BACT2 so far: 1,429.35 sec.\n",
      "\tOn aln 300,000 in seq BACT2. Time spent on BACT2 so far: 1,449.35 sec.\n",
      "\tOn aln 305,000 in seq BACT2. Time spent on BACT2 so far: 1,470.40 sec.\n",
      "\tOn aln 310,000 in seq BACT2. Time spent on BACT2 so far: 1,490.72 sec.\n",
      "\tOn aln 315,000 in seq BACT2. Time spent on BACT2 so far: 1,511.49 sec.\n",
      "\tOn aln 320,000 in seq BACT2. Time spent on BACT2 so far: 1,532.42 sec.\n",
      "\tOn aln 325,000 in seq BACT2. Time spent on BACT2 so far: 1,553.59 sec.\n",
      "\tOn aln 330,000 in seq BACT2. Time spent on BACT2 so far: 1,574.97 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tOn aln 335,000 in seq BACT2. Time spent on BACT2 so far: 1,595.55 sec.\n",
      "\tOn aln 340,000 in seq BACT2. Time spent on BACT2 so far: 1,616.13 sec.\n",
      "\tOn aln 345,000 in seq BACT2. Time spent on BACT2 so far: 1,636.92 sec.\n",
      "\tOn aln 350,000 in seq BACT2. Time spent on BACT2 so far: 1,656.19 sec.\n",
      "\tOn aln 355,000 in seq BACT2. Time spent on BACT2 so far: 1,676.75 sec.\n",
      "\tOn aln 360,000 in seq BACT2. Time spent on BACT2 so far: 1,696.99 sec.\n",
      "\tOn aln 365,000 in seq BACT2. Time spent on BACT2 so far: 1,717.63 sec.\n",
      "\tOn aln 370,000 in seq BACT2. Time spent on BACT2 so far: 1,737.78 sec.\n",
      "\tOn aln 375,000 in seq BACT2. Time spent on BACT2 so far: 1,758.50 sec.\n",
      "\tOn aln 380,000 in seq BACT2. Time spent on BACT2 so far: 1,779.06 sec.\n",
      "\tOn aln 385,000 in seq BACT2. Time spent on BACT2 so far: 1,799.42 sec.\n",
      "\tOn aln 390,000 in seq BACT2. Time spent on BACT2 so far: 1,819.94 sec.\n",
      "\tOn aln 395,000 in seq BACT2. Time spent on BACT2 so far: 1,840.12 sec.\n",
      "\tOn aln 400,000 in seq BACT2. Time spent on BACT2 so far: 1,860.97 sec.\n",
      "\tOn aln 405,000 in seq BACT2. Time spent on BACT2 so far: 1,882.31 sec.\n",
      "\tOn aln 410,000 in seq BACT2. Time spent on BACT2 so far: 1,903.06 sec.\n",
      "\tOn aln 415,000 in seq BACT2. Time spent on BACT2 so far: 1,927.14 sec.\n",
      "\tOn aln 420,000 in seq BACT2. Time spent on BACT2 so far: 1,952.86 sec.\n",
      "\tOn aln 425,000 in seq BACT2. Time spent on BACT2 so far: 1,976.78 sec.\n",
      "\tOn aln 430,000 in seq BACT2. Time spent on BACT2 so far: 1,999.98 sec.\n",
      "\tOn aln 435,000 in seq BACT2. Time spent on BACT2 so far: 2,025.98 sec.\n",
      "\tOn aln 440,000 in seq BACT2. Time spent on BACT2 so far: 2,048.88 sec.\n",
      "\tOn aln 445,000 in seq BACT2. Time spent on BACT2 so far: 2,071.03 sec.\n",
      "\tOn aln 450,000 in seq BACT2. Time spent on BACT2 so far: 2,094.44 sec.\n",
      "\tOn aln 455,000 in seq BACT2. Time spent on BACT2 so far: 2,116.49 sec.\n",
      "\tOn aln 460,000 in seq BACT2. Time spent on BACT2 so far: 2,137.59 sec.\n",
      "\tOn aln 465,000 in seq BACT2. Time spent on BACT2 so far: 2,159.17 sec.\n",
      "\tOn aln 470,000 in seq BACT2. Time spent on BACT2 so far: 2,181.76 sec.\n",
      "\tOn aln 475,000 in seq BACT2. Time spent on BACT2 so far: 2,203.37 sec.\n",
      "\tOn aln 480,000 in seq BACT2. Time spent on BACT2 so far: 2,225.19 sec.\n",
      "\tOn aln 485,000 in seq BACT2. Time spent on BACT2 so far: 2,247.43 sec.\n",
      "\tOn aln 490,000 in seq BACT2. Time spent on BACT2 so far: 2,268.56 sec.\n",
      "\tOn aln 495,000 in seq BACT2. Time spent on BACT2 so far: 2,289.80 sec.\n",
      "\tOn aln 500,000 in seq BACT2. Time spent on BACT2 so far: 2,311.94 sec.\n",
      "\tOn aln 505,000 in seq BACT2. Time spent on BACT2 so far: 2,334.33 sec.\n",
      "\tOn aln 510,000 in seq BACT2. Time spent on BACT2 so far: 2,359.09 sec.\n",
      "\tOn aln 515,000 in seq BACT2. Time spent on BACT2 so far: 2,383.21 sec.\n",
      "\tOn aln 520,000 in seq BACT2. Time spent on BACT2 so far: 2,406.38 sec.\n",
      "\tOn aln 525,000 in seq BACT2. Time spent on BACT2 so far: 2,429.98 sec.\n",
      "\tOn aln 530,000 in seq BACT2. Time spent on BACT2 so far: 2,454.58 sec.\n",
      "\tOn aln 535,000 in seq BACT2. Time spent on BACT2 so far: 2,475.66 sec.\n",
      "\tOn aln 540,000 in seq BACT2. Time spent on BACT2 so far: 2,497.35 sec.\n",
      "\tOn aln 545,000 in seq BACT2. Time spent on BACT2 so far: 2,518.90 sec.\n",
      "\tOn aln 550,000 in seq BACT2. Time spent on BACT2 so far: 2,541.20 sec.\n",
      "\tOn aln 555,000 in seq BACT2. Time spent on BACT2 so far: 2,564.75 sec.\n",
      "\tOn aln 560,000 in seq BACT2. Time spent on BACT2 so far: 2,588.89 sec.\n",
      "\tOn aln 565,000 in seq BACT2. Time spent on BACT2 so far: 2,611.38 sec.\n",
      "\tOn aln 570,000 in seq BACT2. Time spent on BACT2 so far: 2,632.39 sec.\n",
      "\tOn aln 575,000 in seq BACT2. Time spent on BACT2 so far: 2,654.45 sec.\n",
      "\tOn aln 580,000 in seq BACT2. Time spent on BACT2 so far: 2,675.87 sec.\n",
      "\tOn aln 585,000 in seq BACT2. Time spent on BACT2 so far: 2,696.33 sec.\n",
      "\tOn aln 590,000 in seq BACT2. Time spent on BACT2 so far: 2,716.36 sec.\n",
      "\tOn aln 595,000 in seq BACT2. Time spent on BACT2 so far: 2,737.81 sec.\n",
      "\tOn aln 600,000 in seq BACT2. Time spent on BACT2 so far: 2,759.21 sec.\n",
      "\tOn aln 605,000 in seq BACT2. Time spent on BACT2 so far: 2,780.75 sec.\n",
      "\tOn aln 610,000 in seq BACT2. Time spent on BACT2 so far: 2,801.93 sec.\n",
      "\tOn aln 615,000 in seq BACT2. Time spent on BACT2 so far: 2,823.36 sec.\n",
      "\tOn aln 620,000 in seq BACT2. Time spent on BACT2 so far: 2,844.19 sec.\n",
      "\tOn aln 625,000 in seq BACT2. Time spent on BACT2 so far: 2,865.13 sec.\n",
      "\tOn aln 630,000 in seq BACT2. Time spent on BACT2 so far: 2,886.34 sec.\n",
      "\tOn aln 635,000 in seq BACT2. Time spent on BACT2 so far: 2,907.59 sec.\n",
      "\tOn aln 640,000 in seq BACT2. Time spent on BACT2 so far: 2,928.65 sec.\n",
      "\tOn aln 645,000 in seq BACT2. Time spent on BACT2 so far: 2,951.34 sec.\n",
      "\tOn aln 650,000 in seq BACT2. Time spent on BACT2 so far: 2,971.26 sec.\n",
      "\tOn aln 655,000 in seq BACT2. Time spent on BACT2 so far: 2,992.99 sec.\n",
      "\tOn aln 660,000 in seq BACT2. Time spent on BACT2 so far: 3,017.17 sec.\n",
      "\tOn aln 665,000 in seq BACT2. Time spent on BACT2 so far: 3,039.86 sec.\n",
      "\tOn aln 670,000 in seq BACT2. Time spent on BACT2 so far: 3,059.28 sec.\n",
      "\tOn aln 675,000 in seq BACT2. Time spent on BACT2 so far: 3,081.04 sec.\n",
      "\tOn aln 680,000 in seq BACT2. Time spent on BACT2 so far: 3,102.74 sec.\n",
      "\tOn aln 685,000 in seq BACT2. Time spent on BACT2 so far: 3,127.18 sec.\n",
      "\tOn aln 690,000 in seq BACT2. Time spent on BACT2 so far: 3,149.97 sec.\n",
      "\tOn aln 695,000 in seq BACT2. Time spent on BACT2 so far: 3,175.67 sec.\n",
      "\tOn aln 700,000 in seq BACT2. Time spent on BACT2 so far: 3,196.09 sec.\n",
      "\tOn aln 705,000 in seq BACT2. Time spent on BACT2 so far: 3,216.56 sec.\n",
      "\tOn aln 710,000 in seq BACT2. Time spent on BACT2 so far: 3,236.94 sec.\n",
      "\tOn aln 715,000 in seq BACT2. Time spent on BACT2 so far: 3,256.90 sec.\n",
      "\tOn aln 720,000 in seq BACT2. Time spent on BACT2 so far: 3,278.10 sec.\n",
      "\tOn aln 725,000 in seq BACT2. Time spent on BACT2 so far: 3,299.12 sec.\n",
      "\tOn aln 730,000 in seq BACT2. Time spent on BACT2 so far: 3,319.65 sec.\n",
      "\tOn aln 735,000 in seq BACT2. Time spent on BACT2 so far: 3,335.02 sec.\n",
      "\tOn aln 740,000 in seq BACT2. Time spent on BACT2 so far: 3,343.21 sec.\n",
      "Done with edge_2358! Took 3,345.87 sec.\n",
      "Time taken: 10,437.035605669022 sec.\n"
     ]
    }
   ],
   "source": [
    "bf = pysam.AlignmentFile(\"../main-workflow/output/fully-filtered-and-sorted-aln.bam\", \"rb\")\n",
    "output_dir = \"phasing-data/smoothed-reads/\"\n",
    "\n",
    "# verbose?\n",
    "no_indoor_voice = False\n",
    "\n",
    "def write_out_reads(filepath, readname2seq):\n",
    "    # Notably, this uses the \"a\" (append) method in order to add to the end of a file\n",
    "    with open(filepath, \"a\") as of:\n",
    "        for readname in readname2seq:\n",
    "            # Write out both the header and the sequence for each read\n",
    "            of.write(f\">{readname}\\n{str(readname2seq[readname])}\\n\")\n",
    "            \n",
    "ALN_UPDATE_FREQ = 5000\n",
    "ALN_BUFFER_FREQ = 1000\n",
    "VR_EXTRA_SPAN = 100\n",
    "\n",
    "P = 1\n",
    "            \n",
    "t1 = time.time()\n",
    "for seq in SEQS:\n",
    "    \n",
    "    # Record which positions (0-indexed) aren't covered by any smoothed reads in this MAG.\n",
    "    # We'll add \"virtual reads\" that span these positions.\n",
    "    uncovered_positions = set(range(0, seq2len[seq]))\n",
    "    \n",
    "    fasta = skbio.DNA.read(f\"../seqs/{seq}.fasta\")\n",
    "    \n",
    "    output_smoothed_reads_file = os.path.join(output_dir, f\"{seq}_smoothed_reads.fasta\")\n",
    "    \n",
    "    # Identify all (0-indexed, so compatible with skbio / pysam!)\n",
    "    # mutated positions in this genome up front to save time.\n",
    "    #\n",
    "    # Equivalently, we could also just take in an arbitrary VCF as input\n",
    "    # (e.g. one produced from another variant calling tool), although we'd\n",
    "    # need to be careful to only include SNVs and not indels/etc...\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Identifying mutated positions (p = {P}%) in genome {seq2name[seq]}...\")\n",
    "    mutpos2pileup = find_mutated_positions(seq, p_to_use=P, incl_pileup=True)\n",
    "    # We sort because the code below relies on these being in ascending order\n",
    "    mutated_positions = sorted(mutpos2pileup.keys())\n",
    "    print(f\"Found {len(mutated_positions):,} mutated positions (p = {P}%) in {seq2name[seq]}.\")\n",
    "    print(\n",
    "        f\"Note that this tally is higher than you'd see in e.g. the CP1/2/3 plots, because now we're including \"\n",
    "        \"both 'rare' and non-rare mutations. Just so you don't waste five minutes sanity-checking this like I did.\"\n",
    "    )\n",
    "    \n",
    "    print(\"Going through these positions...\")\n",
    "    \n",
    "    # Instead of just writing out every smoothed alignment as soon as we generate it, we build up a \"buffer\"\n",
    "    # of these alignments and then write a bunch out at once. This way we limit slowdown due to constantly\n",
    "    # having to open/close files. I don't really have a good source for this as best practice, but I remembered\n",
    "    # to do it while writing this code, so somewhere in College Park the CS faculty at Maryland are smiling\n",
    "    #\n",
    "    # Also fyi this maps read name to smoothed alignment (well, at this point, just read) sequence. The read name\n",
    "    # is useful to preserve in fasta files so we have some idea of provenance (where smoothed reads came from)\n",
    "    smoothed_aln_buffer = {}\n",
    "    \n",
    "    # The first time we see an alignment of a read, it's 1; if we see a supp aln of this read, it's 2; etc.\n",
    "    # Lets us distinguish alignments with different names\n",
    "    readname2freq_so_far = defaultdict(int)\n",
    "    \n",
    "    # Go through all linear alignments of each read to this genome, focusing (for now) on just the primary\n",
    "    # alignments...\n",
    "    ts1 = time.time()\n",
    "    for ai, aln in enumerate(bf.fetch(seq), 1):\n",
    "        \n",
    "        if ai % ALN_UPDATE_FREQ == 0:\n",
    "            print(\n",
    "                f\"\\tOn aln {ai:,} in seq {seq2name[seq]}. \"\n",
    "                f\"Time spent on {seq2name[seq]} so far: {time.time() - ts1:,.2f} sec.\"\n",
    "            )\n",
    "            \n",
    "        if aln.is_secondary:\n",
    "            raise ValueError(\n",
    "                \"Not to get political or anything, but you should've already filtered secondary alns out\"\n",
    "            )\n",
    "            \n",
    "        # Note that supplementary alignments are ok, though! We implicitly handle these here.\n",
    "        #\n",
    "        # Different alignments of the same read will have different new_readnames, because we're gonna\n",
    "        # be treating them as distinct \"reads\". We should have already filtered reference-overlapping\n",
    "        # supp alns so this shouldn't be a problem\n",
    "        \n",
    "        readname = aln.query_name\n",
    "        readname2freq_so_far[readname] += 1\n",
    "        new_readname = f\"{readname}_{readname2freq_so_far[readname]}\"\n",
    "        \n",
    "        # should never happen\n",
    "        if new_readname in smoothed_aln_buffer:\n",
    "            raise ValueError(\"This exact read alignment has already been smoothed? Weird.\")\n",
    "            \n",
    "        # Figure out where on the MAG this alignment \"hits.\" These are 0-indexed positions from Pysam.\n",
    "        # (reference_end points to the position after the actual final position, since these are designed to\n",
    "        # be interoperable with Python's half-open intervals.)\n",
    "        #\n",
    "        # Of course, there likely will be indels within this range: we're purposefully ignoring those here.\n",
    "        ref_start = aln.reference_start\n",
    "        ref_end = aln.reference_end - 1\n",
    "        \n",
    "        # This should never happen (TM)\n",
    "        if ref_start >= ref_end:\n",
    "            # Du sollst jetzt mit Gott sprechen.\n",
    "            raise ValueError(\n",
    "                f\"Ref start {ref_start:,} >= ref end {ref_end:,} for read {new_readname}?\"\n",
    "            )\n",
    "            \n",
    "        # Record which positions this read covers (of course, it may not exactly \"cover\" these positions\n",
    "        # originally due to indels, but the smoothed version will cover them).\n",
    "        uncovered_positions -= set(range(ref_start, ref_end + 1))\n",
    "        \n",
    "        # Smoothed sequence; we'll edit this so that if this read has (mis)matches to any called mutated\n",
    "        # positions, these positions are updated with the read's aligned nucleotides at these positions.\n",
    "        smoothed_aln_seq = fasta[ref_start: ref_end + 1]\n",
    "        \n",
    "        if actually_include_mutations_in_the_smoothed_reads:\n",
    "            # just for debugging: track the exact edits made to smoothed_aln_seq\n",
    "            replacements_made = {}\n",
    "\n",
    "            ap = aln.get_aligned_pairs(matches_only=True)\n",
    "\n",
    "            # Iterating through the aligned pairs is expensive. Since read lengths are generally in the thousands\n",
    "            # to tens of thousands of bp (which is much less than the > 1 million bp length of any bacterial genome),\n",
    "            # we set things up so that we only iterate through the aligned pairs once. We maintain an integer, mpi,\n",
    "            # that is a poor man's \"pointer\" to an index in mutated_positions.\n",
    "\n",
    "            mpi = 0\n",
    "\n",
    "            # Go through this aln's aligned pairs. As we see each pair, compare the pair's reference position\n",
    "            # (refpos) to the mpi-th mutated position (herein referred to as \"mutpos\").\n",
    "            #\n",
    "            # If refpos >  mutpos, increment mpi until refpos <= mutpos (stopping as early as possible).\n",
    "            # If refpos == mutpos, we have a match! Update readname2mutpos2ismutated[mutpos] based on\n",
    "            #                      comparing the read to the reference at the aligned positions.\n",
    "            # If refpos <  mutpos, continue to the next pair.\n",
    "\n",
    "            for pair in ap:\n",
    "\n",
    "                refpos = pair[1]\n",
    "                mutpos = mutated_positions[mpi]\n",
    "\n",
    "                no_mutations_to_right_of_here = False\n",
    "\n",
    "                # Increment mpi until we get to the next mutated position at or after the reference pos for this\n",
    "                # aligned pair (or until we run out of mutated positions).\n",
    "                while refpos > mutpos:\n",
    "                    mpi += 1\n",
    "                    if mpi < len(mutated_positions):\n",
    "                        mutpos = mutated_positions[mpi]\n",
    "                    else:\n",
    "                        no_mutations_to_right_of_here = True\n",
    "                        break\n",
    "\n",
    "                # I expect this should happen only for reads aligned near the right end of the genome.\n",
    "                if no_mutations_to_right_of_here:\n",
    "                    break\n",
    "\n",
    "                # If the next mutation occurs after this aligned pair, continue on to a later pair.\n",
    "                if refpos < mutpos:\n",
    "                    continue\n",
    "\n",
    "                # If we've made it here, refpos == mutpos!\n",
    "                # (...unless I messed something up in how I designed this code.)\n",
    "                if refpos != mutpos:\n",
    "                    raise ValueError(\"This should never happen!\")\n",
    "\n",
    "                # Finally, get the nucleotide aligned to this mutated position from this read.\n",
    "                readpos = pair[0]\n",
    "                read_nt = aln.query_sequence[readpos]\n",
    "\n",
    "                # If this read doesn't have match the first or second most common nucleotide at this position,\n",
    "                # \"smooth\" it to just have the most common nucleotide. In the future, when we perform read\n",
    "                # smoothing based on an arbitrary set of SNV calls, we can be more careful about this; but for now\n",
    "                # we make the simplifiying assumption that a mutation likely only has one alternate nucleotide,\n",
    "                # and that the 3rd and 4th most common nucleotides indicate errors.\n",
    "                # (Also, note that we break ties here arbitrarily.)\n",
    "                nt2ct = dict(zip(\"ACGT\", mutpos2pileup[mutpos][0]))\n",
    "                nt1 = max(nt2ct, key=nt2ct.get)\n",
    "                del nt2ct[nt1]\n",
    "                nt2 = max(nt2ct, key=nt2ct.get)\n",
    "                \n",
    "                if read_nt != nt1 and read_nt != nt2:\n",
    "                    if no_indoor_voice:\n",
    "                        print(\n",
    "                            f\"Read {new_readname} has 3rd or 4th most common nt at mutpos {mutpos + 1:,}: \"\n",
    "                            f\"pileup = {mutpos2pileup[mutpos]}, read = {read_nt}\"\n",
    "                        )\n",
    "                    read_nt = nt1\n",
    "                    \n",
    "                # Notably, the nucleotide at a mutated position in a smoothed read will always be the first\n",
    "                # or second most common nucleotide at this position. So \"unreasonable\" positions, in which\n",
    "                # the ref nt != the consensus nt, will not be treated as you might expect -- we ignore the\n",
    "                # reference in this particular case. Shouldn't make a big difference, since in most cases\n",
    "                # the ref and consensus nt agree.\n",
    "                relative_pos_on_aln = mutpos - ref_start\n",
    "                smoothed_aln_seq = smoothed_aln_seq.replace([relative_pos_on_aln], read_nt)\n",
    "                replacements_made[relative_pos_on_aln] = read_nt\n",
    "                if no_indoor_voice:\n",
    "                    print(\n",
    "                        f\"Read {new_readname} mismatches ref at mutpos {mutpos + 1:,}: \"\n",
    "                        f\"ref = {ref_nt}, read = {read_nt}\"\n",
    "                    )\n",
    "\n",
    "            if no_indoor_voice:\n",
    "                print(f\"Read {new_readname} required {len(replacements_made):,} replacements!\")\n",
    "        \n",
    "        # Now that we've finished processing all called mutations that this alignment spans, prepare it\n",
    "        # to be written out to a FASTA file. See comments above on smoothed_aln_buffer, and why we don't\n",
    "        # just write everything out as soon as it's ready.\n",
    "        #\n",
    "        # (Also, we've already guaranteed readname isn't already in smoothed_aln_buffer, so no need to worry\n",
    "        # about accidentally overwriting something from earlier.)\n",
    "        smoothed_aln_buffer[new_readname] = smoothed_aln_seq\n",
    "        \n",
    "        if ai % ALN_BUFFER_FREQ == 0:\n",
    "            write_out_reads(output_smoothed_reads_file, smoothed_aln_buffer)\n",
    "            # Clear the buffer\n",
    "            smoothed_aln_buffer = {}\n",
    "        \n",
    "    # We're probably going to have left over smoothed reads that we still haven't written out, unless things\n",
    "    # worked out so that on the final alignment we saw ai was exactly divisible by ALN_BUFFER_FREQ (and that's\n",
    "    # pretty unlikely unless you set the buffer freq to a low number). So make one last dump of the buffer.\n",
    "    if len(smoothed_aln_buffer) > 0:\n",
    "        write_out_reads(output_smoothed_reads_file, smoothed_aln_buffer)\n",
    "        \n",
    "    if len(uncovered_positions) > 0:\n",
    "        print(f\"For reference, there are {len(uncovered_positions):,} uncovered positions in {seq2name[seq]}.\")\n",
    "        \n",
    "        sup = sorted(uncovered_positions)\n",
    "        uc_runs = convert_to_runs(sup)\n",
    "        print(f'And there are {len(uc_runs)} \"runs\" of uncovered positions.')\n",
    "        \n",
    "        rounded_meancov = round(seq2meancov[seq])\n",
    "        print(\n",
    "            f'Adding \"virtual reads\" spanning each of these runs, at rounded mean coverage '\n",
    "            f'of {rounded_meancov:,}x, to account for this...'\n",
    "        )\n",
    "        \n",
    "        num_vr = 0\n",
    "        vr_buffer = {}\n",
    "        for run in uc_runs:\n",
    "            # Construct a virtual read that includes this entire run of uncovered positions as well\n",
    "            # as VR_EXTRA_SPAN positions before and after (clamping to the start/end of the seq if needed).\n",
    "            #\n",
    "            # Notably, we could try to make this loop around from end -> start if this is a cyclic MAG, but\n",
    "            # to remain consistent with how we handle supplementary alignments above -- and because implementing\n",
    "            # the loop around would be a lot of work and it's like 3am -- we ignore this for now.\n",
    "            #\n",
    "            # Also, note that run_start can equal run_end, if only a single isolated position is uncovered.\n",
    "            # This is fine -- the code handles this case automatically. (I guess the only potential problem is\n",
    "            # if the length of the MAG is less than VR_EXTRA_SPAN, but... that should never happen. If you have\n",
    "            # like 100bp-long MAGs that's a problem! I guess, TODO, make note of this when generalizing this\n",
    "            # code.)\n",
    "            \n",
    "            run_start = max(run[0] - VR_EXTRA_SPAN, 0)\n",
    "            run_end = min(run[1] + VR_EXTRA_SPAN, seq2len[seq] - 1)\n",
    "            \n",
    "            # Generate a sequence matching the \"reference\" MAG at these positions. We of course don't have\n",
    "            # any info about mutations here, because these positions are uncovered by the real reads!\n",
    "            vr_seq = fasta[run_start: run_end + 1]\n",
    "            \n",
    "            # We need to assign reads unique names, and including the run coordinates here is a nice way\n",
    "            # to preserve uniqueness across runs and also make our smoothed reads files easier to interpret\n",
    "            vr_name_prefix = f\"vr_{run[0]}_{run[1]}\"\n",
    "            \n",
    "            # Add M copies of this virtual read, where M = (rounded mean coverage of this MAG)\n",
    "            for vr_num in range(1, rounded_meancov + 1):\n",
    "                vr_name = f\"{vr_name_prefix}_{vr_num}\"\n",
    "                vr_buffer[vr_name] = vr_seq\n",
    "                num_vr += 1\n",
    "                \n",
    "        write_out_reads(output_smoothed_reads_file, vr_buffer)\n",
    "        print(f\"Wrote out {num_vr:,} virtual reads.\")\n",
    "    \n",
    "    print(f\"Done with {seq}! Took {time.time() - ts1:,.2f} sec.\")\n",
    "        \n",
    "print(f\"Time taken: {time.time() - t1:,} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Stats about smoothed read lengths\n",
    "\n",
    "We could have just figured this out while computing the stuff above, but ... I didn't have the foresight to think of this earlier, and I don't want to rerun that stuff for another >1 hour, so we just loop through the FASTA files we just generated quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T11:08:10.170008Z",
     "iopub.status.busy": "2022-02-07T11:08:10.169168Z",
     "iopub.status.idle": "2022-02-07T11:08:22.517122Z",
     "shell.execute_reply": "2022-02-07T11:08:22.516400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAMP: 476,356 smoothed reads\n",
      "\tmin / mean / median / max length = 200 / 11,264.7 / 11,154.0 / 35,099\n",
      "\tNum of reads with length ≥ 7,001: 438,334 / 476,356 (92.0%)\n",
      "BACT1: 265,975 smoothed reads\n",
      "\tmin / mean / median / max length = 202 / 11,487.2 / 11,473 / 39,723\n",
      "\tNum of reads with length ≥ 7,001: 243,480 / 265,975 (91.5%)\n",
      "BACT2: 742,926 smoothed reads\n",
      "\tmin / mean / median / max length = 200 / 11,318.0 / 11,259.0 / 40,346\n",
      "\tNum of reads with length ≥ 7,001: 673,220 / 742,926 (90.6%)\n"
     ]
    }
   ],
   "source": [
    "for seq in SEQS:\n",
    "    read_lengths = []\n",
    "    \n",
    "    # Parse a FASTA file -- I stole this code from myself in the Diversity Indices notebook\n",
    "    with open(f\"phasing-data/smoothed-reads/{seq}_smoothed_reads.fasta\", \"r\") as fastafile:\n",
    "\n",
    "        # Assumes that sequences are not split up over multiple lines (so a FASTA file with N sequences\n",
    "        # should have only 2N lines, maybe 2N + 1 if there's an extra empty newline at the bottom of the file)\n",
    "        for linenum, line in enumerate(fastafile):\n",
    "\n",
    "            if line.startswith(\">\"):\n",
    "                if linenum % 2 != 0:\n",
    "                    raise ValueError(\"something weird with > location in all_edges.fasta. Go yell at Marcus.\")\n",
    "            else:\n",
    "                if linenum % 2 != 1:\n",
    "                    raise ValueError(\"something weird with non > location in all_edges.fasta. Go yell at Marcus.\")\n",
    "\n",
    "                read_lengths.append(len(line.strip()))\n",
    "\n",
    "    num_reads = len(read_lengths)\n",
    "    minlen = min(read_lengths)\n",
    "    maxlen = max(read_lengths)\n",
    "    avglen = mean(read_lengths)\n",
    "    medlen = median(read_lengths)\n",
    "    \n",
    "    # Reads with length less than w + k = (threshold) will be ignored by jumboDB when constructing\n",
    "    # the graph, so we output stats about this to verify that we're not dropping a TON of reads\n",
    "    # (ofc ideally we wouldn't drop any tho...)\n",
    "    threshold = 7001\n",
    "    geq_threshold = len([rl for rl in read_lengths if rl >= threshold])\n",
    "    pct = 100 * (geq_threshold / num_reads)\n",
    "    \n",
    "    print(f\"{seq2name[seq]}: {num_reads:,} smoothed reads\")\n",
    "    print(f\"\\tmin / mean / median / max length = {minlen:,} / {avglen:,.1f} / {medlen:,} / {maxlen:,}\")\n",
    "    print(f\"\\tNum of reads with length \\u2265 {threshold:,}: {geq_threshold:,} / {num_reads:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct de Bruijn graph(s) from these smoothed reads\n",
    "\n",
    "Possible TODO: adjust this to detect the options above (which mutations are we using? did we actually include mutations in the smoothed reads?) and adjust where files get spit out to accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. jumboDBG\n",
    "\n",
    "#### 2.1.1. Run jumboDBG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:00 14.5Gb  INFO: Hello! You are running jumboDBG, a tool for construction of de Bruijn graphs for arbitrarily large values of k\n",
      "00:00:00 14.5Gb  INFO: Note that jumboDBG does not perform any error correction and ignores all reads shorter than k + w = 7001\n",
      "00:00:00 0Mb  INFO: Reading reads\n",
      "00:00:00 0Mb  INFO: Extracting minimizers\n",
      "00:00:28 3.3Gb  INFO: Finished read processing\n",
      "00:00:28 3.3Gb  INFO: 2541151 hashs collected. Starting sorting.\n",
      "00:00:28 3.4Gb  INFO: Finished sorting. Total distinct minimizers: 1695\n",
      "00:00:28 3.4Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:28 3.4Gb  INFO: Vertex map constructed.\n",
      "00:00:28 3.4Gb  INFO: Filling edge sequences.\n",
      "00:00:55 4.5Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:00:55 4.5Gb  INFO:  Collecting tips \n",
      "00:00:55 4.5Gb  INFO: Added 58 artificial minimizers from tips.\n",
      "00:00:55 4.5Gb  INFO: Collected 3498 old edges.\n",
      "00:00:55 4.5Gb  INFO: New minimizers added to sparse graph.\n",
      "00:00:55 4.5Gb  INFO: Refilling graph with old edges.\n",
      "00:00:55 4.5Gb  INFO: Filling graph with new edges.\n",
      "00:00:55 4.5Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:00:55 4.5Gb  INFO: Starting to extract disjointigs.\n",
      "00:00:55 4.5Gb  INFO: Finished extracting 179 disjointigs of total size 2588827\n",
      "00:00:56 14.5Gb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_6104_jumbo_20220221/disjointigs.fasta\"\n",
      "00:00:56 14.5Gb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:00:56 14.5Gb  INFO: Filled 7840161 bits out of 54196736\n",
      "00:00:56 14.5Gb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:00:56 14.5Gb  INFO: Collected 846 junctions.\n",
      "00:00:56 14.5Gb  INFO: Starting DBG construction.\n",
      "00:00:56 14.5Gb  INFO: Vertices created.\n",
      "00:00:57 14.5Gb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:00:57 14.5Gb  INFO: Added 0 hanging vertices\n",
      "00:00:57 14.5Gb  INFO: Merging unbranching paths\n",
      "00:00:57 14.5Gb  INFO: Ended merging edges. Resulting size 222\n",
      "00:00:57 14.5Gb  INFO: Calculating edge coverage.\n",
      "00:00:57 14.5Gb  INFO: Starting to fill edge coverages\n",
      "00:01:15 14.5Gb  INFO: Edge coverage calculated.\n",
      "00:01:15 14.5Gb  INFO: Printing graph to fasta file \"phasing-data/smoothed-reads/edge_6104_jumbo_20220221/graph.fasta\"\n",
      "00:01:15 14.5Gb  INFO: Printing graph to gfa file \"phasing-data/smoothed-reads/edge_6104_jumbo_20220221/graph.gfa\"\n",
      "00:01:15 14.5Gb  INFO: Printing graph to dot file \"phasing-data/smoothed-reads/edge_6104_jumbo_20220221/graph.dot\"\n",
      "00:01:15 14.5Gb  INFO: DBG construction finished\n",
      "00:01:15 14.5Gb  INFO: Please cite our paper if you use jumboDBG in your research: https://www.biorxiv.org/content/10.1101/2020.12.10.420448\n",
      "00:00:00 14.5Gb  INFO: Hello! You are running jumboDBG, a tool for construction of de Bruijn graphs for arbitrarily large values of k\n",
      "00:00:00 14.5Gb  INFO: Note that jumboDBG does not perform any error correction and ignores all reads shorter than k + w = 7001\n",
      "00:00:00 0Mb  INFO: Reading reads\n",
      "00:00:00 0Mb  INFO: Extracting minimizers\n",
      "00:00:16 3.1Gb  INFO: Finished read processing\n",
      "00:00:16 3.1Gb  INFO: 1489880 hashs collected. Starting sorting.\n",
      "00:00:16 3.1Gb  INFO: Finished sorting. Total distinct minimizers: 26714\n",
      "00:00:17 3.1Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:17 3.1Gb  INFO: Vertex map constructed.\n",
      "00:00:17 3.1Gb  INFO: Filling edge sequences.\n",
      "00:00:36 3.2Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:00:36 3.2Gb  INFO:  Collecting tips \n",
      "00:00:36 3.2Gb  INFO: Added 6025 artificial minimizers from tips.\n",
      "00:00:36 3.2Gb  INFO: Collected 54446 old edges.\n",
      "00:00:36 3.2Gb  INFO: New minimizers added to sparse graph.\n",
      "00:00:36 3.2Gb  INFO: Refilling graph with old edges.\n",
      "00:00:37 3.2Gb  INFO: Filling graph with new edges.\n",
      "00:00:38 3.2Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:00:38 3.2Gb  INFO: Starting to extract disjointigs.\n",
      "00:00:38 3.2Gb  INFO: Finished extracting 11230 disjointigs of total size 84898603\n",
      "00:00:39 14.5Gb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_1671_jumbo_20220221/disjointigs.fasta\"\n",
      "00:00:40 14.5Gb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:00:43 14.5Gb  INFO: Filled 132326722 bits out of 919595936\n",
      "00:00:43 14.5Gb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:00:45 14.5Gb  INFO: Collected 23795 junctions.\n",
      "00:00:45 14.5Gb  INFO: Starting DBG construction.\n",
      "00:00:45 14.5Gb  INFO: Vertices created.\n",
      "00:00:46 14.5Gb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:00:46 14.5Gb  INFO: Added 3 hanging vertices\n",
      "00:00:46 14.5Gb  INFO: Merging unbranching paths\n",
      "00:00:46 14.5Gb  INFO: Ended merging edges. Resulting size 13158\n",
      "00:00:46 14.5Gb  INFO: Calculating edge coverage.\n",
      "00:00:46 14.5Gb  INFO: Starting to fill edge coverages\n",
      "00:00:52 14.5Gb  INFO: Edge coverage calculated.\n",
      "00:00:52 14.5Gb  INFO: Printing graph to fasta file \"phasing-data/smoothed-reads/edge_1671_jumbo_20220221/graph.fasta\"\n",
      "00:00:54 14.5Gb  INFO: Printing graph to gfa file \"phasing-data/smoothed-reads/edge_1671_jumbo_20220221/graph.gfa\"\n",
      "00:00:55 14.5Gb  INFO: Printing graph to dot file \"phasing-data/smoothed-reads/edge_1671_jumbo_20220221/graph.dot\"\n",
      "00:00:55 14.5Gb  INFO: DBG construction finished\n",
      "00:00:55 14.5Gb  INFO: Please cite our paper if you use jumboDBG in your research: https://www.biorxiv.org/content/10.1101/2020.12.10.420448\n",
      "00:00:00 14.5Gb  INFO: Hello! You are running jumboDBG, a tool for construction of de Bruijn graphs for arbitrarily large values of k\n",
      "00:00:00 14.5Gb  INFO: Note that jumboDBG does not perform any error correction and ignores all reads shorter than k + w = 7001\n",
      "00:00:00 0Mb  INFO: Reading reads\n",
      "00:00:00 0Mb  INFO: Extracting minimizers\n",
      "00:00:45 4.4Gb  INFO: Finished read processing\n",
      "00:00:45 4.4Gb  INFO: 4059344 hashs collected. Starting sorting.\n",
      "00:00:45 4.4Gb  INFO: Finished sorting. Total distinct minimizers: 4653\n",
      "00:00:46 4.4Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:46 4.4Gb  INFO: Vertex map constructed.\n",
      "00:00:46 4.4Gb  INFO: Filling edge sequences.\n",
      "00:01:27 8.6Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:01:27 8.6Gb  INFO:  Collecting tips \n",
      "00:01:27 8.6Gb  INFO: Added 88 artificial minimizers from tips.\n",
      "00:01:27 8.6Gb  INFO: Collected 9962 old edges.\n",
      "00:01:27 8.6Gb  INFO: New minimizers added to sparse graph.\n",
      "00:01:27 8.6Gb  INFO: Refilling graph with old edges.\n",
      "00:01:27 8.6Gb  INFO: Filling graph with new edges.\n",
      "00:01:27 8.6Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:01:27 8.6Gb  INFO: Starting to extract disjointigs.\n",
      "00:01:28 8.6Gb  INFO: Finished extracting 905 disjointigs of total size 9102264\n",
      "00:01:28 14.5Gb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_2358_jumbo_20220221/disjointigs.fasta\"\n",
      "00:01:28 14.5Gb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:01:29 14.5Gb  INFO: Filled 21163651 bits out of 146443488\n",
      "00:01:29 14.5Gb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:01:29 14.5Gb  INFO: Collected 2598 junctions.\n",
      "00:01:29 14.5Gb  INFO: Starting DBG construction.\n",
      "00:01:29 14.5Gb  INFO: Vertices created.\n",
      "00:01:29 14.5Gb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:01:29 14.5Gb  INFO: Added 0 hanging vertices\n",
      "00:01:29 14.5Gb  INFO: Merging unbranching paths\n",
      "00:01:29 14.5Gb  INFO: Ended merging edges. Resulting size 818\n",
      "00:01:30 14.5Gb  INFO: Calculating edge coverage.\n",
      "00:01:30 14.5Gb  INFO: Starting to fill edge coverages\n",
      "00:01:51 14.5Gb  INFO: Edge coverage calculated.\n",
      "00:01:51 14.5Gb  INFO: Printing graph to fasta file \"phasing-data/smoothed-reads/edge_2358_jumbo_20220221/graph.fasta\"\n",
      "00:01:51 14.5Gb  INFO: Printing graph to gfa file \"phasing-data/smoothed-reads/edge_2358_jumbo_20220221/graph.gfa\"\n",
      "00:01:51 14.5Gb  INFO: Printing graph to dot file \"phasing-data/smoothed-reads/edge_2358_jumbo_20220221/graph.dot\"\n",
      "00:01:51 14.5Gb  INFO: DBG construction finished\n",
      "00:01:51 14.5Gb  INFO: Please cite our paper if you use jumboDBG in your research: https://www.biorxiv.org/content/10.1101/2020.12.10.420448\n"
     ]
    }
   ],
   "source": [
    "!/home/mfedarko/software/LJA/bin/jumboDBG \\\n",
    "    --reads phasing-data/smoothed-reads/edge_6104_smoothed_reads.fasta \\\n",
    "    -k 5001 \\\n",
    "    --coverage \\\n",
    "    --output-dir phasing-data/smoothed-reads/edge_6104_jumbo_20220221\n",
    "\n",
    "!/home/mfedarko/software/LJA/bin/jumboDBG \\\n",
    "    --reads phasing-data/smoothed-reads/edge_1671_smoothed_reads.fasta \\\n",
    "    -k 5001 \\\n",
    "    --coverage \\\n",
    "    --output-dir phasing-data/smoothed-reads/edge_1671_jumbo_20220221\n",
    "\n",
    "!/home/mfedarko/software/LJA/bin/jumboDBG \\\n",
    "    --reads phasing-data/smoothed-reads/edge_2358_smoothed_reads.fasta \\\n",
    "    -k 5001 \\\n",
    "    --coverage \\\n",
    "    --output-dir phasing-data/smoothed-reads/edge_2358_jumbo_20220221"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Filter jumboDBG graphs to remove low-coverage edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yanked and slightly modified from main-workflow/utils.py\n",
    "import networkx as nx\n",
    "\n",
    "def load_gfa(filepath):\n",
    "    \n",
    "    # We ignore directionality for right now.\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    with open(filepath, \"r\") as gfafile:\n",
    "        for line in gfafile:\n",
    "            if line[0] == \"S\":\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                node_name = parts[1]\n",
    "                node_len = len(parts[2])\n",
    "                node_cov = None\n",
    "\n",
    "                # Parse GFA tags\n",
    "                extra_data = parts[3:]\n",
    "                for tag in extra_data:\n",
    "                    # This is overcautious, probably, but whatevs\n",
    "                    if tag.startswith(\"LN:i:\"):\n",
    "                        raise ValueError(\"Duplicate length for node {}\".format(node_name))\n",
    "                    elif tag.startswith(\"dp:i:\") or tag.startswith(\"KC:i:\"):\n",
    "                        if node_cov is None:\n",
    "                            node_cov = int(tag[5:])\n",
    "                        else:\n",
    "                            raise ValueError(\"Duplicate coverage for node {}\".format(node_name))\n",
    "\n",
    "                if node_cov is None:\n",
    "                    raise ValueError(\"No coverage tag given for node {}\".format(node_name))\n",
    "\n",
    "                graph.add_node(node_name, length=node_len, cov=node_cov)\n",
    "\n",
    "            elif line[0] == \"L\":\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                src = parts[1]\n",
    "                snk = parts[3]\n",
    "                src_orient = parts[2]\n",
    "                snk_orient = parts[4]\n",
    "                graph.add_edge(src, snk, src_orient=src_orient, snk_orient=snk_orient)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAMP: Removed 1 node(s) with coverage < 5x.\n",
      "BACT1: Removed 14 node(s) with coverage < 5x.\n",
      "BACT2: Removed 4 node(s) with coverage < 5x.\n"
     ]
    }
   ],
   "source": [
    "MIN_EDGE_COV = 5\n",
    "for seq in SEQS:\n",
    "    phdir = f\"phasing-data/smoothed-reads/{seq}_jumbo_20220221\"\n",
    "    g = load_gfa(os.path.join(phdir, \"graph.gfa\"))\n",
    "    removed_nodes = []\n",
    "    for n in list(g.nodes):\n",
    "        if g.nodes[n][\"cov\"] < MIN_EDGE_COV:\n",
    "            g.remove_node(n)\n",
    "            removed_nodes.append(n)\n",
    "    print(f\"{seq2name[seq]}: Removed {len(removed_nodes):,} node(s) with coverage < {MIN_EDGE_COV}x.\")\n",
    "\n",
    "    # Probs possible to merge this together with the filtering step above so we only loop over the nodes once\n",
    "    with open(os.path.join(phdir, \"graph-covfilt.gfa\"), \"w\") as f:\n",
    "        f.write(\"H\\tVN:Z:1.0\\n\")\n",
    "        for n in g.nodes:\n",
    "            f.write(f\"S\\t{n}\\t*\\tKC:i:{g.nodes[n]['cov']}\\tLN:i:{g.nodes[n]['length']}\\n\")\n",
    "        for e in g.edges:\n",
    "            src_or = g.edges[e][\"src_orient\"]\n",
    "            snk_or = g.edges[e][\"snk_orient\"]\n",
    "            f.write(f\"L\\t{e[0]}\\t{src_or}\\t{e[1]}\\t{snk_or}\\t*\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. LJA (including jumboDBG and multiplexDBG)\n",
    "\n",
    "`--noec` is an undocumented flag that avoids the error correction step in LJA, which is what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T11:08:22.524979Z",
     "iopub.status.busy": "2022-02-07T11:08:22.524164Z",
     "iopub.status.idle": "2022-02-07T11:17:31.130849Z",
     "shell.execute_reply": "2022-02-07T11:17:31.131477Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:00 14.5Gb  INFO: Hello! You are running La Jolla Assembler (LJA), a tool for genome assembly from PacBio HiFi reads\n",
      "00:00:00 14.5Gb  INFO: 02ec23dd81b81ce8718a70fb64ea6fab0d88c5ca\n",
      "00:00:00 14.5Gb  INFO: LJA pipeline started\n",
      "00:00:00 14.5Gb  INFO: Performing initial correction with k = 5001\n",
      "00:00:00 0Mb  INFO: Reading reads\n",
      "00:00:00 0Mb  INFO: Extracting minimizers\n",
      "00:00:17 4.3Gb  INFO: Finished read processing\n",
      "00:00:17 4.3Gb  INFO: 5257129 hashs collected. Starting sorting.\n",
      "00:00:17 4.3Gb  INFO: Finished sorting. Total distinct minimizers: 4475\n",
      "00:00:17 4.3Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:17 4.3Gb  INFO: Vertex map constructed.\n",
      "00:00:17 4.3Gb  INFO: Filling edge sequences.\n",
      "00:00:34 4.3Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:00:34 4.3Gb  INFO:  Collecting tips \n",
      "00:00:34 4.3Gb  INFO: Added 60 artificial minimizers from tips.\n",
      "00:00:34 4.3Gb  INFO: Collected 8998 old edges.\n",
      "00:00:34 4.3Gb  INFO: New minimizers added to sparse graph.\n",
      "00:00:34 4.3Gb  INFO: Refilling graph with old edges.\n",
      "00:00:34 4.3Gb  INFO: Filling graph with new edges.\n",
      "00:00:34 4.3Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:00:34 4.3Gb  INFO: Starting to extract disjointigs.\n",
      "00:00:34 4.3Gb  INFO: Finished extracting 150 disjointigs of total size 1886270\n",
      "00:00:34 0Mb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_6104_lja_noec/k5001/disjointigs.fasta\"\n",
      "00:00:34 8Mb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:00:35 8Mb  INFO: Filled 5259018 bits out of 36355840\n",
      "00:00:35 8Mb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:00:35 8Mb  INFO: Collected 575 junctions.\n",
      "00:00:35 8Mb  INFO: Starting DBG construction.\n",
      "00:00:35 8Mb  INFO: Vertices created.\n",
      "00:00:35 8Mb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:00:35 8Mb  INFO: Added 1 hanging vertices\n",
      "00:00:35 8Mb  INFO: Merging unbranching paths\n",
      "00:00:35 8Mb  INFO: Ended merging edges. Resulting size 168\n",
      "00:00:35 8Mb  INFO: Cleaning edge coverages\n",
      "00:00:35 8Mb  INFO: Collecting alignments of sequences to the graph\n",
      "00:00:49 5.6Gb  INFO: Alignment collection finished. Total length of alignments is 499074\n",
      "00:00:51 5.6Gb  INFO: Printing reads to fasta file \"phasing-data/smoothed-reads/edge_6104_lja_noec/k5001/corrected_reads.fasta\"\n",
      "00:01:22 14.5Gb  INFO: Performing repeat resolution by transforming de Bruijn graph into Multiplex de Bruijn graph\n",
      "00:01:22 0Mb  INFO: Loading graph from fasta\n",
      "00:01:22 9Mb  INFO: Finished loading graph\n",
      "00:01:23 158Mb  INFO: Looking for unique edges\n",
      "00:01:23 158Mb  INFO: Marked 18 long edges as unique\n",
      "00:01:23 158Mb  INFO: Marking extra edges as unique based on read paths\n",
      "00:01:23 158Mb  INFO: Marked 18 edges as unique\n",
      "00:01:23 158Mb  INFO: Splitting graph with unique edges\n",
      "00:01:23 158Mb  INFO: Processing 14 components\n",
      "00:01:23 158Mb  INFO: Finished unique edges search. Found 32 unique edges\n",
      "00:01:23 158Mb  INFO: Analysing repeats of multiplicity 2 and looking for additional unique edges\n",
      "00:01:23 158Mb  INFO: Finished processing of repeats of multiplicity 2. Found 0 erroneous edges.\n",
      "00:01:23 158Mb  INFO: Resolving repeats\n",
      "00:01:23 158Mb  INFO: Constructing paths\n",
      "00:01:27 323Mb  INFO: Building graph\n",
      "00:01:27 323Mb  INFO: Increasing k\n",
      "00:01:35 323Mb  INFO: Finished increasing k\n",
      "00:01:35 323Mb  INFO: Exporting remaining active transitions\n",
      "00:01:35 323Mb  INFO: Export to Dot\n",
      "00:01:35 323Mb  INFO: Export to GFA and compressed contigs\n",
      "00:01:35 329Mb  INFO: Finished repeat resolution\n",
      "00:01:35 14.5Gb  INFO: Performing polishing and homopolymer uncompression\n",
      "00:01:35 9Mb  INFO: Aligning reads back to assembly\n",
      "00:02:00 4.8Gb  INFO: Finished alignment.\n",
      "00:02:00 4.8Gb  INFO: Printing alignments to \"phasing-data/smoothed-reads/edge_6104_lja_noec/uncompressing/alignments.txt\"\n",
      "00:02:02 4.8Gb  INFO: Reading and processing initial reads from [\"phasing-data/smoothed-reads/edge_6104_smoothed_reads.fasta\"]\n",
      "00:09:10 4.8Gb  INFO: Uncompressing homopolymers in contigs\n",
      "00:09:11 4.8Gb  INFO: Total zero covered nucleotides 659\n",
      "00:09:11 4.8Gb  INFO: Calculating overlaps between adjacent uncompressed edges\n",
      "00:09:11 4.8Gb  INFO: Printing final gfa file to \"phasing-data/smoothed-reads/edge_6104_lja_noec/mdbg.gfa\"\n",
      "00:09:11 4.8Gb  INFO: Printing final assembly to \"phasing-data/smoothed-reads/edge_6104_lja_noec/assembly.fasta\"\n",
      "00:09:11 14.5Gb  INFO: Final homopolymer compressed and corrected reads can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_noec/k5001/corrected_reads.fasta\"\n",
      "00:09:11 14.5Gb  INFO: Final graph with homopolymer compressed edges can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_noec/mdbg/mdbg.hpc.gfa\"\n",
      "00:09:11 14.5Gb  INFO: Final graph can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_noec/mdbg.gfa\"\n",
      "00:09:11 14.5Gb  INFO: Final assembly can be found here: \"phasing-data/smoothed-reads/edge_6104_lja_noec/assembly.fasta\"\n",
      "00:09:11 14.5Gb  INFO: LJA pipeline finished\n",
      "00:00:00 14.5Gb  INFO: Hello! You are running La Jolla Assembler (LJA), a tool for genome assembly from PacBio HiFi reads\n",
      "00:00:00 14.5Gb  INFO: 02ec23dd81b81ce8718a70fb64ea6fab0d88c5ca\n",
      "00:00:00 14.5Gb  INFO: LJA pipeline started\n",
      "00:00:00 14.5Gb  INFO: Performing initial correction with k = 5001\n",
      "00:00:00 0Mb  INFO: Reading reads\n",
      "00:00:00 0Mb  INFO: Extracting minimizers\n",
      "00:00:10 3.1Gb  INFO: Finished read processing\n",
      "00:00:10 3.1Gb  INFO: 3259426 hashs collected. Starting sorting.\n",
      "00:00:11 3.1Gb  INFO: Finished sorting. Total distinct minimizers: 66366\n",
      "00:00:11 3.1Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:11 3.1Gb  INFO: Vertex map constructed.\n",
      "00:00:11 3.1Gb  INFO: Filling edge sequences.\n",
      "00:00:22 3.1Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:00:22 3.1Gb  INFO:  Collecting tips \n",
      "00:00:22 3.1Gb  INFO: Added 6140 artificial minimizers from tips.\n",
      "00:00:22 3.1Gb  INFO: Collected 130682 old edges.\n",
      "00:00:22 3.1Gb  INFO: New minimizers added to sparse graph.\n",
      "00:00:22 3.1Gb  INFO: Refilling graph with old edges.\n",
      "00:00:23 3.1Gb  INFO: Filling graph with new edges.\n",
      "00:00:23 3.1Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:00:23 3.1Gb  INFO: Starting to extract disjointigs.\n",
      "00:00:23 3.1Gb  INFO: Finished extracting 8690 disjointigs of total size 60720845\n",
      "00:00:24 0Mb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_1671_lja_noec/k5001/disjointigs.fasta\"\n",
      "00:00:25 87Mb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:00:28 87Mb  INFO: Filled 79862907 bits out of 552388960\n",
      "00:00:28 87Mb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:00:29 88Mb  INFO: Collected 16889 junctions.\n",
      "00:00:29 88Mb  INFO: Starting DBG construction.\n",
      "00:00:29 88Mb  INFO: Vertices created.\n",
      "00:00:30 88Mb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:00:30 88Mb  INFO: Added 5 hanging vertices\n",
      "00:00:30 88Mb  INFO: Merging unbranching paths\n",
      "00:00:30 88Mb  INFO: Ended merging edges. Resulting size 10274\n",
      "00:00:30 88Mb  INFO: Cleaning edge coverages\n",
      "00:00:30 88Mb  INFO: Collecting alignments of sequences to the graph\n",
      "00:00:36 3.2Gb  INFO: Alignment collection finished. Total length of alignments is 954076\n",
      "00:00:39 3.2Gb  INFO: Printing reads to fasta file \"phasing-data/smoothed-reads/edge_1671_lja_noec/k5001/corrected_reads.fasta\"\n",
      "00:00:58 14.5Gb  INFO: Performing repeat resolution by transforming de Bruijn graph into Multiplex de Bruijn graph\n",
      "00:00:58 0Mb  INFO: Loading graph from fasta\n",
      "00:00:59 195Mb  INFO: Finished loading graph\n",
      "00:01:00 321Mb  INFO: Looking for unique edges\n",
      "00:01:00 321Mb  INFO: Marked 0 long edges as unique\n",
      "00:01:00 321Mb  INFO: Marking extra edges as unique based on read paths\n",
      "00:01:00 321Mb  INFO: Marked 0 edges as unique\n",
      "00:01:00 321Mb  INFO: Splitting graph with unique edges\n",
      "00:01:00 321Mb  INFO: Processing 1158 components\n",
      "00:01:07 321Mb  INFO: Finished unique edges search. Found 0 unique edges\n",
      "00:01:07 321Mb  INFO: Analysing repeats of multiplicity 2 and looking for additional unique edges\n",
      "00:01:07 321Mb  INFO: Finished processing of repeats of multiplicity 2. Found 0 erroneous edges.\n",
      "00:01:07 321Mb  INFO: Resolving repeats\n",
      "00:01:07 321Mb  INFO: Constructing paths\n",
      "00:01:14 0.5Gb  INFO: Building graph\n",
      "00:01:15 0.5Gb  INFO: Increasing k\n",
      "00:04:23 0.5Gb  INFO: Finished increasing k\n",
      "00:04:23 0.5Gb  INFO: Exporting remaining active transitions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:04:23 0.5Gb  INFO: Export to Dot\n",
      "00:04:23 0.5Gb  INFO: Export to GFA and compressed contigs\n",
      "00:04:28 0.5Gb  INFO: Finished repeat resolution\n",
      "00:04:29 14.5Gb  INFO: Performing polishing and homopolymer uncompression\n",
      "00:04:31 51Mb  INFO: Aligning reads back to assembly\n",
      "00:04:52 3.4Gb  INFO: Finished alignment.\n",
      "00:04:52 3.4Gb  INFO: Printing alignments to \"phasing-data/smoothed-reads/edge_1671_lja_noec/uncompressing/alignments.txt\"\n",
      "00:04:56 4.6Gb  INFO: Reading and processing initial reads from [\"phasing-data/smoothed-reads/edge_1671_smoothed_reads.fasta\"]\n",
      "00:09:07 5.8Gb  INFO: Uncompressing homopolymers in contigs\n",
      "00:09:13 5.8Gb  INFO: Total zero covered nucleotides 62090\n",
      "00:09:13 5.8Gb  INFO: Calculating overlaps between adjacent uncompressed edges\n",
      "00:09:16 5.8Gb  INFO: Printing final gfa file to \"phasing-data/smoothed-reads/edge_1671_lja_noec/mdbg.gfa\"\n",
      "00:09:17 5.8Gb  INFO: Printing final assembly to \"phasing-data/smoothed-reads/edge_1671_lja_noec/assembly.fasta\"\n",
      "00:09:17 14.5Gb  INFO: Final homopolymer compressed and corrected reads can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_noec/k5001/corrected_reads.fasta\"\n",
      "00:09:17 14.5Gb  INFO: Final graph with homopolymer compressed edges can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_noec/mdbg/mdbg.hpc.gfa\"\n",
      "00:09:17 14.5Gb  INFO: Final graph can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_noec/mdbg.gfa\"\n",
      "00:09:17 14.5Gb  INFO: Final assembly can be found here: \"phasing-data/smoothed-reads/edge_1671_lja_noec/assembly.fasta\"\n",
      "00:09:17 14.5Gb  INFO: LJA pipeline finished\n",
      "00:00:00 14.5Gb  INFO: Hello! You are running La Jolla Assembler (LJA), a tool for genome assembly from PacBio HiFi reads\n",
      "00:00:00 14.5Gb  INFO: 02ec23dd81b81ce8718a70fb64ea6fab0d88c5ca\n",
      "00:00:00 14.5Gb  INFO: LJA pipeline started\n",
      "00:00:00 14.5Gb  INFO: Performing initial correction with k = 5001\n",
      "00:00:00 0Mb  INFO: Reading reads\n",
      "00:00:00 0Mb  INFO: Extracting minimizers\n",
      "00:00:30 5.4Gb  INFO: Finished read processing\n",
      "00:00:30 5.4Gb  INFO: 9394310 hashs collected. Starting sorting.\n",
      "00:00:30 5.5Gb  INFO: Finished sorting. Total distinct minimizers: 10161\n",
      "00:00:31 5.5Gb  INFO: Starting construction of sparse de Bruijn graph\n",
      "00:00:31 5.5Gb  INFO: Vertex map constructed.\n",
      "00:00:31 5.5Gb  INFO: Filling edge sequences.\n",
      "00:01:00 5.7Gb  INFO: Finished sparse de Bruijn graph construction.\n",
      "00:01:00 5.7Gb  INFO:  Collecting tips \n",
      "00:01:00 5.7Gb  INFO: Added 29 artificial minimizers from tips.\n",
      "00:01:00 5.7Gb  INFO: Collected 20508 old edges.\n",
      "00:01:00 5.7Gb  INFO: New minimizers added to sparse graph.\n",
      "00:01:00 5.7Gb  INFO: Refilling graph with old edges.\n",
      "00:01:01 5.7Gb  INFO: Filling graph with new edges.\n",
      "00:01:01 5.7Gb  INFO: Finished fixing sparse de Bruijn graph.\n",
      "00:01:01 5.7Gb  INFO: Starting to extract disjointigs.\n",
      "00:01:01 5.7Gb  INFO: Finished extracting 327 disjointigs of total size 4183486\n",
      "00:01:01 0Mb  INFO: Loading disjointigs from file \"phasing-data/smoothed-reads/edge_2358_lja_noec/k5001/disjointigs.fasta\"\n",
      "00:01:01 14Mb  INFO: Filling bloom filter with k+1-mers.\n",
      "00:01:02 15Mb  INFO: Filled 11795859 bits out of 81541088\n",
      "00:01:02 15Mb  INFO: Finished filling bloom filter. Selecting junctions.\n",
      "00:01:02 15Mb  INFO: Collected 1242 junctions.\n",
      "00:01:02 15Mb  INFO: Starting DBG construction.\n",
      "00:01:02 15Mb  INFO: Vertices created.\n",
      "00:01:02 15Mb  INFO: Filled dbg edges. Adding hanging vertices \n",
      "00:01:02 15Mb  INFO: Added 0 hanging vertices\n",
      "00:01:02 15Mb  INFO: Merging unbranching paths\n",
      "00:01:02 15Mb  INFO: Ended merging edges. Resulting size 244\n",
      "00:01:03 15Mb  INFO: Cleaning edge coverages\n",
      "00:01:03 15Mb  INFO: Collecting alignments of sequences to the graph\n",
      "00:01:24 8.9Gb  INFO: Alignment collection finished. Total length of alignments is 887388\n",
      "00:01:27 8.9Gb  INFO: Printing reads to fasta file \"phasing-data/smoothed-reads/edge_2358_lja_noec/k5001/corrected_reads.fasta\"\n",
      "00:02:24 14.5Gb  INFO: Performing repeat resolution by transforming de Bruijn graph into Multiplex de Bruijn graph\n",
      "00:02:24 0Mb  INFO: Loading graph from fasta\n",
      "00:02:25 14Mb  INFO: Finished loading graph\n",
      "00:02:26 245Mb  INFO: Looking for unique edges\n",
      "00:02:26 245Mb  INFO: Marked 18 long edges as unique\n",
      "00:02:26 245Mb  INFO: Marking extra edges as unique based on read paths\n",
      "00:02:26 245Mb  INFO: Marked 18 edges as unique\n",
      "00:02:26 245Mb  INFO: Splitting graph with unique edges\n",
      "00:02:26 245Mb  INFO: Processing 15 components\n",
      "00:02:26 245Mb  INFO: Finished unique edges search. Found 72 unique edges\n",
      "00:02:26 245Mb  INFO: Analysing repeats of multiplicity 2 and looking for additional unique edges\n",
      "00:02:26 245Mb  INFO: Finished processing of repeats of multiplicity 2. Found 0 erroneous edges.\n",
      "00:02:26 245Mb  INFO: Resolving repeats\n",
      "00:02:26 245Mb  INFO: Constructing paths\n",
      "00:02:33 0.5Gb  INFO: Building graph\n",
      "00:02:33 0.5Gb  INFO: Increasing k\n",
      "00:02:40 0.5Gb  INFO: Finished increasing k\n",
      "00:02:40 0.5Gb  INFO: Exporting remaining active transitions\n",
      "00:02:40 0.5Gb  INFO: Export to Dot\n",
      "00:02:40 0.5Gb  INFO: Export to GFA and compressed contigs\n",
      "00:02:41 0.5Gb  INFO: Finished repeat resolution\n",
      "00:02:41 14.5Gb  INFO: Performing polishing and homopolymer uncompression\n",
      "00:02:41 12Mb  INFO: Aligning reads back to assembly\n",
      "00:03:15 5.7Gb  INFO: Finished alignment.\n",
      "00:03:15 5.7Gb  INFO: Printing alignments to \"phasing-data/smoothed-reads/edge_2358_lja_noec/uncompressing/alignments.txt\"\n",
      "00:03:17 5.7Gb  INFO: Reading and processing initial reads from [\"phasing-data/smoothed-reads/edge_2358_smoothed_reads.fasta\"]\n",
      "00:09:36 6.6Gb  INFO: Uncompressing homopolymers in contigs\n",
      "00:09:37 6.6Gb  INFO: Total zero covered nucleotides 0\n",
      "00:09:37 6.6Gb  INFO: Calculating overlaps between adjacent uncompressed edges\n",
      "00:09:40 6.6Gb  INFO: Printing final gfa file to \"phasing-data/smoothed-reads/edge_2358_lja_noec/mdbg.gfa\"\n",
      "00:09:40 6.6Gb  INFO: Printing final assembly to \"phasing-data/smoothed-reads/edge_2358_lja_noec/assembly.fasta\"\n",
      "00:09:40 14.5Gb  INFO: Final homopolymer compressed and corrected reads can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_noec/k5001/corrected_reads.fasta\"\n",
      "00:09:40 14.5Gb  INFO: Final graph with homopolymer compressed edges can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_noec/mdbg/mdbg.hpc.gfa\"\n",
      "00:09:40 14.5Gb  INFO: Final graph can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_noec/mdbg.gfa\"\n",
      "00:09:40 14.5Gb  INFO: Final assembly can be found here: \"phasing-data/smoothed-reads/edge_2358_lja_noec/assembly.fasta\"\n",
      "00:09:40 14.5Gb  INFO: LJA pipeline finished\n"
     ]
    }
   ],
   "source": [
    "!/home/mfedarko/software/LJA/bin/lja \\\n",
    "    --reads phasing-data/smoothed-reads/edge_6104_smoothed_reads.fasta \\\n",
    "    --noec \\\n",
    "    --output-dir phasing-data/smoothed-reads/edge_6104_lja_noec\n",
    "\n",
    "!/home/mfedarko/software/LJA/bin/lja \\\n",
    "    --reads phasing-data/smoothed-reads/edge_1671_smoothed_reads.fasta \\\n",
    "    --noec \\\n",
    "    --output-dir phasing-data/smoothed-reads/edge_1671_lja_noec\n",
    "\n",
    "!/home/mfedarko/software/LJA/bin/lja \\\n",
    "    --reads phasing-data/smoothed-reads/edge_2358_smoothed_reads.fasta \\\n",
    "    --noec \\\n",
    "    --output-dir phasing-data/smoothed-reads/edge_2358_lja_noec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
